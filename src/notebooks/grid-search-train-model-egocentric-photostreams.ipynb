{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_notebook(fix_python_path=True, reduce_margins=True, plot_inline=True):\n",
    "    if reduce_margins:\n",
    "        # Reduce side margins of the notebook\n",
    "        from IPython.core.display import display, HTML\n",
    "        display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "    if fix_python_path:\n",
    "        # add egosocial to the python path\n",
    "        import os, sys\n",
    "        sys.path.extend([os.path.dirname(os.path.abspath('.'))])\n",
    "\n",
    "    if plot_inline:\n",
    "        # Plots inside cells\n",
    "        %matplotlib inline\n",
    "    \n",
    "    global __file__\n",
    "    __file__ = 'Notebook'\n",
    "\n",
    "setup_notebook(reduce_margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Constants Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import functools\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import sys\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import scipy\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "import egosocial\n",
    "import egosocial.config\n",
    "from egosocial.core.attributes import AttributeSelector\n",
    "from egosocial.core.models import create_model_top_down\n",
    "from egosocial.core.models import create_model_bottom_up\n",
    "from egosocial.utils.filesystem import create_directory \n",
    "from egosocial.utils.filesystem import check_directory\n",
    "from egosocial.utils.keras.autolosses import AutoMultiLossWrapper\n",
    "from egosocial.utils.keras.backend import limit_gpu_allocation_tensorflow\n",
    "from egosocial.utils.keras.callbacks import PlotLearning\n",
    "from egosocial.utils.keras.metrics import precision\n",
    "from egosocial.utils.keras.metrics import recall\n",
    "from egosocial.utils.keras.metrics import fmeasure\n",
    "from egosocial.utils.keras.processing import TimeSeriesDataGenerator\n",
    "from egosocial.utils.keras.scikit_learn import KerasGeneratorClassifier\n",
    "from egosocial.utils.logging import setup_logging\n",
    "from egosocial.utils.misc import RELATIONS, DOMAINS\n",
    "from egosocial.utils.misc import LabelExpander\n",
    "from egosocial.utils.misc import relation_to_domain_vec\n",
    "from egosocial.utils.sklearn.model_selection import StratifiedGroupShuffleSplitWrapper\n",
    "\n",
    "SHARED_SEED = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit GPU memory allocation with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_memory = True\n",
    "if limit_memory and K.backend() == 'tensorflow':\n",
    "    memory_ratio = 0.001\n",
    "    limit_gpu_allocation_tensorflow(memory_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def main(*fake_args):\n",
    "    entry_msg = 'Train model for social relations classification in egosocial photo-streams.'\n",
    "    parser = argparse.ArgumentParser(description=entry_msg)\n",
    "\n",
    "    parser.add_argument('--dataset_path', required=True,\n",
    "                        help='Path to file containing the input data and labels information merged.')\n",
    "\n",
    "    parser.add_argument('--features_dir', required=True,\n",
    "                        help='Directory where the extracted features are stored.')\n",
    "    \n",
    "    parser.add_argument('--batch_size', required=False, type=int,\n",
    "                        default=32,\n",
    "                        help='Batch size.')\n",
    "    \n",
    "    if not os.path.isdir(egosocial.config.TMP_DIR):\n",
    "        os.mkdir(egosocial.config.TMP_DIR)\n",
    "\n",
    "    setup_logging(egosocial.config.LOGGING_CONFIG,\n",
    "                  log_dir=egosocial.config.LOGS_DIR)\n",
    "    \n",
    "    # TODO: implement correctly\n",
    "    args = parser.parse_args(*fake_args)\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_day(image_path):\n",
    "    image_name = os.path.basename(image_path)\n",
    "    # a valid image follows the day_hour_x.ext format\n",
    "    day_hour_rest = image_name.split('_')\n",
    "    \n",
    "    if len(day_hour_rest) == 3:\n",
    "        # day is the first item\n",
    "        return day_hour_rest[0]\n",
    "    else:\n",
    "        # day isn't available\n",
    "        return ''\n",
    "    \n",
    "def load_dataset_defition(dataset_path, include_day=True):\n",
    "    with open(dataset_path, 'r') as json_file:\n",
    "        dataset_def = json.load(json_file)\n",
    "\n",
    "    # flatten the segments structure\n",
    "    samples = pd.DataFrame(list(itertools.chain(*dataset_def)))\n",
    "    \n",
    "    if include_day:\n",
    "        samples['day'] = samples['global_image_path'].apply(parse_day)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def as_sequences(features, sequences_index):\n",
    "\n",
    "    feature_sequences = []\n",
    "    for index in sequences_index:\n",
    "        feature_seq = features[index]\n",
    "        feature_seq.shape = (feature_seq.shape[0], -1)\n",
    "        feature_sequences.append(feature_seq)\n",
    "    \n",
    "    return np.asarray(feature_sequences)\n",
    "\n",
    "def load_features(features_path, sequences_index):    \n",
    "    return as_sequences(np.load(features_path), sequences_index)\n",
    "\n",
    "def load_fields(data_frames, fields, valid_frames_idx=None):\n",
    "    assert len(fields) > 0\n",
    "    \n",
    "    if valid_frames_idx is None:\n",
    "        sequences_info = data_frames.groupby(['split', 'segment_id', 'group_id'])\n",
    "    else:\n",
    "        sequences_info = data_frames[valid_frames_idx].groupby(['split', 'segment_id', 'group_id'])\n",
    "    \n",
    "    fst_seq_frames = [group.index[0] for _, group in sequences_info]\n",
    "    fields_data =  data_frames.iloc[fst_seq_frames][fields].values\n",
    "\n",
    "    return [fields_data[:, field_idx] for field_idx in range(len(fields))]\n",
    "        \n",
    "def compute_stats(y, y_predicted):\n",
    "    acc = sklearn.metrics.accuracy_score(y, y_predicted)\n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(y, y_predicted)\n",
    "    report = sklearn.metrics.classification_report(y, y_predicted)\n",
    "\n",
    "    return acc, confusion_matrix, report\n",
    "\n",
    "def print_statistics(val_stats=None, test_stats=None, fdesc=sys.stdout):\n",
    "    for description, stats in [('Validation set:', val_stats), (('Test set:', test_stats))]:\n",
    "        \n",
    "        if stats is not None:\n",
    "            print(description, file=fdesc)\n",
    "            accuracy, confusion_matrix, report = stats\n",
    "            print('Confusion matrix:', file=fdesc)\n",
    "            print(confusion_matrix, file=fdesc)\n",
    "            print(file=fdesc)\n",
    "            print(report, file=fdesc)\n",
    "            print('Accuracy: {:.3f}'.format(accuracy), file=fdesc)\n",
    "            print('------------------------------------------------', file=fdesc)\n",
    "            \n",
    "def compute_class_frequency(y, index=None):\n",
    "    if index:\n",
    "        y =  y[index]\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    freq = counts / len(y)\n",
    "    return freq\n",
    "                \n",
    "class DimReductionTransformer(object):        \n",
    "\n",
    "    def __init__(self, n_components, Q=32, normalize=True, random_state=None):\n",
    "        # PCA configuration (number of components or min explained variance)\n",
    "        self.pca_param = n_components\n",
    "         # features quantization (smaller Q promotes sparsity)\n",
    "        self.Q = Q\n",
    "        self.normalize = normalize\n",
    "        self.random_state = sklearn.utils.check_random_state(random_state)\n",
    "        \n",
    "        self._scaler = None\n",
    "        self._pca = None\n",
    "                \n",
    "    def fit(self, x):\n",
    "        # reset state\n",
    "        self._scaler = self._pca = None        \n",
    "    \n",
    "        if self.normalize:\n",
    "            if self.Q: # quantization requires data in range [0, 1] \n",
    "                self._scaler = Normalizer(norm='l2')\n",
    "            else:\n",
    "                self._scaler = StandardScaler()\n",
    "            \n",
    "            x = self._scaler.fit_transform(x)\n",
    "\n",
    "        if self.Q:\n",
    "            # small Q promotes sparsity\n",
    "            x = np.floor(self.Q * x)            \n",
    "            \n",
    "        assert self.pca_param > 0     \n",
    "        # compute pca from scratch\n",
    "        if 0 < self.pca_param <= 1:\n",
    "            # running pca with min explained variance takes much longer\n",
    "            self._pca = PCA(self.pca_param, random_state=self.random_state)\n",
    "        else:\n",
    "            self._pca = PCA(self.pca_param, svd_solver='randomized', random_state=self.random_state)\n",
    "        \n",
    "        self._pca.fit(x)\n",
    "        \n",
    "    def transform(self, x):\n",
    "        if self.normalize:\n",
    "            x = self._scaler.transform(x)\n",
    "\n",
    "        if self.Q:\n",
    "            # small Q promotes sparsity\n",
    "            x = np.floor(self.Q * x)            \n",
    "\n",
    "        # pca transformation\n",
    "        x = self._pca.transform(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Preprocessing(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, features_range=None, create_transformation_cbk=None):\n",
    "        self.features_range = features_range if features_range else dict(all=(0,-1))\n",
    "        self.create_transformation_cbk = create_transformation_cbk\n",
    "        self._transformation_dict = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self._transformation_dict = {}\n",
    "        \n",
    "        X = np.concatenate(list(itertools.chain(X)))     \n",
    "        for features_id in sorted(self.features_range.keys()):\n",
    "            transformation = self._transformation_dict[features_id] = self.create_transformation_cbk(features_id)\n",
    "            if transformation:\n",
    "                begin_slice, end_slice = self.features_range[features_id] \n",
    "                X_features = X[:, begin_slice:end_slice]\n",
    "                transformation.fit(X_features)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        seq_length = list(map(len, X))\n",
    "        X = np.concatenate(list(itertools.chain(X)), axis=0)\n",
    "        \n",
    "        features_list = []\n",
    "        for features_id in sorted(self.features_range.keys()):\n",
    "            transformation = self._transformation_dict.get(features_id, None)            \n",
    "\n",
    "            begin_slice, end_slice = self.features_range[features_id] \n",
    "            X_features = X[:, begin_slice:end_slice]\n",
    "            \n",
    "            if transformation:\n",
    "                X_features = transformation.transform(X_features)\n",
    "            \n",
    "            features_list.append(X_features)\n",
    "            \n",
    "        transformed_features = np.concatenate(features_list, axis=-1)            \n",
    "        seq_end = list(np.cumsum(seq_length))\n",
    "        seq_begin = [0] + seq_end[:-1]\n",
    "        sequences = np.asarray([transformed_features[begin:end, :] for begin, end in zip(seq_begin, seq_end)])\n",
    "        \n",
    "        return sequences\n",
    "\n",
    "class TransformationFactory(object):\n",
    "\n",
    "    def __init__(self, n_components=50, Q=32, seed=None):\n",
    "        self.n_components = n_components\n",
    "        self.Q = Q\n",
    "        self.seed = seed\n",
    "\n",
    "    def __call__(self, attribute):\n",
    "        if attribute in ('camera_user_age', 'camera_user_gender'):\n",
    "            return None\n",
    "        elif attribute == 'distance':\n",
    "            return MinMaxScaler()\n",
    "        else:\n",
    "            return DimReductionTransformer(n_components=self.n_components, Q=self.Q, random_state=self.seed)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialClassifierWithPreComputedFeatures:\n",
    "    \n",
    "    def __init__(self, dataset_path, features_dir, test_size=0.2, k_fold_splits=10, val_size=None, n_components=50, Q=32, seed=42):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.features_dir = features_dir\n",
    "        self.test_size = test_size\n",
    "        self.k_fold_splits = k_fold_splits\n",
    "        self.val_size = val_size if val_size else 1.0 / k_fold_splits\n",
    "        self.seed = seed\n",
    "\n",
    "        self._max_seq_len = None\n",
    "        self._labels = None\n",
    "        self._users = None\n",
    "\n",
    "        self.attributes = []\n",
    "        self.features = None\n",
    "        self.features_range = None\n",
    "\n",
    "        self._train_idx = None\n",
    "        self._test_idx = None\n",
    "        self._k_train_val_idx = None\n",
    "        \n",
    "        self._log = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def load_data(self):        \n",
    "        # load dataset definition\n",
    "        frames = load_dataset_defition(self.dataset_path, include_day=True)\n",
    "        # filter labels with few samples\n",
    "        valid_frames_idx = np.isin(frames['relation_label'], RELATIONS)\n",
    "\n",
    "        # for each sequence get label, user, day of first frame\n",
    "        self._labels, self._users, self._seq_days = load_fields(\n",
    "            frames, ['relation_label', 'camera_user_name', 'day'], \n",
    "            valid_frames_idx=valid_frames_idx\n",
    "        )\n",
    "                \n",
    "        grouped_frames = frames[valid_frames_idx].groupby(['split', 'segment_id', 'group_id'])        \n",
    "        self._max_seq_len = grouped_frames.size().max()\n",
    "        \n",
    "        sequences_index = [list(group.index) for _, group in grouped_frames]\n",
    "        self._init_features(sequences_index)\n",
    "\n",
    "        self._init_grouped_splits()\n",
    "\n",
    "    def _init_features(self, sequences_index):\n",
    "        attribute_files = sorted(next(os.walk(self.features_dir))[2])\n",
    "        \n",
    "        ext = '.npy'\n",
    "        self.attributes = [os.path.splitext(file)[0] for file in attribute_files if file.endswith(ext)]\n",
    "        self.features_range = {}\n",
    "\n",
    "        # load features\n",
    "        attribute_list = []\n",
    "        begin = 0\n",
    "        for attribute_name in self.attributes:\n",
    "            path = os.path.join(self.features_dir, attribute_name + ext)\n",
    "            attribute_features = np.load(path)\n",
    "            attribute_features.shape = (attribute_features.shape[0], -1)\n",
    "            self._log.debug('Loading features {} dim: {}'.format(attribute_name, attribute_features.shape))\n",
    "            end = begin + attribute_features.shape[-1]            \n",
    "\n",
    "            attribute_list.append(attribute_features)            \n",
    "            self.features_range[attribute_name] = (begin, end)            \n",
    "            begin = end\n",
    "        \n",
    "        self.features = as_sequences(np.concatenate(attribute_list, axis=-1), sequences_index)\n",
    "        \n",
    "    def _init_grouped_splits(self):\n",
    "        # define data splits\n",
    "        # define train, test splits        \n",
    "        criteria = np.array([ user + '_' + day for user, day in zip(self._users, self._seq_days) ])\n",
    "        y, groups = self._labels, criteria\n",
    "\n",
    "        n_tries, group_size, epsilon = 1000, self.test_size, 0.025\n",
    "\n",
    "        split_wrapper = StratifiedGroupShuffleSplitWrapper(\n",
    "            GroupShuffleSplit(n_splits=n_tries, test_size=group_size, random_state=self.seed), \n",
    "            n_splits=1, \n",
    "            max_test_size=min(self.test_size + epsilon, 1.0), min_test_size=max(self.test_size - epsilon, 0.0)\n",
    "        )\n",
    "        self._train_idx, self._test_idx, train_test_score = next(split_wrapper.split(np.zeros(len(y)), y, groups, return_score=True))\n",
    "        test_size = len(self._test_idx) / (len(self._train_idx) + len(self._test_idx))\n",
    "        self._log.debug('Split train-test score: {:.3} real_test_size: {:.3}'.format(train_test_score, test_size))\n",
    "        \n",
    "        # define k-fold splits\n",
    "        y, groups = y[self._train_idx], groups[self._train_idx]\n",
    "        \n",
    "        if self.k_fold_splits > 1:\n",
    "            # k-fold strategy\n",
    "            # search 50 times the number of splits, encourage diversity\n",
    "            # double the epsilon (more flexible)\n",
    "            n_tries, group_size, epsilon = self.k_fold_splits * 50, self.val_size, 0.05\n",
    "        else:\n",
    "            # holdout strategy\n",
    "            # keep n_tries and epsilon same as train-test split\n",
    "            group_size = self.val_size\n",
    "        \n",
    "        split_wrapper = StratifiedGroupShuffleSplitWrapper(\n",
    "            GroupShuffleSplit(n_splits=n_tries, test_size=group_size, random_state=self.seed), \n",
    "            n_splits=self.k_fold_splits,\n",
    "            max_test_size=min(self.val_size + epsilon, 1.0), min_test_size=max(self.val_size - epsilon, 0.0)\n",
    "        )\n",
    "        \n",
    "        self._k_train_val_idx = []\n",
    "        for k, (t_idx, v_idx, t_v_score) in enumerate(split_wrapper.split(np.zeros(len(y)), y, groups, return_score=True)):\n",
    "            self._k_train_val_idx.append((self._train_idx[t_idx], self._train_idx[v_idx]))\n",
    "            val_size = (1 - test_size) * len(v_idx) / (len(t_idx) + len(v_idx))\n",
    "            self._log.debug('{}-fold split score: {:.3} real_val_size={:.3}'.format(k, t_v_score, val_size))            \n",
    "            \n",
    "    def list_attributes(self):\n",
    "        # list attributes\n",
    "        return self.attributes\n",
    "            \n",
    "    def get_split_idx(self, split, k_fold=None):\n",
    "        assert split in ('train', 'test', 'val')\n",
    "        if split == 'train':\n",
    "            if k_fold is None:\n",
    "                return self._train_idx\n",
    "            else:\n",
    "                assert 0 <= k_fold < self.k_fold_splits\n",
    "                return self._k_train_val_idx[k_fold][0]\n",
    "\n",
    "        if split == 'val':\n",
    "            assert 0 <= k_fold < self.k_fold_splits\n",
    "            return self._k_train_val_idx[k_fold][1]\n",
    "        \n",
    "        if split == 'test':\n",
    "            return self._test_idx\n",
    "\n",
    "    def max_sequence_len(self):\n",
    "        return self._max_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_callbacks(output_mode, plot_stats=True, save_model=False, save_stats=False, stop_early=False, plot_step=1, reduce_lr=False, figsize=None):\n",
    "    callbacks = []\n",
    "\n",
    "    training_dir = os.path.join(egosocial.config.TMP_DIR, 'training')\n",
    "    create_directory(training_dir, 'Training')\n",
    "\n",
    "    if save_model:\n",
    "        checkpoint_path = os.path.join(training_dir,\n",
    "                                       'weights.{epoch:02d}-{val_loss:.2f}.h5')\n",
    "        checkpointer = ModelCheckpoint( \n",
    "            filepath=checkpoint_path, monitor='val_loss',\n",
    "            save_best_only=True, period=5,\n",
    "        )\n",
    "        callbacks.append(checkpointer)\n",
    "\n",
    "    if save_stats:\n",
    "        metrics_path = os.path.join(training_dir,\n",
    "                                    'metrics.csv')\n",
    "        csv_logger = CSVLogger(metrics_path)\n",
    "        callbacks.append(csv_logger)\n",
    "\n",
    "    if reduce_lr:\n",
    "        lr_handler = ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001\n",
    "        )\n",
    "        callbacks.append(lr_handler)\n",
    "   \n",
    "    if plot_stats:\n",
    "        # more plots need more space\n",
    "        if not figsize:\n",
    "            if output_mode != 'both_splitted':\n",
    "                figsize = (25, 5)\n",
    "            else:\n",
    "                figsize = (25, 13)\n",
    "\n",
    "        plot_metrics = PlotLearning(update_step=plot_step, figsize=figsize)\n",
    "        callbacks.append(plot_metrics)\n",
    "        \n",
    "    if stop_early:\n",
    "        stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, mode='auto')\n",
    "        callbacks.append(stopper)\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "def compile_model(\n",
    "    model, \n",
    "    optimizer='adam', \n",
    "    loss='categorical_crossentropy', \n",
    "    loss_weights='auto',\n",
    "    **kwargs\n",
    "):\n",
    "    # wrapper allows to train the loss weights\n",
    "    model_wrapper = AutoMultiLossWrapper(model)\n",
    "    model_wrapper.compile(optimizer=optimizer, loss=loss, \n",
    "                          loss_weights=loss_weights, **kwargs)\n",
    "\n",
    "    return model_wrapper.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(results, n_top=3, metric='score'):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_' + metric] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation {}: {:.3f} (std: {:.3f})\".format(\n",
    "                  metric,\n",
    "                  results['mean_test_' + metric][candidate],\n",
    "                  results['std_test_' + metric][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    mode='both_splitted',\n",
    "    model_strategy='top_down', \n",
    "    recurrent_type='LSTM',\n",
    "    max_seq_len=34,\n",
    "    feature_vector_size=None,\n",
    "    hidden_fc=0,\n",
    "    units=128, \n",
    "    drop_rate=0.5,\n",
    "    l2_reg=0.01,    \n",
    "    learning_rate=0.0001,\n",
    "    loss='categorical_crossentropy',    \n",
    "    loss_weights='auto',\n",
    "    metrics=None,\n",
    "):\n",
    "    if feature_vector_size is None:\n",
    "        global n_features\n",
    "        feature_vector_size = n_features\n",
    "    \n",
    "    model_strategy_select = {\n",
    "        'top_down' : create_model_top_down,\n",
    "        'bottom_up' : create_model_bottom_up,\n",
    "    }\n",
    "    \n",
    "    model_parameters = dict(\n",
    "        mode=mode,\n",
    "        max_seq_len=max_seq_len, \n",
    "        n_features=feature_vector_size,\n",
    "        units=units,\n",
    "        drop_rate=drop_rate,\n",
    "        rec_drop_rate=drop_rate,        \n",
    "        l2_reg=l2_reg,\n",
    "        hidden_fc=hidden_fc,\n",
    "        recurrent_type=recurrent_type,\n",
    "        n_relations=len(RELATIONS),\n",
    "        n_domains=len(DOMAINS),  \n",
    "    )\n",
    "    \n",
    "    model = model_strategy_select[model_strategy](**model_parameters)\n",
    "    if model_strategy_select == 'bottom_up':\n",
    "        model.get_layer('domain').set_weights([relation_to_domain_weights()])\n",
    "        \n",
    "    model = compile_model(\n",
    "        model,   \n",
    "        optimizer=keras.optimizers.Adam(learning_rate, decay=1e-5),\n",
    "        loss=loss,\n",
    "        loss_weights=loss_weights,\n",
    "        metrics=metrics if metrics else ['accuracy'],\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "class TimeSeriesDataGeneratorBuilder(object):\n",
    "    def __init__(self, noise_stddev=0.01, seed=SHARED_SEED, maxlen=None, output_cbk=None, balanced=False):\n",
    "        self.noise_stddev = 0.01\n",
    "        self.seed = seed\n",
    "        self.maxlen = maxlen\n",
    "        self.output_cbk = output_cbk\n",
    "        self.balanced = balanced\n",
    "\n",
    "    def __call__(self, X, y=None, batch_size=32, phase='train'):\n",
    "        assert phase in ('train', 'test')  \n",
    "        \n",
    "        if phase == 'train':\n",
    "            datagen = TimeSeriesDataGenerator(fancy_pca=True, \n",
    "                                              noise_stddev=self.noise_stddev, \n",
    "                                              random_state=self.seed)\n",
    "            datagen.fit(X)\n",
    "        else:\n",
    "            datagen = TimeSeriesDataGenerator(fancy_pca=False)\n",
    "        \n",
    "        shuffle = (phase == 'train')\n",
    "        \n",
    "        if phase == 'train':\n",
    "            generator = datagen.flow(\n",
    "                X, y,\n",
    "                maxlen=self.maxlen,\n",
    "                output_cbk=self.output_cbk,\n",
    "                balanced=self.balanced,\n",
    "                batch_size=batch_size,\n",
    "                seed=self.seed,\n",
    "                shuffle=shuffle,\n",
    "            )\n",
    "        else:\n",
    "            generator = datagen.flow(\n",
    "                X, y,\n",
    "                maxlen=self.maxlen,\n",
    "                output_cbk=self.output_cbk,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffle,\n",
    "            )\n",
    "            \n",
    "        return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.join(egosocial.config.TMP_DIR, 'egocentric', 'datasets')\n",
    "\n",
    "args = [\n",
    "    \"--dataset_path\", os.path.join(BASE_DIR, 'merged_dataset.json'),\n",
    "    \"--features_dir\", os.path.join(BASE_DIR, 'extracted_features'),\n",
    "]\n",
    "\n",
    "conf = main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading precomputed features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "helper = SocialClassifierWithPreComputedFeatures(\n",
    "    conf.dataset_path, conf.features_dir, \n",
    "    test_size=0.2, \n",
    "    k_fold_splits=3, \n",
    "    val_size=0.2,\n",
    "    seed=SHARED_SEED\n",
    ")\n",
    "\n",
    "helper.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_val_splits = [ \n",
    "    (helper.get_split_idx('train', k_fold=k), helper.get_split_idx('val', k_fold=k)) \n",
    "    for k in range(helper.k_fold_splits)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components, Q = 50, 32\n",
    "n_features = n_components * 9 + 6 + 2 + 1\n",
    "max_timestep = helper.max_sequence_len()\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "print('Length of the largest sequence:', max_timestep)\n",
    "print('Number of features:', n_features)\n",
    "print('Batch size: {}'.format(batch_size))\n",
    "\n",
    "#output_mode = 'both_fused' # domain-relation outputs fused\n",
    "#output_mode = 'both_splitted' # multi-loss domain-relation\n",
    "output_mode = 'domain' # domain only\n",
    "#output_mode = 'relation' # relation only\n",
    "\n",
    "print('Output mode: {}'.format(output_mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_dim = Preprocessing(\n",
    "    features_range=helper.features_range, \n",
    "    create_transformation_cbk=TransformationFactory(n_components=n_components, Q=Q, seed=SHARED_SEED)\n",
    ")\n",
    "\n",
    "generator_builder = TimeSeriesDataGeneratorBuilder(\n",
    "    maxlen=max_timestep,\n",
    "    output_cbk=LabelExpander(mode=output_mode),\n",
    "    seed=SHARED_SEED,\n",
    ")\n",
    "\n",
    "# both_splitted mode uses relation fmeasure score \n",
    "single_output = 'domain' if output_mode == 'domain' else 'relation'\n",
    "metric_suffix = 'fmeasure'\n",
    "# used only if GridSearchCV scoring attribute is set to None \n",
    "metric_score = single_output + '_' + metric_suffix if output_mode == 'both_splitted' else metric_suffix\n",
    "\n",
    "clf = KerasGeneratorClassifier(\n",
    "    build_fn=build_model,\n",
    "    build_generator=generator_builder,\n",
    "    output_mode=output_mode,\n",
    "    metric_score=metric_score,\n",
    "    single_output=single_output,\n",
    "    balanced=True,        \n",
    "\n",
    "    max_seq_len=max_timestep,\n",
    "    feature_vector_size=n_features,        \n",
    "    recurrent_type='GRU',\n",
    "    hidden_fc=1,\n",
    "    mode=output_mode,\n",
    "\n",
    "    metrics=['accuracy', fmeasure],\n",
    "    verbose=1,\n",
    "    workers=2,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([('reduce_dim', reduce_dim), ('clf', clf)])\n",
    "\n",
    "callbacks = init_callbacks(output_mode, plot_stats=False, figsize=(25, 13), plot_step=10)\n",
    "\n",
    "fit_params = dict(\n",
    "    clf__verbose=1,\n",
    "    clf__callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_search = 'grid'\n",
    "\n",
    "common_search_params = dict(\n",
    "    estimator=pipeline, \n",
    "    cv=train_val_splits,\n",
    "    scoring=['accuracy', 'recall_weighted', 'precision_weighted', 'f1_weighted'],\n",
    "    refit=False,\n",
    "    return_train_score=True,\n",
    "    iid=False,\n",
    "    verbose=4,\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "if do_search == 'grid':\n",
    "    param_grid_1 = dict(\n",
    "        clf__drop_rate=[0.3, 0.6],\n",
    "        clf__learning_rate=[0.0001, 0.001],\n",
    "        clf__epochs=[150, 300],\n",
    "        clf__units=[128],\n",
    "        clf__l2_reg=[0.01, 0.001],\n",
    "    )\n",
    "\n",
    "    param_grid = dict(\n",
    "        clf__epochs=[2],\n",
    "    )\n",
    "    \n",
    "    search_cv = GridSearchCV(\n",
    "        param_grid=param_grid,\n",
    "        **common_search_params,\n",
    "    )\n",
    "    \n",
    "elif do_search == 'random':\n",
    "    param_dist = dict(\n",
    "        clf__drop_rate=np.linspace(0, 0.9),\n",
    "        clf__learning_rate=np.logspace(-4, -2, base=10),\n",
    "        clf__epochs=np.linspace(100, 300).astype(int),\n",
    "        clf__units=np.logspace(6, 8, base=2).astype(int),\n",
    "        clf__l2_reg=np.logspace(-4, -2, base=10)\n",
    "    )\n",
    "\n",
    "    search_cv = RandomizedSearchCV(\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=10,\n",
    "        **common_search_params,\n",
    "    )\n",
    "\n",
    "X = helper.features\n",
    "if output_mode == 'domain':\n",
    "    # domain specific-labels\n",
    "    y = relation_to_domain_vec(helper._labels)\n",
    "else:\n",
    "    y = helper._labels\n",
    "    \n",
    "search_result = search_cv.fit(X, y, **fit_params)\n",
    "\n",
    "cv_results = search_result.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = os.path.join(egosocial.config.TMP_DIR, 'training')\n",
    "create_directory(training_dir, 'Training')\n",
    "\n",
    "save_results = False\n",
    "\n",
    "if save_results:\n",
    "    date_str = datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    results_path = os.path.join(training_dir, \n",
    "                                '{}_{}_cv_results.pkl'.format(date_str, do_search)\n",
    "                               )\n",
    "\n",
    "    with open(results_path, 'wb') as file:\n",
    "        pickle.dump(search_result, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join(training_dir, 'lstm_relu_split_v2')\n",
    "\n",
    "result_files = sorted(os.listdir(data_dir))\n",
    "result_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_cv_results(results_path):\n",
    "    with open(results_path, 'rb') as file:\n",
    "            search_result = pickle.load(file)\n",
    "\n",
    "    if hasattr(search_result, 'cv_results_'):\n",
    "        cv_results = search_result.cv_results_\n",
    "    else:\n",
    "        cv_results = search_result\n",
    "\n",
    "    return cv_results\n",
    "        \n",
    "def load_cv_results(results_path):\n",
    "    cv_results = load_raw_cv_results(results_path)\n",
    "    mean_train_acc = pd.DataFrame(cv_results['mean_train_accuracy'], columns=['train_acc'])\n",
    "    mean_test_acc = pd.DataFrame(cv_results['mean_test_accuracy'], columns=['test_acc'])\n",
    "    params = pd.DataFrame(cv_results['params'])\n",
    "    \n",
    "    fields = [params, mean_train_acc, mean_test_acc]\n",
    "    \n",
    "    optional = ['mean_train_f1_macro', 'mean_test_f1_macro', 'mean_train_f1_weighted', 'mean_test_f1_weighted']\n",
    "    for opt_field in optional:\n",
    "        if opt_field in cv_results:\n",
    "            name = opt_field[len('mean_'):]\n",
    "            field_df = pd.DataFrame(cv_results[opt_field], columns=[name])\n",
    "            fields.append(field_df)\n",
    "    \n",
    "    table = pd.concat(fields, axis=1)\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_results = True\n",
    "\n",
    "if load_results:\n",
    "    output = 'both_splitted_top_down'\n",
    "    index_list = [idx for idx, name in enumerate(result_files) if output in name]    \n",
    "    \n",
    "    tables = [load_cv_results(os.path.join(data_dir, result_files[idx])) for idx in index_list]\n",
    "    table = pd.concat(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.sort_values('test_f1_weighted', ascending=False).head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table.sort_values('test_f1_macro', ascending=False).head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.sort_values('test_acc', ascending=False).head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_f1_weighted = table.sort_values('test_f1_weighted', ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_f1_macro = table.sort_values('test_f1_macro', ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_acc = table.sort_values('test_acc', ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [idx_f1_weighted, idx_f1_macro, idx_acc]\n",
    "global_rank = np.zeros(np.max(indices)+1)\n",
    "\n",
    "for idx in indices:\n",
    "    for rank, pos in enumerate(idx):\n",
    "        global_rank[pos] += rank+1\n",
    "\n",
    "sorted_idx = np.argsort(global_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(sorted_idx, global_rank[sorted_idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
