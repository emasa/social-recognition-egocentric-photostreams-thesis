{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_notebook(fix_python_path=True, reduce_margins=True, plot_inline=True):\n",
    "    if reduce_margins:\n",
    "        # Reduce side margins of the notebook\n",
    "        from IPython.core.display import display, HTML\n",
    "        display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "    if fix_python_path:\n",
    "        # add egosocial to the python path\n",
    "        import os, sys\n",
    "        sys.path.extend([os.path.dirname(os.path.abspath('.'))])\n",
    "\n",
    "    if plot_inline:\n",
    "        # Plots inside cells\n",
    "        %matplotlib inline\n",
    "    \n",
    "    global __file__\n",
    "    __file__ = 'Notebook'\n",
    "\n",
    "setup_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Constants Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import functools\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import sys\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import scipy\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "import egosocial\n",
    "import egosocial.config\n",
    "from egosocial.core.attributes import AttributeSelector\n",
    "from egosocial.core.models import create_model_top_down\n",
    "from egosocial.core.models import create_model_bottom_up\n",
    "from egosocial.utils.filesystem import create_directory \n",
    "from egosocial.utils.filesystem import check_directory\n",
    "from egosocial.utils.keras.autolosses import AutoMultiLossWrapper\n",
    "from egosocial.utils.keras.backend import limit_gpu_allocation_tensorflow\n",
    "from egosocial.utils.keras.callbacks import PlotLearning\n",
    "from egosocial.utils.keras.metrics import precision\n",
    "from egosocial.utils.keras.metrics import recall\n",
    "from egosocial.utils.keras.metrics import fmeasure\n",
    "from egosocial.utils.keras.processing import TimeSeriesDataGenerator\n",
    "from egosocial.utils.logging import setup_logging\n",
    "from egosocial.utils.sklearn.model_selection import StratifiedGroupShuffleSplitWrapper\n",
    "from egosocial.utils.misc import RELATIONS, DOMAINS\n",
    "from egosocial.utils.misc import LabelExpander\n",
    "from egosocial.utils.misc import decode_prediction\n",
    "from egosocial.utils.misc import relation_to_domain_vec\n",
    "\n",
    "SHARED_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit GPU memory allocation with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_memory = True\n",
    "if limit_memory and K.backend() == 'tensorflow':\n",
    "    memory_ratio = 0.3\n",
    "    limit_gpu_allocation_tensorflow(memory_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def main(*fake_args):\n",
    "    entry_msg = 'Train model for social relations classification in egosocial photo-streams.'\n",
    "    parser = argparse.ArgumentParser(description=entry_msg)\n",
    "\n",
    "    parser.add_argument('--dataset_path', required=True,\n",
    "                        help='Path to file containing the input data and labels information merged.')\n",
    "\n",
    "    parser.add_argument('--features_dir', required=True,\n",
    "                        help='Directory where the extracted features are stored.')\n",
    "    \n",
    "    parser.add_argument('--reuse_model', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Use precomputed model if available.')\n",
    "\n",
    "    parser.add_argument('--save_model', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Save model to disk.')\n",
    "\n",
    "    parser.add_argument('--save_stats', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Save statistics to disk.')\n",
    "\n",
    "    parser.add_argument('--epochs', required=False, type=int,\n",
    "                        default=30,\n",
    "                        help='Max number of epochs.')\n",
    "\n",
    "    parser.add_argument('--batch_size', required=False, type=int,\n",
    "                        default=32,\n",
    "                        help='Batch size.')\n",
    "    \n",
    "    parser.add_argument('--lr', required=False, type=float,\n",
    "                        default=0.001,\n",
    "                        help='Initial learning rate.')\n",
    "    \n",
    "    if not os.path.isdir(egosocial.config.TMP_DIR):\n",
    "        os.mkdir(egosocial.config.TMP_DIR)\n",
    "\n",
    "    setup_logging(egosocial.config.LOGGING_CONFIG,\n",
    "                  log_dir=egosocial.config.LOGS_DIR)\n",
    "    \n",
    "    # TODO: implement correctly\n",
    "    args = parser.parse_args(*fake_args)\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_day(image_path):\n",
    "    image_name = os.path.basename(image_path)\n",
    "    # a valid image follows the day_hour_x.ext format\n",
    "    day_hour_rest = image_name.split('_')\n",
    "    \n",
    "    if len(day_hour_rest) == 3:\n",
    "        # day is the first item\n",
    "        return day_hour_rest[0]\n",
    "    else:\n",
    "        # day isn't available\n",
    "        return ''\n",
    "    \n",
    "def load_dataset_defition(dataset_path, include_day=True):\n",
    "    with open(dataset_path, 'r') as json_file:\n",
    "        dataset_def = json.load(json_file)\n",
    "\n",
    "    # flatten the segments structure\n",
    "    samples = pd.DataFrame(list(itertools.chain(*dataset_def)))\n",
    "    \n",
    "    if include_day:\n",
    "        samples['day'] = samples['global_image_path'].apply(parse_day)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def load_features(features_path, data_frames):\n",
    "    features = np.load(features_path)\n",
    "    sequences_info = data_frames.groupby(['split', 'segment_id', 'group_id'])    \n",
    "\n",
    "    feature_sequences = []\n",
    "    for seq_id, group in sequences_info:\n",
    "        feature_seq = features[group.index]\n",
    "        feature_seq.shape = (feature_seq.shape[0], -1)\n",
    "        feature_sequences.append(feature_seq)\n",
    "    \n",
    "    return np.asarray(feature_sequences)\n",
    "\n",
    "def load_fields(data_frames, fields, valid_frames_idx=None):\n",
    "    assert len(fields) > 0\n",
    "    \n",
    "    if valid_frames_idx is None:\n",
    "        sequences_info = data_frames.groupby(['split', 'segment_id', 'group_id'])\n",
    "    else:\n",
    "        sequences_info = data_frames[valid_frames_idx].groupby(['split', 'segment_id', 'group_id'])\n",
    "    \n",
    "    fst_seq_frames = [group.index[0] for _, group in sequences_info]\n",
    "    fields_data =  data_frames.iloc[fst_seq_frames][fields].values\n",
    "\n",
    "    return [fields_data[:, field_idx] for field_idx in range(len(fields))]\n",
    "        \n",
    "def compute_stats(y, y_predicted):\n",
    "    acc = sklearn.metrics.accuracy_score(y, y_predicted)\n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(y, y_predicted)\n",
    "    report = sklearn.metrics.classification_report(y, y_predicted)\n",
    "\n",
    "    return acc, confusion_matrix, report\n",
    "\n",
    "def print_statistics(val_stats=None, test_stats=None, fdesc=sys.stdout):\n",
    "    for description, stats in [('Validation set:', val_stats), (('Test set:', test_stats))]:\n",
    "        \n",
    "        if stats is not None:\n",
    "            print(description, file=fdesc)\n",
    "            accuracy, confusion_matrix, report = stats\n",
    "            print('Confusion matrix:', file=fdesc)\n",
    "            print(confusion_matrix, file=fdesc)\n",
    "            print(file=fdesc)\n",
    "            print(report, file=fdesc)\n",
    "            print('Accuracy: {:.3f}'.format(accuracy), file=fdesc)\n",
    "            print('------------------------------------------------', file=fdesc)\n",
    "            \n",
    "def compute_class_frequency(y, index=None):\n",
    "    if index:\n",
    "        y =  y[index]\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    freq = counts / len(y)\n",
    "    return freq\n",
    "\n",
    "def compute_class_weight_labels(y, mode=None):\n",
    "    if mode == 'both_fused':\n",
    "        raise NotImplementedError('Class weights are not available in mode both_fused.')\n",
    "\n",
    "    encoder = LabelExpander(mode=mode)\n",
    "    y = encoder(y)\n",
    "\n",
    "    class_weight = {}\n",
    "    for y_type, y_data in y.items():\n",
    "        data = list(np.argmax(y_data, axis=1))    \n",
    "        classes = np.unique(data)\n",
    "        weights = compute_class_weight('balanced', classes, data)\n",
    "        class_weight[y_type] = dict(zip(classes, weights))\n",
    "    \n",
    "    return class_weight\n",
    "                \n",
    "class DimReductionTransformer(object):        \n",
    "\n",
    "    def __init__(self, n_components, Q=32, normalize=True, random_state=None):\n",
    "        # PCA configuration (number of components or min explained variance)\n",
    "        self.pca_param = n_components\n",
    "         # features quantization (smaller Q promotes sparsity)\n",
    "        self.Q = Q\n",
    "        self.normalize = normalize\n",
    "        self.random_state = sklearn.utils.check_random_state(random_state)\n",
    "        \n",
    "        self._scaler = None\n",
    "        self._pca = None\n",
    "        \n",
    "        self._log = logging.getLogger(self.__class__.__name__)\n",
    "        \n",
    "    def fit(self, x):\n",
    "        # reset state\n",
    "        self._scaler = None\n",
    "        self._pca = None        \n",
    "    \n",
    "        if self.normalize:\n",
    "            if self.Q: # quantization requires data in range [0, 1] \n",
    "                self._scaler = Normalizer(norm='l2')\n",
    "            else:\n",
    "                self._scaler = StandardScaler()\n",
    "            \n",
    "            self._log.debug('Fitting data normalization.')            \n",
    "            x = self._scaler.fit_transform(x)            \n",
    "            \n",
    "        if self.Q:\n",
    "            # small Q promotes sparsity\n",
    "            x = np.floor(self.Q * x)\n",
    "\n",
    "        self._log.debug('Fitting PCA.')        \n",
    "        assert self.pca_param > 0     \n",
    "        # compute pca from scratch\n",
    "        if 0 < self.pca_param <= 1:\n",
    "            # running pca with min explained variance takes much longer\n",
    "            self._pca = PCA(self.pca_param, random_state=self.random_state)\n",
    "        else:\n",
    "            self._pca = PCA(self.pca_param, svd_solver='randomized', random_state=self.random_state)\n",
    "        \n",
    "        self._pca.fit(x)\n",
    "        \n",
    "    def transform(self, x):\n",
    "        if self.normalize:\n",
    "            self._log.debug('Applying data normalization')\n",
    "            x = self._scaler.transform(x)\n",
    "\n",
    "        if self.Q:\n",
    "            # small Q promotes sparsity\n",
    "            self._log.debug('Applying Q-sparsity Q={}'.format(self.Q))\n",
    "            x = np.floor(self.Q * x)            \n",
    "\n",
    "        explained_var = np.sum(self._pca.explained_variance_ratio_)\n",
    "        n_components = self._pca.n_components_\n",
    "        msg = 'Applying PCA with explained var {} dims {}'\n",
    "        self._log.debug(msg.format(explained_var, n_components))\n",
    "        # pca transformation\n",
    "        x = self._pca.transform(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialClassifierWithPreComputedFeatures:\n",
    "    \n",
    "    def __init__(self, dataset_path, features_dir, test_size=0.2, k_fold_splits=10, val_size=None, n_components=50, Q=32, seed=42):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.features_dir = features_dir\n",
    "        self.test_size = test_size\n",
    "        self.k_fold_splits = k_fold_splits\n",
    "        self.val_size = val_size if val_size else 1.0 / k_fold_splits\n",
    "        self.n_components = n_components\n",
    "        self.Q = Q\n",
    "        self.seed = seed\n",
    "\n",
    "        self._frames = None\n",
    "        self._grouped_frames = None\n",
    "        self._labels = None\n",
    "        self._users = None\n",
    "        self._attribute_features = None\n",
    "        self._train_idx = None\n",
    "        self._test_idx = None\n",
    "        self._k_train_val_idx = None\n",
    "        \n",
    "        self._log = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def load_data(self):        \n",
    "        # load dataset definition\n",
    "        frames = load_dataset_defition(self.dataset_path, include_day=True)\n",
    "        # filter labels with few samples\n",
    "        valid_frames_idx = np.isin(frames['relation_label'], RELATIONS)\n",
    "\n",
    "        self._frames = frames[valid_frames_idx]\n",
    "        self._grouped_frames = self._frames.groupby(['split', 'segment_id', 'group_id'])\n",
    "\n",
    "        # for each sequence get label, user, day of first frame\n",
    "        self._labels, self._users, self._seq_days = load_fields(\n",
    "            frames, ['relation_label', 'camera_user_name', 'day'], \n",
    "            valid_frames_idx=valid_frames_idx\n",
    "        )\n",
    "        \n",
    "        self._init_features(self._frames)\n",
    "        self._init_grouped_splits()\n",
    "        self._dim_reduction = defaultdict(dict)\n",
    "\n",
    "    def _init_features(self, valid_frames):\n",
    "        attributes = sorted(next(os.walk(self.features_dir))[2])\n",
    "        attributes = [attr for attr in attributes if attr.endswith('.npy')]\n",
    "\n",
    "        # load features\n",
    "        self._attribute_features = {}\n",
    "        for attribute_file in attributes:\n",
    "            attribute_name, ext = os.path.splitext(attribute_file)\n",
    "            features_path = os.path.join(self.features_dir, attribute_file)\n",
    "            self._attribute_features[attribute_name] = load_features(features_path, valid_frames)        \n",
    "        \n",
    "    def _init_stratified_splits(self):\n",
    "        # define data splits\n",
    "        # define train, test splits\n",
    "        sss = StratifiedShuffleSplit(test_size=self.test_size, random_state=self.seed)\n",
    "        y = self._labels\n",
    "        self._train_idx, self._test_idx = next(sss.split(np.zeros(len(y)), y))  \n",
    "\n",
    "        # define k-fold train splits\n",
    "        skf = StratifiedKFold(n_splits=self.k_fold_splits, random_state=self.seed)\n",
    "        y = np.array(y[self._train_idx])\n",
    "        \n",
    "        self._k_train_val_idx = []\n",
    "        for t_idx, v_idx in skf.split(np.zeros(len(y)), y):\n",
    "            self._k_train_val_idx.append((self._train_idx[t_idx], self._train_idx[v_idx]))\n",
    "\n",
    "    def _init_grouped_splits(self):\n",
    "        # define data splits\n",
    "        # define train, test splits        \n",
    "        criteria = np.array([ user + '_' + day for user, day in zip(self._users, self._seq_days) ])\n",
    "        y, groups = helper._labels, criteria\n",
    "\n",
    "        n_tries, group_size, epsilon = 1000, self.test_size, 0.025\n",
    "\n",
    "        split_wrapper = StratifiedGroupShuffleSplitWrapper(\n",
    "            GroupShuffleSplit(n_splits=n_tries, test_size=group_size, random_state=self.seed), \n",
    "            n_splits=1, \n",
    "            max_test_size=min(self.test_size + epsilon, 1.0), min_test_size=max(self.test_size - epsilon, 0.0)\n",
    "        )\n",
    "        self._train_idx, self._test_idx, train_test_score = next(split_wrapper.split(np.zeros(len(y)), y, groups, return_score=True))\n",
    "        test_size = len(self._test_idx) / (len(self._train_idx) + len(self._test_idx))\n",
    "        self._log.debug('Split train-test score: {:.3} real_test_size: {:.3}'.format(train_test_score, test_size))\n",
    "        \n",
    "        # define k-fold splits\n",
    "        y, groups = y[self._train_idx], groups[self._train_idx]\n",
    "        \n",
    "        if self.k_fold_splits > 1:\n",
    "            # k-fold strategy\n",
    "            # search 50 times the number of splits, encourage diversity\n",
    "            # double the epsilon (more flexible)\n",
    "            n_tries, group_size, epsilon = self.k_fold_splits * 50, self.val_size, 0.05\n",
    "        else:\n",
    "            # holdout strategy\n",
    "            # keep n_tries and epsilon same as train-test split\n",
    "            group_size = self.val_size\n",
    "        \n",
    "        split_wrapper = StratifiedGroupShuffleSplitWrapper(\n",
    "            GroupShuffleSplit(n_splits=n_tries, test_size=group_size, random_state=self.seed), \n",
    "            n_splits=self.k_fold_splits,\n",
    "            max_test_size=min(self.val_size + epsilon, 1.0), min_test_size=max(self.val_size - epsilon, 0.0)\n",
    "        )\n",
    "        \n",
    "        self._k_train_val_idx = []\n",
    "        for k, (t_idx, v_idx, t_v_score) in enumerate(split_wrapper.split(np.zeros(len(y)), y, groups, return_score=True)):\n",
    "            self._k_train_val_idx.append((self._train_idx[t_idx], self._train_idx[v_idx]))\n",
    "            val_size = (1 - test_size) * len(v_idx) / (len(t_idx) + len(v_idx))\n",
    "            self._log.debug('{}-fold split score: {:.3} real_val_size={:.3}'.format(k, t_v_score, val_size))            \n",
    "            \n",
    "    def list_attributes(self):\n",
    "        # list attributes\n",
    "        if self._attribute_features:\n",
    "            return sorted(self._attribute_features.keys())\n",
    "        else:\n",
    "            return []\n",
    "            \n",
    "    def _get_split_idx(self, split, k_fold=None):\n",
    "        assert split in ('train', 'test', 'val')\n",
    "        if split == 'train':\n",
    "            if k_fold is None:\n",
    "                return self._train_idx\n",
    "            else:\n",
    "                assert 0 <= k_fold < self.k_fold_splits\n",
    "                return self._k_train_val_idx[k_fold][0]\n",
    "\n",
    "        if split == 'val':\n",
    "            assert 0 <= k_fold < self.k_fold_splits\n",
    "            return self._k_train_val_idx[k_fold][1]\n",
    "        \n",
    "        if split == 'test':\n",
    "            return self._test_idx\n",
    "        \n",
    "    def get_k_train_val_split(self, selected_attributes, k_fold, preprocess_mask=None):\n",
    "        assert 0 <= k_fold < self.k_fold_splits\n",
    "        return self._get_train_test_val_split(selected_attributes, k_fold=k_fold, preprocess_mask=preprocess_mask)\n",
    "        \n",
    "    def get_train_test_split(self, selected_attributes, preprocess_mask=None):\n",
    "        return self._get_train_test_val_split(selected_attributes, preprocess_mask=preprocess_mask)\n",
    "    \n",
    "    def _get_train_test_val_split(self, selected_attributes, k_fold=None, preprocess_mask=None):\n",
    "        # get data splits composed by selected attributes only\n",
    "        # preprocess the data\n",
    "\n",
    "        # preconditions\n",
    "        assert self._attribute_features is not None\n",
    "        assert self._labels is not None\n",
    "        assert len(selected_attributes) > 0 \n",
    "        assert preprocess_mask is None or len(preprocess_mask) == len(selected_attributes)\n",
    "        min_dim_for_pca = self.n_components\n",
    "        \n",
    "        if k_fold is None:\n",
    "            train_idx, test_idx = self._get_split_idx('train'), self._get_split_idx('test')\n",
    "            k_fold = -1 # see DimReductionTransformer init below\n",
    "        else:\n",
    "            assert 0 <= k_fold < self.k_fold_splits\n",
    "            train_idx, test_idx = self._get_split_idx('train', k_fold), self._get_split_idx('val', k_fold)\n",
    "            \n",
    "        # preprocess each selected attribute individually\n",
    "        train_features_list, test_features_list = [], []\n",
    "        for attr_i, attr in enumerate(selected_attributes):            \n",
    "            train_features = self._attribute_features[attr][train_idx]\n",
    "            train_features = list(itertools.chain(*train_features))\n",
    "            \n",
    "            test_features = self._attribute_features[attr][test_idx]\n",
    "            test_features = list(itertools.chain(*test_features))\n",
    "            \n",
    "            n_features = train_features[0].shape[-1]\n",
    "            if preprocess_mask and preprocess_mask[attr_i]:\n",
    "                self._log.debug('Preprocessing {}.'.format(attr))\n",
    "                # initialize transformation for each attribute\n",
    "                if min_dim_for_pca <= n_features:\n",
    "                    # init transformation with different (fixed) seed for each fold\n",
    "                    transformation = DimReductionTransformer(n_components=self.n_components, Q=self.Q, random_state=self.seed + k_fold)\n",
    "                else:\n",
    "                    transformation = StandardScaler()\n",
    "\n",
    "                transformation.fit(train_features)\n",
    "            \n",
    "                train_features = transformation.transform(train_features)\n",
    "                test_features = transformation.transform(test_features)\n",
    "            else:\n",
    "                self._log.debug('Skip preprocessing {}.'.format(attr))\n",
    "\n",
    "            train_features_list.append(train_features)\n",
    "            test_features_list.append(test_features)\n",
    "\n",
    "        attr = selected_attributes[0]\n",
    "        # create sequence structure for train\n",
    "        train_features_fused = np.concatenate(train_features_list, axis=-1)\n",
    "        train_seq_len = map(len, self._attribute_features[attr][train_idx])\n",
    "        train_features_as_seqs = []\n",
    "        offset = 0\n",
    "        for seq_len in train_seq_len:        \n",
    "            seq_range = range(offset, offset+seq_len)\n",
    "            train_features_as_seqs.append(train_features_fused[seq_range])\n",
    "            offset += seq_len\n",
    "        \n",
    "        # create sequence structure for testtrain\n",
    "        test_features_fused = np.concatenate(test_features_list, axis=-1)\n",
    "        test_seq_len = map(len, self._attribute_features[attr][test_idx])\n",
    "        test_features_as_seqs = []\n",
    "        offset = 0\n",
    "        for seq_len in test_seq_len:        \n",
    "            seq_range = range(offset, offset+seq_len)\n",
    "            test_features_as_seqs.append(test_features_fused[seq_range])\n",
    "            offset += seq_len\n",
    "        \n",
    "        return (train_features_as_seqs, self._labels[train_idx]), (test_features_as_seqs, self._labels[test_idx])\n",
    "    \n",
    "    def max_sequence_len(self):\n",
    "        return self._grouped_frames.size().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_callbacks(output_mode, plot_stats=True, save_model=False, save_stats=False, stop_early=False, reduce_lr=False, figsize=None):\n",
    "    callbacks = []\n",
    "\n",
    "    training_dir = os.path.join(egosocial.config.TMP_DIR, 'training')\n",
    "    create_directory(training_dir, 'Training')\n",
    "\n",
    "    if save_model:\n",
    "        checkpoint_path = os.path.join(training_dir,\n",
    "                                       'weights.{epoch:02d}-{val_loss:.2f}.h5')\n",
    "        checkpointer = ModelCheckpoint( \n",
    "            filepath=checkpoint_path, monitor='val_loss',\n",
    "            save_best_only=True, period=5,\n",
    "        )\n",
    "        callbacks.append(checkpointer)\n",
    "\n",
    "    if save_stats:\n",
    "        metrics_path = os.path.join(training_dir,\n",
    "                                    'metrics.csv')\n",
    "        csv_logger = CSVLogger(metrics_path)\n",
    "        callbacks.append(csv_logger)\n",
    "\n",
    "    if reduce_lr:\n",
    "        lr_handler = ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001\n",
    "        )\n",
    "        callbacks.append(lr_handler)\n",
    "   \n",
    "    if plot_stats:\n",
    "        # more plots need more space\n",
    "        if not figsize:\n",
    "            if output_mode != 'both_splitted':\n",
    "                figsize = (25, 5)\n",
    "            else:\n",
    "                figsize = (25, 13)\n",
    "\n",
    "        plot_metrics = PlotLearning(update_step=5, figsize=figsize)\n",
    "        callbacks.append(plot_metrics)\n",
    "        \n",
    "    if stop_early:\n",
    "        stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, mode='auto')\n",
    "        callbacks.append(stopper)\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "def compile_model(\n",
    "    model, \n",
    "    optimizer='adam', \n",
    "    loss='categorical_crossentropy', \n",
    "    loss_weights='auto',\n",
    "    **kwargs\n",
    "):\n",
    "    # wrapper allows to train the loss weights\n",
    "    model_wrapper = AutoMultiLossWrapper(model)\n",
    "    model_wrapper.compile(optimizer=optimizer, loss=loss, \n",
    "                          loss_weights=loss_weights, **kwargs)\n",
    "\n",
    "    return model_wrapper.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.join(egosocial.config.TMP_DIR, 'egocentric', 'datasets')\n",
    "\n",
    "args = [\n",
    "    \"--dataset_path\", os.path.join(BASE_DIR, 'merged_dataset.json'),\n",
    "    \"--features_dir\", os.path.join(BASE_DIR, 'extracted_features'),\n",
    "]\n",
    "\n",
    "conf = main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading precomputed features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper = SocialClassifierWithPreComputedFeatures(\n",
    "    conf.dataset_path, conf.features_dir, \n",
    "    test_size=0.2, \n",
    "    k_fold_splits=3, \n",
    "    val_size=0.2, \n",
    "    n_components=50, \n",
    "    Q=32,\n",
    "    seed=SHARED_SEED\n",
    ")\n",
    "\n",
    "helper.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = []\n",
    "for user in np.unique(helper._users):\n",
    "    user_labels = helper._labels[helper._users == user]\n",
    "    unique_user_labels = np.unique(user_labels)\n",
    "    stats.append({'User':user, '# samples': len(user_labels), '# labels:': len(unique_user_labels),  'labels' : unique_user_labels})\n",
    "stats = pd.DataFrame(stats)\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select attributes (default all), prepare output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_selector = AttributeSelector(helper.list_attributes())\n",
    "\n",
    "# all / face / body / or single attribute (accept name substring, e.g. activity)\n",
    "attributes_query = 'all'\n",
    "# expand all / face / body / single attribute\n",
    "selected_attributes = attribute_selector.filter(attributes_query)\n",
    "print('Selected attribute(s): {}'.format(selected_attributes))\n",
    "\n",
    "to_process = lambda attr : attr not in ('camera_user_age', 'camera_user_gender')\n",
    "mask = [to_process(attr) for attr in selected_attributes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.Q = 32\n",
    "helper.n_components = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_fold=0\n",
    "# prepare splits for selected attributes\n",
    "(k_x_train, k_y_train), (k_x_val, k_y_val) = helper.get_k_train_val_split(selected_attributes, k_fold=k_fold, preprocess_mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_val = len(k_x_train), len(k_x_val)\n",
    "n_features = k_x_train[0].shape[-1]\n",
    "max_timestep = helper.max_sequence_len()\n",
    "batch_size = 64\n",
    "\n",
    "print('Length of the largest sequence:', max_timestep)\n",
    "print('Number of features:', n_features)\n",
    "print('Number of samples fold={}. Training: {} / Validation {}'.format(k_fold, n_train, n_val))\n",
    "print('Batch size: {}'.format(batch_size))\n",
    "\n",
    "#output_mode = 'both_fused' # domain-relation outputs fused\n",
    "#output_mode = 'both_splitted' # multi-loss domain-relation\n",
    "output_mode = 'domain' # domain only\n",
    "#output_mode = 'relation' # relation only\n",
    "\n",
    "print('Output mode: {}'.format(output_mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use domain specific labels\n",
    "if output_mode == 'domain':\n",
    "    k_y_train = relation_to_domain_vec(k_y_train)\n",
    "    k_y_val = relation_to_domain_vec(k_y_val)    \n",
    "    \n",
    "# class_weight for keras (balance domain/relation instances)\n",
    "class_weight = compute_class_weight_labels(k_y_train, mode=output_mode)\n",
    "\n",
    "if 'domain' in class_weight:\n",
    "    pprint.pprint(('domains', [(domain, class_weight['domain'][idx]) for idx, domain in enumerate(DOMAINS)]))\n",
    "    \n",
    "if 'relation' in class_weight:\n",
    "    pprint.pprint(('relations', [(relation, class_weight['relation'][idx]) for idx, relation in enumerate(RELATIONS)]))\n",
    "    \n",
    "process_labels = LabelExpander(mode=output_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0002\n",
    "print('LR: {}'.format(learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_strategy = 'top_down'\n",
    "model_strategy_select = {\n",
    "    'top_down' : create_model_top_down,\n",
    "    'bottom_up' : create_model_bottom_up,\n",
    "}\n",
    "\n",
    "model_parameters = dict(\n",
    "    max_seq_len=max_timestep,\n",
    "    n_features=n_features,\n",
    "    mode=output_mode,\n",
    "    units=96,\n",
    "    recurrent_type='GRU',\n",
    "    drop_rate=0.5,\n",
    "    rec_drop_rate=0.5,\n",
    "    l2_reg=0.01,\n",
    "    hidden_fc=0,\n",
    "    n_relations=len(RELATIONS),\n",
    "    n_domains=len(DOMAINS),  \n",
    ")\n",
    "    \n",
    "model = model_strategy_select[model_strategy](**model_parameters)\n",
    "if model_strategy_select == 'bottom_up':\n",
    "    model.get_layer('domain').set_weights([relation_to_domain_weights()])\n",
    "\n",
    "model = compile_model(\n",
    "    model,\n",
    "    loss='categorical_crossentropy',    \n",
    "    optimizer=keras.optimizers.Adam(learning_rate, decay=1e-5),\n",
    "    metrics=['accuracy', recall, fmeasure],\n",
    "    loss_weights='auto',\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "callbacks = init_callbacks(output_mode, plot_stats=True, stop_early=True, figsize=(25, 13))\n",
    "\n",
    "train_datagen = TimeSeriesDataGenerator(fancy_pca=True, noise_stddev=0.01, random_state=SHARED_SEED)\n",
    "train_datagen.fit(k_x_train)\n",
    "\n",
    "val_datagen = TimeSeriesDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "    k_x_train, k_y_train,\n",
    "    maxlen=max_timestep,\n",
    "    output_cbk=process_labels,\n",
    "    batch_size=batch_size,\n",
    "    seed=SHARED_SEED,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_batch_size = n_val\n",
    "val_generator = val_datagen.flow(\n",
    "    k_x_val, k_y_val,\n",
    "    maxlen=max_timestep,\n",
    "    output_cbk=process_labels,\n",
    "    batch_size=val_batch_size,\n",
    "    seed=SHARED_SEED,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_epochs, epochs = 0, 400\n",
    "print('Initial epoch: {} Number of epochs: {}'.format(cache_epochs, epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(1.0 * n_train / batch_size),\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=np.ceil(1.0 * n_val / val_batch_size),\n",
    "    epochs=cache_epochs + epochs,\n",
    "    initial_epoch=cache_epochs,\n",
    "    callbacks=callbacks,\n",
    "    verbose=0,\n",
    "    max_queue_size=10,\n",
    "    workers=4,\n",
    "    class_weight=class_weight,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val_generator = val_datagen.flow(\n",
    "    k_x_val, k_y_val, \n",
    "    maxlen=max_timestep, \n",
    "    output_cbk=process_labels, \n",
    "    batch_size=n_val,\n",
    "    seed=SHARED_SEED,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "k_y_val_predicted = model.predict_generator(val_generator, steps=1)\n",
    "\n",
    "decoded_prediction = decode_prediction(k_y_val_predicted, mode=output_mode)\n",
    "\n",
    "if 'domain' in decoded_prediction:\n",
    "    print_statistics(val_stats=compute_stats(k_y_val, decoded_prediction['domain']))\n",
    "\n",
    "if 'relation' in decoded_prediction:\n",
    "    print_statistics(val_stats=compute_stats(k_y_val,  decoded_prediction['relation']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition and graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
