{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_notebook(fix_python_path=True, reduce_margins=True, plot_inline=True):\n",
    "    if reduce_margins:\n",
    "        # Reduce side margins of the notebook\n",
    "        from IPython.core.display import display, HTML\n",
    "        display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "    if fix_python_path:\n",
    "        # add egosocial to the python path\n",
    "        import os, sys\n",
    "        sys.path.extend([os.path.dirname(os.path.abspath('.'))])\n",
    "\n",
    "    if plot_inline:\n",
    "        # Plots inside cells\n",
    "        %matplotlib inline\n",
    "    \n",
    "    global __file__\n",
    "    __file__ = 'Notebook'\n",
    "\n",
    "setup_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Constants Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import functools\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import pprint\n",
    "import sys\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import scipy\n",
    "\n",
    "SHARED_SEED = 42\n",
    "np.random.seed(SHARED_SEED)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import egosocial\n",
    "import egosocial.config\n",
    "from egosocial.core.attributes import AttributeSelector\n",
    "from egosocial.core.models import create_model_top_down\n",
    "from egosocial.core.models import create_model_bottom_up\n",
    "from egosocial.core.models import create_model_independent_outputs\n",
    "from egosocial.utils.filesystem import create_directory \n",
    "from egosocial.utils.filesystem import check_directory\n",
    "from egosocial.utils.keras.autolosses import AutoMultiLossWrapper\n",
    "from egosocial.utils.keras.backend import limit_gpu_allocation_tensorflow\n",
    "from egosocial.utils.keras.callbacks import PlotLearning\n",
    "from egosocial.utils.keras.metrics import precision\n",
    "from egosocial.utils.keras.metrics import recall\n",
    "from egosocial.utils.keras.metrics import fmeasure\n",
    "from egosocial.utils.keras.processing import TimeSeriesDataGenerator\n",
    "from egosocial.utils.logging import setup_logging\n",
    "from egosocial.utils.sklearn.model_selection import StratifiedGroupShuffleSplitWrapper\n",
    "from egosocial.utils.misc import RELATIONS, DOMAINS\n",
    "from egosocial.utils.misc import LabelExpander\n",
    "from egosocial.utils.misc import decode_prediction\n",
    "from egosocial.utils.misc import relation_to_domain_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit GPU memory allocation with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_memory = True\n",
    "if limit_memory and K.backend() == 'tensorflow':\n",
    "    memory_ratio = 0.3\n",
    "    limit_gpu_allocation_tensorflow(memory_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def main(*fake_args):\n",
    "    entry_msg = 'Train model for social relations classification in egosocial photo-streams.'\n",
    "    parser = argparse.ArgumentParser(description=entry_msg)\n",
    "\n",
    "    parser.add_argument('--dataset_path', required=True,\n",
    "                        help='Path to file containing the input data and labels information merged.')\n",
    "\n",
    "    parser.add_argument('--features_dir', required=True,\n",
    "                        help='Directory where the extracted features are stored.')\n",
    "    \n",
    "    parser.add_argument('--reuse_model', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Use precomputed model if available.')\n",
    "\n",
    "    parser.add_argument('--save_model', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Save model to disk.')\n",
    "\n",
    "    parser.add_argument('--save_stats', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Save statistics to disk.')\n",
    "\n",
    "    parser.add_argument('--epochs', required=False, type=int,\n",
    "                        default=30,\n",
    "                        help='Max number of epochs.')\n",
    "\n",
    "    parser.add_argument('--batch_size', required=False, type=int,\n",
    "                        default=32,\n",
    "                        help='Batch size.')\n",
    "    \n",
    "    parser.add_argument('--lr', required=False, type=float,\n",
    "                        default=0.001,\n",
    "                        help='Initial learning rate.')\n",
    "    \n",
    "    if not os.path.isdir(egosocial.config.TMP_DIR):\n",
    "        os.mkdir(egosocial.config.TMP_DIR)\n",
    "\n",
    "    setup_logging(egosocial.config.LOGGING_CONFIG,\n",
    "                  log_dir=egosocial.config.LOGS_DIR)\n",
    "    \n",
    "    # TODO: implement correctly\n",
    "    args = parser.parse_args(*fake_args)\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_day(image_path):\n",
    "    image_name = os.path.basename(image_path)\n",
    "    # a valid image follows the day_hour_x.ext format\n",
    "    day_hour_rest = image_name.split('_')\n",
    "    \n",
    "    if len(day_hour_rest) == 3:\n",
    "        # day is the first item\n",
    "        return day_hour_rest[0]\n",
    "    else:\n",
    "        # day isn't available\n",
    "        return ''\n",
    "    \n",
    "def load_dataset_defition(dataset_path, include_day=True):\n",
    "    with open(dataset_path, 'r') as json_file:\n",
    "        dataset_def = json.load(json_file)\n",
    "\n",
    "    # flatten the segments structure\n",
    "    samples = pd.DataFrame(list(itertools.chain(*dataset_def)))\n",
    "    \n",
    "    if include_day:\n",
    "        samples['day'] = samples['global_image_path'].apply(parse_day)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def load_features(features_path, data_frames):\n",
    "    features = np.load(features_path)\n",
    "    sequences_info = data_frames.groupby(['split', 'segment_id', 'group_id'])    \n",
    "\n",
    "    feature_sequences = []\n",
    "    for seq_id, group in sequences_info:\n",
    "        feature_seq = features[group.index]\n",
    "        feature_seq.shape = (feature_seq.shape[0], -1)\n",
    "        feature_sequences.append(feature_seq)\n",
    "    \n",
    "    return np.asarray(feature_sequences)\n",
    "\n",
    "def load_fields(data_frames, fields, valid_frames_idx=None):\n",
    "    assert len(fields) > 0\n",
    "    \n",
    "    if valid_frames_idx is None:\n",
    "        sequences_info = data_frames.groupby(['split', 'segment_id', 'group_id'])\n",
    "    else:\n",
    "        sequences_info = data_frames[valid_frames_idx].groupby(['split', 'segment_id', 'group_id'])\n",
    "    \n",
    "    fst_seq_frames = [group.index[0] for _, group in sequences_info]\n",
    "    fields_data =  data_frames.iloc[fst_seq_frames][fields].values\n",
    "\n",
    "    return [fields_data[:, field_idx] for field_idx in range(len(fields))]\n",
    "            \n",
    "def compute_class_frequency(y, index=None):\n",
    "    if index:\n",
    "        y =  y[index]\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    freq = counts / len(y)\n",
    "    return freq\n",
    "\n",
    "def compute_class_weight_labels(y, mode=None):\n",
    "    if mode == 'both_fused':\n",
    "        raise NotImplementedError('Class weights are not available in mode both_fused.')\n",
    "\n",
    "    encoder = LabelExpander(mode=mode)\n",
    "    y = encoder(y)\n",
    "\n",
    "    class_weight = {}\n",
    "    for y_type, y_data in y.items():\n",
    "        data = list(np.argmax(y_data, axis=1))    \n",
    "        classes = np.unique(data)\n",
    "        weights = compute_class_weight('balanced', classes, data)\n",
    "        class_weight[y_type] = dict(zip(classes, weights))\n",
    "    \n",
    "    return class_weight\n",
    "                \n",
    "class DimReductionTransformer(object):        \n",
    "\n",
    "    def __init__(self, n_components, Q=32, normalize=True, random_state=None):\n",
    "        # PCA configuration (number of components or min explained variance)\n",
    "        self.pca_param = n_components\n",
    "         # features quantization (smaller Q promotes sparsity)\n",
    "        self.Q = Q\n",
    "        self.normalize = normalize\n",
    "        self.random_state = sklearn.utils.check_random_state(random_state)\n",
    "        \n",
    "        self._scaler = None\n",
    "        self._pca = None\n",
    "        \n",
    "        self._log = logging.getLogger(self.__class__.__name__)\n",
    "        \n",
    "    def fit(self, x):\n",
    "        # reset state\n",
    "        self._scaler = None\n",
    "        self._pca = None        \n",
    "    \n",
    "        if self.normalize:\n",
    "            if self.Q: # quantization requires data in range [0, 1] \n",
    "                self._scaler = Normalizer(norm='l2')\n",
    "            else:\n",
    "                self._scaler = StandardScaler()\n",
    "            \n",
    "            self._log.debug('Fitting data normalization.')            \n",
    "            x = self._scaler.fit_transform(x)            \n",
    "            \n",
    "        if self.Q:\n",
    "            # small Q promotes sparsity\n",
    "            x = np.floor(self.Q * x)\n",
    "\n",
    "        self._log.debug('Fitting PCA.')        \n",
    "        assert self.pca_param > 0     \n",
    "        # compute pca from scratch\n",
    "        if 0 < self.pca_param <= 1:\n",
    "            # running pca with min explained variance takes much longer\n",
    "            self._pca = PCA(self.pca_param, random_state=self.random_state)\n",
    "        else:\n",
    "            self._pca = PCA(self.pca_param, svd_solver='randomized', random_state=self.random_state)\n",
    "        \n",
    "        self._pca.fit(x)\n",
    "        \n",
    "    def transform(self, x):\n",
    "        if self.normalize:\n",
    "            self._log.debug('Applying data normalization')\n",
    "            x = self._scaler.transform(x)\n",
    "\n",
    "        if self.Q:\n",
    "            # small Q promotes sparsity\n",
    "            self._log.debug('Applying Q-sparsity Q={}'.format(self.Q))\n",
    "            x = np.floor(self.Q * x)            \n",
    "\n",
    "        explained_var = np.sum(self._pca.explained_variance_ratio_)\n",
    "        n_components = self._pca.n_components_\n",
    "        msg = 'Applying PCA with explained var {} dims {}'\n",
    "        self._log.debug(msg.format(explained_var, n_components))\n",
    "        # pca transformation\n",
    "        x = self._pca.transform(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_classification_report(y_true, y_pred, labels=None, target_names=None,\n",
    "                                 sample_weight=None, digits=2,\n",
    "                                 weighted=True):\n",
    "    \"\"\"Build a text report showing the main classification metrics\n",
    "    Read more in the :ref:`User Guide <classification_report>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
    "        Estimated targets as returned by a classifier.\n",
    "    labels : array, shape = [n_labels]\n",
    "        Optional list of label indices to include in the report.\n",
    "    target_names : list of strings\n",
    "        Optional display names matching the labels (same order).\n",
    "    sample_weight : array-like of shape = [n_samples], optional\n",
    "        Sample weights.\n",
    "    digits : int\n",
    "        Number of digits for formatting output floating point values\n",
    "    Returns\n",
    "    -------\n",
    "    report : string\n",
    "        Text summary of the precision, recall, F1 score for each class.\n",
    "        The reported averages are a prevalence-weighted macro-average across\n",
    "        classes (equivalent to :func:`precision_recall_fscore_support` with\n",
    "        ``average='weighted'``).\n",
    "        Note that in binary classification, recall of the positive class\n",
    "        is also known as \"sensitivity\"; recall of the negative class is\n",
    "        \"specificity\".\n",
    "    Examples\n",
    "    --------\n",
    "    >>> y_true = [0, 1, 2, 2, 2]\n",
    "    >>> y_pred = [0, 0, 2, 2, 1]\n",
    "    >>> target_names = ['class 0', 'class 1', 'class 2']\n",
    "    >>> print(custom_classification_report(y_true, y_pred, target_names=target_names))\n",
    "                 precision    recall  f1-score   support\n",
    "    <BLANKLINE>\n",
    "        class 0       0.50      1.00      0.67         1\n",
    "        class 1       0.00      0.00      0.00         1\n",
    "        class 2       1.00      0.67      0.80         3\n",
    "    <BLANKLINE>\n",
    "    avg / total       0.70      0.60      0.61         5\n",
    "    <BLANKLINE>\n",
    "    \"\"\"\n",
    "\n",
    "    if labels is None:\n",
    "        labels = sklearn.utils.multiclass.unique_labels(y_true, y_pred)\n",
    "    else:\n",
    "        labels = np.asarray(labels)\n",
    "\n",
    "    if target_names is not None and len(labels) != len(target_names):\n",
    "        warnings.warn(\n",
    "            \"labels size, {0}, does not match size of target_names, {1}\"\n",
    "            .format(len(labels), len(target_names))\n",
    "        )\n",
    "\n",
    "    last_line_heading = 'avg / total'\n",
    "\n",
    "    if target_names is None:\n",
    "        target_names = [u'%s' % l for l in labels]\n",
    "    name_width = max(len(cn) for cn in target_names)\n",
    "    width = max(name_width, len(last_line_heading), digits)\n",
    "\n",
    "    headers = [\"precision\", \"recall\", \"f1-score\", \"support\"]\n",
    "    head_fmt = u'{:>{width}s} ' + u' {:>9}' * len(headers)\n",
    "    report = head_fmt.format(u'', *headers, width=width)\n",
    "    report += u'\\n\\n'\n",
    "\n",
    "    p, r, f1, s = sklearn.metrics.precision_recall_fscore_support(\n",
    "        y_true, y_pred,\n",
    "        labels=labels,\n",
    "        average=None,\n",
    "        sample_weight=sample_weight)\n",
    "\n",
    "    row_fmt = u'{:>{width}s} ' + u' {:>9.{digits}f}' * 3 + u' {:>9}\\n'\n",
    "    rows = zip(target_names, p, r, f1, s)\n",
    "    for row in rows:\n",
    "        report += row_fmt.format(*row, width=width, digits=digits)\n",
    "\n",
    "    report += u'\\n'\n",
    "\n",
    "    \n",
    "    if weighted:\n",
    "        s_weights = s\n",
    "    else:\n",
    "        s_weights = None\n",
    "    # compute averages\n",
    "    report += row_fmt.format(last_line_heading,\n",
    "                             np.average(p, weights=s_weights),\n",
    "                             np.average(r, weights=s_weights),\n",
    "                             np.average(f1, weights=s_weights),\n",
    "                             np.sum(s),\n",
    "                             width=width, digits=digits)\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "def compute_stats(y, y_predicted, weighted=True):\n",
    "    acc = sklearn.metrics.accuracy_score(y, y_predicted)\n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(y, y_predicted)\n",
    "    report = custom_classification_report(y, y_predicted, weighted=weighted, digits=4)\n",
    "\n",
    "    return acc, confusion_matrix, report\n",
    "\n",
    "def print_statistics(val_stats=None, test_stats=None, fdesc=sys.stdout):\n",
    "    for description, stats in [('Validation set:', val_stats), (('Test set:', test_stats))]:\n",
    "        \n",
    "        if stats is not None:\n",
    "            print(description, file=fdesc)\n",
    "            accuracy, confusion_matrix, report = stats\n",
    "            print('Confusion matrix:', file=fdesc)\n",
    "            print(confusion_matrix, file=fdesc)\n",
    "            print(file=fdesc)\n",
    "            print(report, file=fdesc)\n",
    "            print('Accuracy: {:.3f}'.format(accuracy), file=fdesc)\n",
    "            print('------------------------------------------------', file=fdesc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialClassifierWithPreComputedFeatures:\n",
    "    \n",
    "    def __init__(self, dataset_path, features_dir, test_size=0.2, k_fold_splits=10, val_size=None, n_components=50, Q=32, seed=42):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.features_dir = features_dir\n",
    "        self.test_size = test_size\n",
    "        self.k_fold_splits = k_fold_splits\n",
    "        self.val_size = val_size if val_size else 1.0 / k_fold_splits\n",
    "        self.n_components = n_components\n",
    "        self.Q = Q\n",
    "        self.seed = seed\n",
    "\n",
    "        self._frames = None\n",
    "        self._grouped_frames = None\n",
    "        self._labels = None\n",
    "        self._users = None\n",
    "        self._attribute_features = None\n",
    "        self._train_idx = None\n",
    "        self._test_idx = None\n",
    "        self._k_train_val_idx = None\n",
    "        \n",
    "        self._log = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def load_data(self):        \n",
    "        # load dataset definition\n",
    "        frames = load_dataset_defition(self.dataset_path, include_day=True)\n",
    "        # filter labels with few samples\n",
    "        valid_frames_idx = np.isin(frames['relation_label'], RELATIONS)\n",
    "\n",
    "        self._frames = frames[valid_frames_idx]\n",
    "        self._grouped_frames = self._frames.groupby(['split', 'segment_id', 'group_id'])\n",
    "\n",
    "        # for each sequence get label, user, day of first frame\n",
    "        self._labels, self._users, self._seq_days = load_fields(\n",
    "            frames, ['relation_label', 'camera_user_name', 'day'], \n",
    "            valid_frames_idx=valid_frames_idx\n",
    "        )\n",
    "        \n",
    "        self._init_features(self._frames)\n",
    "        self._init_grouped_splits()\n",
    "        self._dim_reduction = defaultdict(dict)\n",
    "\n",
    "    def _init_features(self, valid_frames):\n",
    "        attributes = sorted(next(os.walk(self.features_dir))[2])\n",
    "        attributes = [attr for attr in attributes if attr.endswith('.npy')]\n",
    "\n",
    "        # load features\n",
    "        self._attribute_features = {}\n",
    "        for attribute_file in attributes:\n",
    "            attribute_name, ext = os.path.splitext(attribute_file)\n",
    "            features_path = os.path.join(self.features_dir, attribute_file)\n",
    "            self._attribute_features[attribute_name] = load_features(features_path, valid_frames)        \n",
    "        \n",
    "    def _init_stratified_splits(self):\n",
    "        # define data splits\n",
    "        # define train, test splits\n",
    "        sss = StratifiedShuffleSplit(test_size=self.test_size, random_state=self.seed)\n",
    "        y = self._labels\n",
    "        self._train_idx, self._test_idx = next(sss.split(np.zeros(len(y)), y))  \n",
    "\n",
    "        # define k-fold train splits\n",
    "        skf = StratifiedKFold(n_splits=self.k_fold_splits, random_state=self.seed)\n",
    "        y = np.array(y[self._train_idx])\n",
    "        \n",
    "        self._k_train_val_idx = []\n",
    "        for t_idx, v_idx in skf.split(np.zeros(len(y)), y):\n",
    "            self._k_train_val_idx.append((self._train_idx[t_idx], self._train_idx[v_idx]))\n",
    "\n",
    "    def _init_grouped_splits(self):\n",
    "        # define data splits\n",
    "        # define train, test splits        \n",
    "        criteria = np.array([ user + '_' + day for user, day in zip(self._users, self._seq_days) ])\n",
    "        y, groups = helper._labels, criteria\n",
    "\n",
    "        n_tries, group_size, epsilon = 1000, self.test_size, 0.025\n",
    "\n",
    "        split_wrapper = StratifiedGroupShuffleSplitWrapper(\n",
    "            GroupShuffleSplit(n_splits=n_tries, test_size=group_size, random_state=self.seed), \n",
    "            n_splits=1, \n",
    "            max_test_size=min(self.test_size + epsilon, 1.0), min_test_size=max(self.test_size - epsilon, 0.0)\n",
    "        )\n",
    "        self._train_idx, self._test_idx, train_test_score = next(split_wrapper.split(np.zeros(len(y)), y, groups, return_score=True))\n",
    "        test_size = len(self._test_idx) / (len(self._train_idx) + len(self._test_idx))\n",
    "        self._log.debug('Split train-test score: {:.3} real_test_size: {:.3}'.format(train_test_score, test_size))\n",
    "        \n",
    "        # define k-fold splits\n",
    "        y, groups = y[self._train_idx], groups[self._train_idx]\n",
    "        \n",
    "        if self.k_fold_splits > 1:\n",
    "            # k-fold strategy\n",
    "            # search 50 times the number of splits, encourage diversity\n",
    "            # double the epsilon (more flexible)\n",
    "            n_tries, group_size, epsilon = self.k_fold_splits * 50, self.val_size, 0.05\n",
    "        else:\n",
    "            # holdout strategy\n",
    "            # keep n_tries and epsilon same as train-test split\n",
    "            group_size = self.val_size\n",
    "        \n",
    "        split_wrapper = StratifiedGroupShuffleSplitWrapper(\n",
    "            GroupShuffleSplit(n_splits=n_tries, test_size=group_size, random_state=self.seed), \n",
    "            n_splits=self.k_fold_splits,\n",
    "            max_test_size=min(self.val_size + epsilon, 1.0), min_test_size=max(self.val_size - epsilon, 0.0)\n",
    "        )\n",
    "        \n",
    "        self._k_train_val_idx = []\n",
    "        for k, (t_idx, v_idx, t_v_score) in enumerate(split_wrapper.split(np.zeros(len(y)), y, groups, return_score=True)):\n",
    "            self._k_train_val_idx.append((self._train_idx[t_idx], self._train_idx[v_idx]))\n",
    "            val_size = (1 - test_size) * len(v_idx) / (len(t_idx) + len(v_idx))\n",
    "            self._log.debug('{}-fold split score: {:.3} real_val_size={:.3}'.format(k, t_v_score, val_size))            \n",
    "            \n",
    "    def list_attributes(self):\n",
    "        # list attributes\n",
    "        if self._attribute_features:\n",
    "            return sorted(self._attribute_features.keys())\n",
    "        else:\n",
    "            return []\n",
    "            \n",
    "    def _get_split_idx(self, split, k_fold=None):\n",
    "        assert split in ('train', 'test', 'val')\n",
    "        if split == 'train':\n",
    "            if k_fold is None:\n",
    "                return self._train_idx\n",
    "            else:\n",
    "                assert 0 <= k_fold < self.k_fold_splits\n",
    "                return self._k_train_val_idx[k_fold][0]\n",
    "\n",
    "        if split == 'val':\n",
    "            assert 0 <= k_fold < self.k_fold_splits\n",
    "            return self._k_train_val_idx[k_fold][1]\n",
    "        \n",
    "        if split == 'test':\n",
    "            return self._test_idx\n",
    "        \n",
    "    def get_k_train_val_split(self, selected_attributes, k_fold, preprocess_mask=None):\n",
    "        assert 0 <= k_fold < self.k_fold_splits\n",
    "        return self._get_train_test_val_split(selected_attributes, k_fold=k_fold, preprocess_mask=preprocess_mask)\n",
    "        \n",
    "    def get_train_test_split(self, selected_attributes, preprocess_mask=None):\n",
    "        return self._get_train_test_val_split(selected_attributes, preprocess_mask=preprocess_mask)\n",
    "    \n",
    "    def _get_train_test_val_split(self, selected_attributes, k_fold=None, preprocess_mask=None):\n",
    "        # get data splits composed by selected attributes only\n",
    "        # preprocess the data\n",
    "\n",
    "        # preconditions\n",
    "        assert self._attribute_features is not None\n",
    "        assert self._labels is not None\n",
    "        assert len(selected_attributes) > 0 \n",
    "        assert preprocess_mask is None or len(preprocess_mask) == len(selected_attributes)\n",
    "        min_dim_for_pca = self.n_components\n",
    "        \n",
    "        if k_fold is None:\n",
    "            train_idx, test_idx = self._get_split_idx('train'), self._get_split_idx('test')\n",
    "            k_fold = -1 # see DimReductionTransformer init below\n",
    "        else:\n",
    "            assert 0 <= k_fold < self.k_fold_splits\n",
    "            train_idx, test_idx = self._get_split_idx('train', k_fold), self._get_split_idx('val', k_fold)\n",
    "            \n",
    "        # preprocess each selected attribute individually\n",
    "        train_features_list, test_features_list = [], []\n",
    "        for attr_i, attr in enumerate(selected_attributes):            \n",
    "            train_features = self._attribute_features[attr][train_idx]\n",
    "            train_features = list(itertools.chain(*train_features))\n",
    "            \n",
    "            test_features = self._attribute_features[attr][test_idx]\n",
    "            test_features = list(itertools.chain(*test_features))\n",
    "            \n",
    "            n_features = train_features[0].shape[-1]\n",
    "            if preprocess_mask and preprocess_mask[attr_i]:\n",
    "                self._log.debug('Preprocessing {}.'.format(attr))\n",
    "                # initialize transformation for each attribute\n",
    "                if min_dim_for_pca <= n_features:\n",
    "                    # init transformation with different (fixed) seed for each fold\n",
    "                    transformation = DimReductionTransformer(n_components=self.n_components, Q=self.Q, random_state=self.seed + k_fold)\n",
    "                else:\n",
    "                    transformation = MinMaxScaler()\n",
    "\n",
    "                transformation.fit(train_features)\n",
    "            \n",
    "                train_features = transformation.transform(train_features)\n",
    "                test_features = transformation.transform(test_features)\n",
    "            else:\n",
    "                self._log.debug('Skip preprocessing {}.'.format(attr))\n",
    "\n",
    "            train_features_list.append(train_features)\n",
    "            test_features_list.append(test_features)\n",
    "\n",
    "        attr = selected_attributes[0]\n",
    "        # create sequence structure for train\n",
    "        train_features_fused = np.concatenate(train_features_list, axis=-1)\n",
    "        train_seq_len = map(len, self._attribute_features[attr][train_idx])\n",
    "        train_features_as_seqs = []\n",
    "        offset = 0\n",
    "        for seq_len in train_seq_len:        \n",
    "            seq_range = range(offset, offset+seq_len)\n",
    "            train_features_as_seqs.append(train_features_fused[seq_range])\n",
    "            offset += seq_len\n",
    "        \n",
    "        # create sequence structure for testtrain\n",
    "        test_features_fused = np.concatenate(test_features_list, axis=-1)\n",
    "        test_seq_len = map(len, self._attribute_features[attr][test_idx])\n",
    "        test_features_as_seqs = []\n",
    "        offset = 0\n",
    "        for seq_len in test_seq_len:        \n",
    "            seq_range = range(offset, offset+seq_len)\n",
    "            test_features_as_seqs.append(test_features_fused[seq_range])\n",
    "            offset += seq_len\n",
    "        \n",
    "        return (train_features_as_seqs, self._labels[train_idx]), (test_features_as_seqs, self._labels[test_idx])\n",
    "    \n",
    "    def max_sequence_len(self):\n",
    "        return self._grouped_frames.size().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_callbacks(output_mode, plot_stats=True, save_model=False, save_stats=False, stop_early=False, reduce_lr=False, figsize=None):\n",
    "    callbacks = []\n",
    "\n",
    "    training_dir = os.path.join(egosocial.config.TMP_DIR, 'training')\n",
    "    create_directory(training_dir, 'Training')\n",
    "\n",
    "    if save_model:\n",
    "        checkpoint_path = os.path.join(training_dir,\n",
    "                                       'weights.{epoch:02d}-{val_loss:.2f}.h5')\n",
    "        checkpointer = ModelCheckpoint( \n",
    "            filepath=checkpoint_path, monitor='val_loss',\n",
    "            save_best_only=True, period=5,\n",
    "        )\n",
    "        callbacks.append(checkpointer)\n",
    "\n",
    "    if save_stats:\n",
    "        metrics_path = os.path.join(training_dir,\n",
    "                                    'metrics.csv')\n",
    "        csv_logger = CSVLogger(metrics_path)\n",
    "        callbacks.append(csv_logger)\n",
    "        \n",
    "    if reduce_lr:\n",
    "        lr_handler = ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001\n",
    "        )\n",
    "        callbacks.append(lr_handler)\n",
    "        \n",
    "    if plot_stats:\n",
    "        # more plots need more space\n",
    "        if not figsize:\n",
    "            if output_mode != 'both_splitted':\n",
    "                figsize = (25, 5)\n",
    "            else:\n",
    "                figsize = (25, 13)\n",
    "\n",
    "        plot_metrics = PlotLearning(update_step=5, figsize=figsize)\n",
    "        callbacks.append(plot_metrics)\n",
    "        \n",
    "    if stop_early:\n",
    "        stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, mode='auto')\n",
    "        callbacks.append(stopper)\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "def compile_model(\n",
    "    model, \n",
    "    optimizer='adam', \n",
    "    loss='categorical_crossentropy', \n",
    "    loss_weights='auto',\n",
    "    **kwargs\n",
    "):\n",
    "    # wrapper allows to train the loss weights\n",
    "    model_wrapper = AutoMultiLossWrapper(model)\n",
    "    model_wrapper.compile(optimizer=optimizer, loss=loss, \n",
    "                          loss_weights=loss_weights, **kwargs)\n",
    "\n",
    "    return model_wrapper.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.join(egosocial.config.TMP_DIR, 'egocentric', 'datasets')\n",
    "\n",
    "args = [\n",
    "    \"--dataset_path\", os.path.join(BASE_DIR, 'merged_dataset.json'),\n",
    "    \"--features_dir\", os.path.join(BASE_DIR, 'extracted_features'),\n",
    "]\n",
    "\n",
    "conf = main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading precomputed features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper = SocialClassifierWithPreComputedFeatures(\n",
    "    conf.dataset_path, conf.features_dir, \n",
    "    test_size=0.2, \n",
    "    k_fold_splits=3, \n",
    "    val_size=0.2, \n",
    "    n_components=50, \n",
    "    Q=32,\n",
    "    seed=SHARED_SEED\n",
    ")\n",
    "\n",
    "helper.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = []\n",
    "for user in np.unique(helper._users):\n",
    "    user_labels = helper._labels[helper._users == user]\n",
    "    unique_user_labels = np.unique(user_labels)\n",
    "    stats.append({'User':user, '# samples': len(user_labels), '# labels:': len(unique_user_labels),  'labels' : unique_user_labels})\n",
    "stats = pd.DataFrame(stats)\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select attributes (default all), prepare output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_selector = AttributeSelector(helper.list_attributes())\n",
    "\n",
    "# all / face / body / or single attribute (accept name substring, e.g. activity)\n",
    "attributes_query = 'all'\n",
    "# expand all / face / body / single attribute\n",
    "selected_attributes = attribute_selector.filter(attributes_query)\n",
    "\n",
    "#selected_attributes = ['activity', 'distance']\n",
    "# all / face / body / or single attribute (accept name substring, e.g. activity)\n",
    "#attributes_query = 'camera'\n",
    "# expand all / face / body / single attribute\n",
    "#selected_attributes += attribute_selector.filter(attributes_query)\n",
    "print('Selected attribute(s): {}'.format(selected_attributes))\n",
    "\n",
    "to_process = lambda attr : attr not in ('camera_user_age', 'camera_user_gender')\n",
    "mask = [to_process(attr) for attr in selected_attributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.Q = 32\n",
    "helper.n_components = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_fold=0\n",
    "# prepare splits for selected attributes\n",
    "#(k_x_train, k_y_train), (k_x_val, k_y_val) = helper.get_k_train_val_split(selected_attributes, k_fold=k_fold, preprocess_mask=mask)\n",
    "\n",
    "(k_x_train, k_y_train), (k_x_val, k_y_val) = helper.get_train_test_split(selected_attributes, preprocess_mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_val = len(k_x_train), len(k_x_val)\n",
    "n_features = k_x_train[0].shape[-1]\n",
    "max_timestep = helper.max_sequence_len()\n",
    "batch_size = 128\n",
    "\n",
    "print('Length of the largest sequence:', max_timestep)\n",
    "print('Number of features:', n_features)\n",
    "print('Number of samples fold={}. Training: {} / Validation {}'.format(k_fold, n_train, n_val))\n",
    "print('Batch size: {}'.format(batch_size))\n",
    "\n",
    "#output_mode = 'both_splitted' # multi-loss domain-relation\n",
    "#output_mode = 'domain' # domain only\n",
    "output_mode = 'relation' # relation only\n",
    "\n",
    "print('Output mode: {}'.format(output_mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use domain specific labels\n",
    "if output_mode == 'domain':\n",
    "    k_y_train = relation_to_domain_vec(k_y_train)\n",
    "    k_y_val = relation_to_domain_vec(k_y_val)    \n",
    "    \n",
    "# class_weight for keras (balance domain/relation instances)\n",
    "class_weight = compute_class_weight_labels(k_y_train, mode=output_mode)\n",
    "\n",
    "if 'domain' in class_weight:\n",
    "    pprint.pprint(('domains', [(domain, class_weight['domain'][idx]) for idx, domain in enumerate(DOMAINS)]))\n",
    "    \n",
    "if 'relation' in class_weight:\n",
    "    pprint.pprint(('relations', [(relation, class_weight['relation'][idx]) for idx, relation in enumerate(RELATIONS)]))\n",
    "    \n",
    "process_labels = LabelExpander(mode=output_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.002\n",
    "schedule_lr = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_strategy = 'top_down'\n",
    "model_strategy_select = {\n",
    "    'top_down' : create_model_top_down,\n",
    "    'bottom_up' : create_model_bottom_up,\n",
    "    'independent' : create_model_independent_outputs,\n",
    "}\n",
    "\n",
    "model_parameters = dict(\n",
    "    max_seq_len=max_timestep,\n",
    "    n_features=n_features,\n",
    "    mode=output_mode,\n",
    "    units=128,\n",
    "    recurrent_type='LSTM',\n",
    "    drop_rate=0.3,\n",
    "    rec_drop_rate=0.3,\n",
    "    l2_reg=0.001,\n",
    "    hidden_fc=1,\n",
    "    n_relations=len(RELATIONS),\n",
    "    n_domains=len(DOMAINS),\n",
    "    seed=SHARED_SEED,\n",
    "    batch_norm=True,\n",
    ")\n",
    "\n",
    "model = model_strategy_select[model_strategy](**model_parameters)\n",
    "if model_strategy_select == 'bottom_up':\n",
    "    model.get_layer('domain').set_weights([relation_to_domain_weights()])\n",
    "\n",
    "model = compile_model(\n",
    "    model,\n",
    "    loss='categorical_crossentropy',    \n",
    "\n",
    "    optimizer=keras.optimizers.Adam(learning_rate, decay=1e-5),\n",
    "    \n",
    "    metrics=['accuracy', recall, fmeasure],\n",
    "    loss_weights='auto',\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "callbacks = init_callbacks(output_mode, plot_stats=True, stop_early=False, figsize=(25, 13))\n",
    "\n",
    "# learning rate schedule\n",
    "class StepDecay(object):\n",
    "\n",
    "    def __init__(self, initial_lr=0.01, min_lr=0.0, drop_rate=0.5, epochs_drop=10.0):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.drop_rate = drop_rate\n",
    "        self.epochs_drop = epochs_drop\n",
    "        self.min_lr = min_lr\n",
    "\n",
    "    def __call__(self, epoch):\n",
    "        lrate = self.initial_lr * math.pow(self.drop_rate, math.floor((1+epoch)/self.epochs_drop))\n",
    "        lrate = max(lrate, self.min_lr)\n",
    "        \n",
    "        return lrate\n",
    "\n",
    "if schedule_lr:\n",
    "    lr_scheduler = LearningRateScheduler(StepDecay(initial_lr=learning_rate, drop_rate=0.5, epochs_drop=40))\n",
    "    callbacks.append(lr_scheduler)\n",
    "\n",
    "train_datagen = TimeSeriesDataGenerator(fancy_pca=True, noise_stddev=0.01, random_state=SHARED_SEED)\n",
    "train_datagen.fit(k_x_train)\n",
    "\n",
    "val_datagen = TimeSeriesDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "    k_x_train, k_y_train,\n",
    "    maxlen=max_timestep,\n",
    "    output_cbk=process_labels,\n",
    "    batch_size=batch_size,\n",
    "    seed=SHARED_SEED,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_batch_size = n_val\n",
    "val_generator = val_datagen.flow(\n",
    "    k_x_val, k_y_val,\n",
    "    maxlen=max_timestep,\n",
    "    output_cbk=process_labels,\n",
    "    batch_size=val_batch_size,\n",
    "    seed=SHARED_SEED,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_epochs, epochs = 0, 150\n",
    "print('Initial epoch: {} Number of epochs: {}'.format(cache_epochs, epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(1.0 * n_train / batch_size),\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=np.ceil(1.0 * n_val / val_batch_size),\n",
    "    epochs=cache_epochs + epochs,\n",
    "    initial_epoch=cache_epochs,\n",
    "    callbacks=callbacks,\n",
    "    verbose=0,\n",
    "    max_queue_size=5,\n",
    "    workers=4,\n",
    "    class_weight=class_weight,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val_generator = val_datagen.flow(\n",
    "    k_x_val, k_y_val, \n",
    "    maxlen=max_timestep, \n",
    "    output_cbk=process_labels, \n",
    "    batch_size=n_val,\n",
    "    seed=SHARED_SEED,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "k_y_val_predicted = model.predict_generator(val_generator, steps=1)\n",
    "\n",
    "decoded_prediction = decode_prediction(k_y_val_predicted, mode=output_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weighted = False\n",
    "print('Macro{} metrics'.format('_weighted' if weighted else ''))\n",
    "\n",
    "domain_is_present = 'domain' in decoded_prediction\n",
    "if domain_is_present:\n",
    "    if output_mode == 'domain':\n",
    "        dom_stats = compute_stats(k_y_val, decoded_prediction['domain'], weighted=weighted)\n",
    "        print_statistics(val_stats=dom_stats)\n",
    "    else:\n",
    "        dom_stats = compute_stats(relation_to_domain_vec(k_y_val), decoded_prediction['domain'], weighted=weighted)\n",
    "        print_statistics(val_stats=dom_stats)\n",
    "\n",
    "relation_is_present = 'relation' in decoded_prediction\n",
    "if relation_is_present:\n",
    "    dom_from_rel_stats = compute_stats(relation_to_domain_vec(k_y_val), relation_to_domain_vec(decoded_prediction['relation']), weighted=weighted)\n",
    "    print_statistics(val_stats=dom_from_rel_stats)    \n",
    "    \n",
    "    rel_stats = compute_stats(k_y_val,  decoded_prediction['relation'], weighted=weighted)\n",
    "    print_statistics(val_stats=rel_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True,\n",
    "                          digits=True,\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=60)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if digits:\n",
    "        thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            if normalize:\n",
    "                plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n",
    "                         horizontalalignment=\"center\",\n",
    "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "            else:\n",
    "                plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                         horizontalalignment=\"center\",\n",
    "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "#    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.xlabel('Predicted label'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if domain_is_present:\n",
    "    cm = dom_stats[1]\n",
    "    plot_confusion_matrix(cm=cm, \n",
    "                          normalize    = True,\n",
    "                          target_names = DOMAINS,\n",
    "                          digits=True,\n",
    "                          title='',\n",
    "                          cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if relation_is_present:\n",
    "    cm = dom_from_rel_stats[1]\n",
    "    plot_confusion_matrix(cm=cm, \n",
    "                          normalize    = True,\n",
    "                          target_names = DOMAINS,\n",
    "                          digits=True,\n",
    "                          title='',\n",
    "                          cmap='Reds')\n",
    "    \n",
    "    cm = rel_stats[1]\n",
    "    plot_confusion_matrix(cm=cm, \n",
    "                          normalize    = True,\n",
    "                          target_names = RELATIONS,\n",
    "                          digits=True,\n",
    "                          title='',                      \n",
    "                          cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition and graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
