{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: Instant Recognition with Caffe\n",
    "\n",
    "In this example we'll classify an image with the bundled CaffeNet model (which is based on the network architecture of Krizhevsky et al. for ImageNet).\n",
    "\n",
    "We'll compare CPU and GPU modes and then dig into the model to inspect features and the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup\n",
    "\n",
    "* First, set up Python, `numpy`, and `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/shared/Documents/final_proj/code/src'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up Python environment: numpy for numerical routines, and matplotlib for plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# display plots in this notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# set display defaults\n",
    "plt.rcParams['figure.figsize'] = (10, 10)        # large images\n",
    "plt.rcParams['image.interpolation'] = 'nearest'  # don't interpolate: show square pixels\n",
    "plt.rcParams['image.cmap'] = 'gray'  # use grayscale output rather than a (potentially misleading) color heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load `caffe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The caffe module needs to be on the Python path;\n",
    "#  we'll add it here explicitly.\n",
    "import sys\n",
    "caffe_root = '/root/caffe/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "import caffe\n",
    "# If you get \"No module named _caffe\", either you have not built pycaffe or you have the wrong path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# social relation models\n",
    "\n",
    "model_switch = 'attributes'\n",
    "\n",
    "if model_switch == 'caffe_net':\n",
    "    models_dir = '../../models/trained_models/caffeNet_models/'\n",
    "    model_def = models_dir + 'end_to_end_training_prototxt/deploy_net_5.prototxt'\n",
    "    model_weights = models_dir + 'finetune_iter_3000.caffemodel'\n",
    "elif model_switch == 'vgg_net': \n",
    "    models_dir = '../../models/trained_models/VGG_models/'\n",
    "    model_def = models_dir + 'end_to_end_training_prototxt/deploy_net.prototxt'\n",
    "    model_weights = models_dir + 'finetune_iter_10000.caffemodel'\n",
    "elif model_switch == 'attributes':\n",
    "    def_dir = '../../models/trained_models/caffeNet_models/'\n",
    "    model_def = def_dir + 'end_to_end_training_prototxt/template_single_stream_net.prototxt'\n",
    "    \n",
    "    attribute_dir = 'head_age_trained_on_pipa/'    \n",
    "    weights_dir = '../../models/trained_models/attribute_models/' + attribute_dir\n",
    "    model_weights = weights_dir + 'finetune_iter_1000.caffemodel'\n",
    "\n",
    "elif model_switch == 'caffe_reference':\n",
    "    model_def = caffe_root + 'models/bvlc_reference_caffenet/deploy.prototxt'\n",
    "    model_weights = caffe_root + 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'\n",
    "    \n",
    "if model_switch == 'attributes':\n",
    "    splits_dir = '../../datasets/splits/annotator_consistency3/'\n",
    "    image_listing_1 = splits_dir + 'single_body1_eval_16.txt'\n",
    "    image_listing_2 = splits_dir + 'single_body2_eval_16.txt'\n",
    "else:\n",
    "    splits_dir = '../../datasets/splits/relation_consistency3/'\n",
    "    image_listing_1 = splits_dir + 'domain_single_body1_eval_5.txt'\n",
    "    image_listing_2 = splits_dir + 'domain_single_body2_eval_5.txt'\n",
    "\n",
    "image_listing_pair = [image_listing_1, image_listing_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['face_appearance_trained_on_CelebAFaces',\n",
       " 'head_age_trained_on_pipa',\n",
       " 'imsitu_body_activity(relation_consistency3)',\n",
       " 'body_clothing_trained_on_berkeleyBodyAttributes',\n",
       " 'head_gender_trained_on_pipa',\n",
       " 'body_gender_trained_on_pipa',\n",
       " 'imsitu_body_activity(annotator_consistency3)',\n",
       " 'body_immediacy(annotator_consistency3)',\n",
       " 'localation_scale_data(annotator_consistency3)',\n",
       " 'body_age_trained_on_pipa',\n",
       " 'readme.txt',\n",
       " 'face_pose_trained_on_IMFDB',\n",
       " 'face_emotion_trained_on_IMFDB']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(os.path.dirname(os.path.dirname(weights_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights found.\n",
      "Model Definition found.\n",
      "Test datasets found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.isfile(model_weights):\n",
    "    print 'Weights found.'\n",
    "if os.path.isfile(model_def):\n",
    "    print 'Model Definition found.'\n",
    "if os.path.isfile(image_listing_1) and os.path.isfile(image_listing_2):\n",
    "    print 'Test datasets found.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load net and set up input preprocessing\n",
    "\n",
    "* Set Caffe to CPU/GPU mode and load the net from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    caffe.set_device(0)  # if we have multiple GPUs, pick the first one\n",
    "    caffe.set_mode_gpu()\n",
    "else:\n",
    "    caffe.set_mode_cpu()\n",
    "\n",
    "INPUT_DIMS = (256, 256)\n",
    "CROP_DIMS = (227, 227)\n",
    "\n",
    "net = caffe.Net(model_def,      # defines the structure of the model\n",
    "                model_weights,  # contains the trained weights\n",
    "                caffe.TEST)     # use test mode (e.g., don't perform dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set up input preprocessing. (We'll use Caffe's `caffe.io.Transformer` to do this, but this step is independent of other parts of Caffe, so any custom preprocessing code may be used).\n",
    "\n",
    "    Our default CaffeNet is configured to take images in BGR format. Values are expected to start in the range [0, 255] and then have the mean ImageNet pixel value subtracted from them. In addition, the channel dimension is expected as the first (_outermost_) dimension.\n",
    "    \n",
    "    As matplotlib will load images with values in the range [0, 1] in RGB format with the channel as the _innermost_ dimension, we are arranging for the needed transformations here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean-subtracted values: [('B', 104.0069879317889), ('G', 116.66876761696767), ('R', 122.6789143406786)]\n"
     ]
    }
   ],
   "source": [
    "# load the mean ImageNet image (as distributed with Caffe) for subtraction\n",
    "mu = np.load(caffe_root + 'python/caffe/imagenet/ilsvrc_2012_mean.npy')\n",
    "mu = mu.mean(1).mean(1)  # average over pixels to obtain the mean (BGR) pixel values\n",
    "print 'mean-subtracted values:', zip('BGR', mu)\n",
    "\n",
    "transformers = []\n",
    "for data_field in ['data', 'data_1']:\n",
    "    # create transformer for the input called 'data'\n",
    "    transformer = caffe.io.Transformer({data_field: net.blobs[data_field].data.shape})\n",
    "\n",
    "    transformer.set_transpose(data_field, (2,0,1))  # move image channels to outermost dimension\n",
    "    transformer.set_channel_swap(data_field, (2,1,0))  # swap channels from RGB to BGR\n",
    "    transformer.set_raw_scale(data_field, 255)      # rescale from [0, 1] to [0, 255]\n",
    "    transformer.set_mean(data_field, mu)            # subtract the dataset-mean value in each channel\n",
    "    transformers.append(transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classification\n",
    "\n",
    "* Now we're ready to perform classification. Even though we'll only classify one image, we'll set a batch size of 50 to demonstrate batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the size of the input (we can skip this if we're happy\n",
    "#  with the default; we can also change it later, e.g., for different batch sizes)\n",
    "for data_field in ['data', 'data_1']:\n",
    "    net.blobs[data_field].reshape(1,         # batch size\n",
    "                                  3,         # 3-channel (BGR) images\n",
    "                                  227, 227)  # image size is 227x227"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load an image (that comes with Caffe) and perform the preprocessing we've set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(image_listing_1) as file_list:\n",
    "    path_label_list_1 = [file_label.split() for file_label in file_list]\n",
    "\n",
    "with open(image_listing_2) as file_list:\n",
    "    path_label_list_2 = [file_label.split() for file_label in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/root/shared/Documents/final_proj/datasets/images/all_single_body/f1_72157624551655535_4870147689.jpg',\n",
       " '/root/shared/Documents/final_proj/datasets/images/all_single_body/f2_72157624551655535_4870147689.jpg']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_idx = 0\n",
    "image_path_1, label = path_label_list_1[image_idx]\n",
    "image_path_2, label = path_label_list_2[image_idx]\n",
    "image_paths = [image_path_1, image_path_2]\n",
    "image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (3,227,227) into shape (1,3,256,256)",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-683829c8e3b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# copy the image data into the memory allocated for the net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_field\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformed_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (3,227,227) into shape (1,3,256,256)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "images = []\n",
    "data_field = ['data', 'data_1']\n",
    "for idx, data_field in enumerate(data_field):\n",
    "    image = caffe.io.load_image(image_paths[idx])\n",
    "    images.append(image)\n",
    "    \n",
    "    transformed_image = transformers[idx].preprocess(data_field, image)\n",
    "    \n",
    "    # copy the image data into the memory allocated for the net\n",
    "    net.blobs[data_field].data[...] = transformed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_img = resize_and_crop_image_2(images[0])\n",
    "plt.imshow(cropped_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(images[0])\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### perform classification\n",
    "output = net.forward()\n",
    "\n",
    "output_prob = output['probs'][0]  # the output probability vector for the first image in the batch\n",
    "\n",
    "print 'predicted class is:', output_prob.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort top five predictions from softmax output\n",
    "top_inds = output_prob.argsort()[::-1][:5]  # reverse sort and take five largest items\n",
    "\n",
    "print 'probabilities and labels:'\n",
    "zip(output_prob[top_inds], np.arange(16)[top_inds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Examining intermediate output\n",
    "\n",
    "* A net is not just a black box; let's take a look at some of the parameters and intermediate activations.\n",
    "\n",
    "First we'll see how to read out the structure of the net in terms of activation and parameter shapes.\n",
    "\n",
    "* For each layer, let's look at the activation shapes, which typically have the form `(batch_size, channel_dim, height, width)`.\n",
    "\n",
    "    The activations are exposed as an `OrderedDict`, `net.blobs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\t(1, 3, 227, 227)\n",
      "label\t(1, 1)\n",
      "label_data_1_split_0\t(1, 1)\n",
      "label_data_1_split_1\t(1, 1)\n",
      "data_1\t(1, 3, 227, 227)\n",
      "dummy_label_1\t(1, 1)\n",
      "conv1\t(1, 96, 55, 55)\n",
      "pool1\t(1, 96, 27, 27)\n",
      "norm1\t(1, 96, 27, 27)\n",
      "conv2\t(1, 256, 27, 27)\n",
      "pool2\t(1, 256, 13, 13)\n",
      "norm2\t(1, 256, 13, 13)\n",
      "conv3\t(1, 384, 13, 13)\n",
      "conv4\t(1, 384, 13, 13)\n",
      "conv5\t(1, 256, 13, 13)\n",
      "pool5\t(1, 256, 6, 6)\n",
      "pool5_reshape\t(1, 9216, 1, 1)\n",
      "conv1_1\t(1, 96, 55, 55)\n",
      "pool1_1\t(1, 96, 27, 27)\n",
      "norm1_1\t(1, 96, 27, 27)\n",
      "conv2_1\t(1, 256, 27, 27)\n",
      "pool2_1\t(1, 256, 13, 13)\n",
      "norm2_1\t(1, 256, 13, 13)\n",
      "conv3_1\t(1, 384, 13, 13)\n",
      "conv4_1\t(1, 384, 13, 13)\n",
      "conv5_1\t(1, 256, 13, 13)\n",
      "pool5_1\t(1, 256, 6, 6)\n",
      "pool5_reshape_1\t(1, 9216, 1, 1)\n",
      "cat_all\t(1, 18432, 1, 1)\n",
      "fc6\t(1, 4096)\n",
      "fc7\t(1, 4096)\n",
      "fc8\t(1, 16)\n",
      "fc8_fc8_RelationN_0_split_0\t(1, 16)\n",
      "fc8_fc8_RelationN_0_split_1\t(1, 16)\n",
      "fc8_fc8_RelationN_0_split_2\t(1, 16)\n",
      "probs\t(1, 16)\n",
      "loss\t()\n",
      "acc\t()\n"
     ]
    }
   ],
   "source": [
    "# for each layer, show the output shape\n",
    "for layer_name, blob in net.blobs.iteritems():\n",
    "    print layer_name + '\\t' + str(blob.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now look at the parameter shapes. The parameters are exposed as another `OrderedDict`, `net.params`. We need to index the resulting values with either `[0]` for weights or `[1]` for biases.\n",
    "\n",
    "    The param shapes typically have the form `(output_channels, input_channels, filter_height, filter_width)` (for the weights) and the 1-dimensional shape `(output_channels,)` (for the biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_name, param in net.params.iteritems():\n",
    "    print layer_name + '\\t' + str(param[0].data.shape), str(param[1].data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since we're dealing with four-dimensional data here, we'll define a helper function for visualizing sets of rectangular heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_square(data):\n",
    "    \"\"\"Take an array of shape (n, height, width) or (n, height, width, 3)\n",
    "       and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)\"\"\"\n",
    "    \n",
    "    # normalize data for display\n",
    "    data = (data - data.min()) / (data.max() - data.min())\n",
    "    \n",
    "    # force the number of filters to be square\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = (((0, n ** 2 - data.shape[0]),\n",
    "               (0, 1), (0, 1))                 # add some space between filters\n",
    "               + ((0, 0),) * (data.ndim - 3))  # don't pad the last dimension (if there is one)\n",
    "    data = np.pad(data, padding, mode='constant', constant_values=1)  # pad with ones (white)\n",
    "    \n",
    "    # tile the filters into an image\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    \n",
    "    plt.imshow(data); plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First we'll look at the first layer filters, `conv1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the parameters are a list of [weights, biases]\n",
    "filters = net.params['conv1'][0].data\n",
    "vis_square(filters.transpose(0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first layer output, `conv1` (rectified responses of the filters above, first 36 only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = net.blobs['conv1'].data[0, :36]\n",
    "vis_square(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The fifth layer after pooling, `pool5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = net.blobs['pool5'].data[0]\n",
    "vis_square(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first fully connected layer, `fc6` (rectified)\n",
    "\n",
    "    We show the output values and the histogram of the positive values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = net.blobs['fc7'].data[0]\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(feat.flat)\n",
    "plt.subplot(2, 1, 2)\n",
    "_ = plt.hist(feat.flat[feat.flat > 0], bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Try your own image\n",
    "\n",
    "Now we'll grab an image from the web and classify it using the steps above.\n",
    "\n",
    "* Try setting `my_image_url` to any JPEG image URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download an image\n",
    "#my_image_url = \"...\"  # paste your URL here\n",
    "# for example:\n",
    "my_image_url = \"https://upload.wikimedia.org/wikipedia/commons/b/be/Orang_Utan%2C_Semenggok_Forest_Reserve%2C_Sarawak%2C_Borneo%2C_Malaysia.JPG\"\n",
    "!wget -O image.jpg $my_image_url\n",
    "\n",
    "# transform it and copy it into the net\n",
    "image = caffe.io.load_image('image.jpg')\n",
    "net.blobs['data'].data[...] = transformer.preprocess('data', image)\n",
    "\n",
    "# perform classification\n",
    "net.forward()\n",
    "\n",
    "# obtain the output probabilities\n",
    "output_prob = net.blobs['prob'].data[0]\n",
    "\n",
    "# sort top five predictions from softmax output\n",
    "top_inds = output_prob.argsort()[::-1][:5]\n",
    "\n",
    "plt.imshow(image)\n",
    "\n",
    "print 'probabilities and labels:'\n",
    "zip(output_prob[top_inds], labels[top_inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classifier is an image classifier specialization of Net.\n",
    "\"\"\"\n",
    "\n",
    "class MultiInputsClassifier(caffe.Net):\n",
    "    \"\"\"\n",
    "    Classifier extends Net for image class prediction\n",
    "    by scaling, center cropping, or oversampling.\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_dims : dimensions to scale input for cropping/sampling.\n",
    "        Default is to scale to net input size for whole-image crop.\n",
    "    mean, input_scale, raw_scale, channel_swap: params for\n",
    "        preprocessing options.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_file, pretrained_file, image_dims=None,\n",
    "                 mean=None, input_scale=None, raw_scale=None,\n",
    "                 channel_swap=None):\n",
    "        caffe.Net.__init__(self, model_file, pretrained_file, caffe.TEST)\n",
    "\n",
    "        # configure pre-processing\n",
    "        self.transformers = [caffe.io.Transformer({in_: self.blobs[in_].data.shape}) for in_ in self.inputs]\n",
    "        \n",
    "        for idx, in_ in enumerate(self.inputs):\n",
    "            self.transformers[idx].set_transpose(in_, (2, 0, 1))\n",
    "            if mean is not None:\n",
    "                self.transformers[idx].set_mean(in_, mean)\n",
    "            if input_scale is not None:\n",
    "                self.transformers[idx].set_input_scale(in_, input_scale)\n",
    "            if raw_scale is not None:\n",
    "                self.transformers[idx].set_raw_scale(in_, raw_scale)\n",
    "            if channel_swap is not None:\n",
    "                self.transformers[idx].set_channel_swap(in_, channel_swap)\n",
    "        \n",
    "        in_ = self.inputs[0]\n",
    "        self.crop_dims = np.array(self.blobs[in_].data.shape[2:])\n",
    "        if not image_dims:\n",
    "            image_dims = self.crop_dims\n",
    "        self.image_dims = image_dims\n",
    "\n",
    "    def predict(self, oversample=True, *multiple_inputs):\n",
    "        \"\"\"\n",
    "        Predict classification probabilities of inputs.\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : iterable of (H x W x K) input ndarrays.\n",
    "        oversample : boolean\n",
    "            average predictions across center, corners, and mirrors\n",
    "            when True (default). Center-only prediction when False.\n",
    "        Returns\n",
    "        -------\n",
    "        predictions: (N x C) ndarray of class probabilities for N images and C\n",
    "            classes.\n",
    "        \"\"\"\n",
    "        caffe_in_list = []\n",
    "        \n",
    "        for input_idx, inputs in enumerate(multiple_inputs):\n",
    "            # Scale to standardize input dimensions.\n",
    "            input_ = np.zeros((len(inputs),\n",
    "                               self.image_dims[0],\n",
    "                               self.image_dims[1],\n",
    "                               inputs[0].shape[2]),\n",
    "                              dtype=np.float32)\n",
    "            \n",
    "            for ix, in_ in enumerate(inputs):\n",
    "                input_[ix] = caffe.io.resize_image(in_, self.image_dims)\n",
    "\n",
    "            if oversample:\n",
    "                # Generate center, corner, and mirrored crops.\n",
    "                input_ = caffe.io.oversample(input_, self.crop_dims)\n",
    "            else:\n",
    "                # Take center crop.\n",
    "                center = np.array(self.image_dims) / 2.0\n",
    "                crop = np.tile(center, (1, 2))[0] + np.concatenate([\n",
    "                    -self.crop_dims / 2.0,\n",
    "                    self.crop_dims / 2.0\n",
    "                ])\n",
    "                crop = crop.astype(int)\n",
    "                input_ = input_[:, crop[0]:crop[2], crop[1]:crop[3], :]\n",
    "\n",
    "            # Classify\n",
    "            caffe_in = np.zeros(np.array(input_.shape)[[0, 3, 1, 2]],\n",
    "                                dtype=np.float32)\n",
    "        \n",
    "            for ix, in_ in enumerate(input_):\n",
    "                caffe_in[ix] = self.transformers[input_idx].preprocess(self.inputs[input_idx], in_)\n",
    "            \n",
    "            caffe_in_list.append(caffe_in)\n",
    "        \n",
    "        out = self.forward_all(**{self.inputs[input_idx] : caffe_in_list[input_idx] for input_idx in range(len(self.inputs))})\n",
    "        predictions = out[self.outputs[0]]\n",
    "\n",
    "        # For oversampling, average predictions across crops.\n",
    "        if oversample:\n",
    "            predictions = predictions.reshape((len(predictions) // 10, 10, -1))\n",
    "            predictions = predictions.mean(1)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Transpose order needs to have the same number of dimensions as the input.",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d63f3f15ae38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                             \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimagenet_mean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                             \u001b[0mraw_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                             channel_swap=(2,0,1))\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-bb3ef64b583c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_file, pretrained_file, image_dims, mean, input_scale, raw_scale, channel_swap)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_transpose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/caffe/python/caffe/io.pyc\u001b[0m in \u001b[0;36mset_transpose\u001b[0;34m(self, in_, order)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__check_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0min_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             raise Exception('Transpose order needs to have the same number of '\n\u001b[0m\u001b[1;32m    200\u001b[0m                             'dimensions as the input.')\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0min_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Transpose order needs to have the same number of dimensions as the input."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "imagenet_mean = np.load(caffe_root + 'python/caffe/imagenet/ilsvrc_2012_mean.npy')\n",
    "imagenet_mean = imagenet_mean.mean(1).mean(1)  # average over pixels to obtain the mean (BGR) pixel values\n",
    "\n",
    "net = MultiInputsClassifier(model_def,      # defines the structure of the model\n",
    "                            model_weights,  # contains the trained weights\n",
    "                            image_dims=INPUT_DIMS,\n",
    "                            mean=imagenet_mean,\n",
    "                            raw_scale=255,\n",
    "                            channel_swap=(2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "description": "Instant recognition with a pre-trained model and a tour of the net interface for visualizing features and parameters layer-by-layer.",
  "example_name": "Image Classification and Filter Visualization",
  "include_in_docs": true,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "priority": 1.0
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
