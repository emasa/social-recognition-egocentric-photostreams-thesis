{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_notebook(fix_python_path=True, reduce_margins=True, plot_inline=True):\n",
    "    if reduce_margins:\n",
    "        # Reduce side margins of the notebook\n",
    "        from IPython.core.display import display, HTML\n",
    "        display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "    if fix_python_path:\n",
    "        # add egosocial to the python path\n",
    "        import os, sys\n",
    "        sys.path.extend([os.path.dirname(os.path.abspath('.'))])\n",
    "\n",
    "    if plot_inline:\n",
    "        # Plots inside cells\n",
    "        %matplotlib inline\n",
    "    \n",
    "    global __file__\n",
    "    __file__ = 'Notebook'\n",
    "\n",
    "setup_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Constants Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import argparse\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "import keras.preprocessing.image\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import egosocial\n",
    "import egosocial.config\n",
    "from egosocial.core.attributes import AttributeSelector\n",
    "from egosocial.utils.filesystem import create_directory\n",
    "from egosocial.utils.filesystem import check_directory\n",
    "from egosocial.utils.keras.backend import limit_gpu_allocation_tensorflow\n",
    "from egosocial.utils.keras.layers import LRN\n",
    "from egosocial.utils.logging import setup_logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit GPU memory allocation with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_memory = False\n",
    "if limit_memory and K.backend() == 'tensorflow':\n",
    "    memory_ratio = 0.6\n",
    "    limit_gpu_allocation_tensorflow(memory_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(*fake_args):\n",
    "    entry_msg = 'Extract features for egosocial photo-streams.'\n",
    "    parser = argparse.ArgumentParser(description=entry_msg)\n",
    "\n",
    "    parser.add_argument('--dataset_path', required=True,\n",
    "                        help='Path to file containing the input data and labels information merged.')\n",
    "\n",
    "    parser.add_argument('--features_dir', required=True,\n",
    "                        help='Directory where the extracted features are stored.') \n",
    "    \n",
    "    parser.add_argument('--batch_size', required=False, type=int,\n",
    "                        default=32,\n",
    "                        help='Batch size.')    \n",
    "    \n",
    "    if not os.path.isdir(egosocial.config.TMP_DIR):\n",
    "        os.mkdir(egosocial.config.TMP_DIR)\n",
    "\n",
    "    setup_logging(egosocial.config.LOGGING_CONFIG,\n",
    "                  log_dir=egosocial.config.LOGS_DIR)\n",
    "    \n",
    "    # TODO: implement correctly\n",
    "    args = parser.parse_args(*fake_args)\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.join(egosocial.config.TMP_DIR, 'egocentric', 'datasets')\n",
    "\n",
    "args = [\n",
    "    \"--dataset_path\", os.path.join(BASE_DIR, 'merged_dataset.json'),\n",
    "    \"--features_dir\", os.path.join(BASE_DIR, 'extracted_features'),\n",
    "]\n",
    "\n",
    "conf = main(args)\n",
    "\n",
    "create_directory(conf.features_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity_model(layer=None, img_width=224, img_height=224):\n",
    "    base_model = ResNet50(weights=None, include_top=False, input_shape=(img_width, img_height, 3))\n",
    "    \n",
    "    if layer:\n",
    "        layer_output = base_model.get_layer(layer).output\n",
    "    else:\n",
    "        layer_output = Flatten(name='flatten')(base_model.output)\n",
    "        \n",
    "    model = Model(inputs=base_model.input, outputs=[layer_output])\n",
    "\n",
    "    weights = os.path.join(egosocial.config.MODELS_CACHE_WEIGHTS, 'ego_activity_weights_resNet50.hdf5')\n",
    "    model.load_weights(weights, by_name=True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_models(query, layer=None):\n",
    "    _log = logging.getLogger(os.path.basename(__file__))\n",
    "    \n",
    "    selector = AttributeSelector(egosocial.config.MODEL_KEYS)\n",
    "    attributes = selector.filter(query)\n",
    "    \n",
    "    directory = egosocial.config.MODELS_CACHE_FULL\n",
    "    if attributes and not os.path.isdir(directory):\n",
    "        create_directory(directory, 'Models cache')\n",
    "\n",
    "    models = {}\n",
    "    for attr in attributes:\n",
    "        model_info = egosocial.config.MODELS[attr]        \n",
    "        \n",
    "        # absolute path to the file\n",
    "        model_file = os.path.join(directory, model_info['file'])        \n",
    "        gdd.download_file_from_google_drive(\n",
    "            file_id=model_info['file_id'],\n",
    "            dest_path=model_file,\n",
    "        )\n",
    "                \n",
    "        _log.debug('Loading model from {}'.format(model_file))        \n",
    "        features_model = keras.models.load_model(\n",
    "            model_file, custom_objects={'LRN': LRN}\n",
    "        )\n",
    "        \n",
    "        if layer:\n",
    "            layer_output = features_model.get_layer(layer).output\n",
    "            features_model = Model(\n",
    "                inputs=features_model.inputs,\n",
    "                outputs=[layer_output],\n",
    "            )\n",
    "\n",
    "        models[attr] = features_model\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_defition(dataset_path):\n",
    "    with open(dataset_path, 'r') as json_file:\n",
    "        dataset_def = json.load(json_file)\n",
    "\n",
    "    # flatten the segments structure\n",
    "    samples = pd.DataFrame(list(itertools.chain(*dataset_def)))\n",
    "    return samples\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path,\n",
    "        extract_features_cbk,\n",
    "        data_type,\n",
    "        features_path=None,\n",
    "        preprocessing_function=None,\n",
    "        target_size=None,\n",
    "        batch_size=32,\n",
    "    ):\n",
    "        assert data_type in ('body', 'face', 'context', 'camera_user_data')\n",
    "        assert batch_size >= 1\n",
    "        \n",
    "        self._dataset_path = dataset_path\n",
    "        self._extract_features_cbk = extract_features_cbk        \n",
    "        self._data_type = data_type\n",
    "        self._features_path = features_path\n",
    "        \n",
    "        self._preprocessing_function = preprocessing_function\n",
    "        self._target_size = target_size\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "        # set up logging\n",
    "        self._log = logging.getLogger(self.__class__.__name__)\n",
    "    \n",
    "    def run(self):\n",
    "        self._log.debug('Loading dataset definition: {}'.format(self._dataset_path))        \n",
    "        samples = load_dataset_defition(self._dataset_path)\n",
    "        \n",
    "        n_samples = len(samples)        \n",
    "        n_batches = int(np.ceil(n_samples / self._batch_size))\n",
    "        \n",
    "        features = []\n",
    "        for batch_i in range(n_batches):\n",
    "            self._log.debug('Processing batch {} / {}'.format(batch_i, n_batches))\n",
    "            \n",
    "            batch_index = range(batch_i * self._batch_size, min((batch_i+1) * self._batch_size, n_samples))\n",
    "            \n",
    "            if self._data_type == 'camera_user_data':\n",
    "                data_batch = samples.loc[batch_index, ['camera_user_age', 'camera_user_gender']]\n",
    "            else:\n",
    "                data_batch = self._load_images_batch(samples, batch_index)\n",
    "            \n",
    "            self._log.debug('Extracting features...')            \n",
    "            features_batch = self._extract_features_cbk(data_batch)\n",
    "            features.append(features_batch)\n",
    "            \n",
    "        features_array = np.concatenate(features)\n",
    "\n",
    "        if self._features_path:\n",
    "            self._save_features(self._features_path, features_array)\n",
    "        \n",
    "        return features_array\n",
    "    \n",
    "    def _load_images_batch(self, samples, batch_idxs):\n",
    "        images_batch = np.zeros((len(batch_idxs),) + self._target_size + (3,))\n",
    "        \n",
    "        for local_idx, global_idx in enumerate(batch_idxs):\n",
    "            self._log.debug('Loading item {}'.format(global_idx))            \n",
    "            sample_dict = samples.loc[global_idx]\n",
    "            \n",
    "            if self._data_type == 'body':\n",
    "                image_path = sample_dict['body_image_path']\n",
    "            elif self._data_type == 'face':\n",
    "                image_path = sample_dict['face_image_path']\n",
    "            elif self._data_type == 'context':\n",
    "                image_path = sample_dict['global_image_path']\n",
    "            \n",
    "            images_batch[local_idx] = self._load_image_array(image_path)\n",
    "\n",
    "        return images_batch\n",
    "        \n",
    "    def _save_features(self, output_path, features_array):\n",
    "        self._log.debug('Saving features to: {}'.format(output_path))        \n",
    "        np.save(output_path, features_array)\n",
    "    \n",
    "    def _load_image_array(self, image_path):\n",
    "        self._log.debug('Loading image: {}'.format(image_path))\n",
    "        \n",
    "        img = keras.preprocessing.image.load_img(\n",
    "            image_path, \n",
    "            target_size=self._target_size\n",
    "        )\n",
    "\n",
    "        x = keras.preprocessing.image.img_to_array(img)\n",
    "        \n",
    "        if self._preprocessing_function:\n",
    "            x = self._preprocessing_function(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = conf.batch_size\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_type in ('body', 'face'):\n",
    "    models = get_models(data_type, layer='relu7')\n",
    "    \n",
    "    for attribute, model in models.items():\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "        extract_features_cbk = lambda x : model.predict(x, batch_size=batch_size)\n",
    "\n",
    "        features_path = os.path.join(conf.features_dir, attribute + '.npy')\n",
    "\n",
    "        feature_extractor = FeatureExtractor(\n",
    "                dataset_path=conf.dataset_path,\n",
    "                extract_features_cbk=model.predict,\n",
    "                data_type=data_type,\n",
    "                features_path=features_path,\n",
    "                preprocessing_function=imagenet_utils.preprocess_input,\n",
    "                target_size=(227, 227),\n",
    "                batch_size=batch_size,\n",
    "        )\n",
    "        \n",
    "        _ = feature_extractor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attribute = 'activity'\n",
    "data_type = 'context'\n",
    "features_path = os.path.join(conf.features_dir, attribute + '.npy')\n",
    "\n",
    "model = get_activity_model()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "rescale_0_1 = lambda x : x / 255.0\n",
    "extract_features_cbk = lambda x : model.predict(x, batch_size=batch_size)\n",
    "\n",
    "feature_extractor = FeatureExtractor(\n",
    "        dataset_path=conf.dataset_path,\n",
    "        extract_features_cbk=extract_features_cbk,\n",
    "        data_type=data_type,\n",
    "        features_path=features_path,\n",
    "        preprocessing_function=rescale_0_1,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    ")\n",
    "\n",
    "_ = feature_extractor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalEncoderHelper:\n",
    "    def __init__(self, categories, attribute_idx):\n",
    "        self.encoding_map = {cls:i for i, cls in enumerate(categories)}        \n",
    "        self.encode = np.vectorize(lambda cls : self.encoding_map[cls])        \n",
    "        self.attribute_idx = attribute_idx\n",
    "\n",
    "    def __call__(self, batch_x):\n",
    "        attribute_batch = batch_x[self.attribute_idx]\n",
    "        return to_categorical(self.encode(attribute_batch), num_classes=len(self.encoding_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_user_attributes = [\n",
    "    ('camera_user_gender', ('male', 'female')),\n",
    "    ('camera_user_age', ('infant', 'child', 'young', 'middleAge', 'senior', 'unknown')),\n",
    "]\n",
    "\n",
    "data_type = 'camera_user_data'\n",
    "\n",
    "for attribute, categories in camera_user_attributes:\n",
    "\n",
    "    features_path = os.path.join(conf.features_dir, attribute + '.npy')\n",
    "    encoder_cbk = CategoricalEncoderHelper(categories, attribute)\n",
    "\n",
    "    feature_extractor = FeatureExtractor(\n",
    "        dataset_path=conf.dataset_path,\n",
    "        extract_features_cbk=encoder_cbk,\n",
    "        data_type=data_type,\n",
    "        features_path=features_path,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    _ = feature_extractor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.listdir(conf.features_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
