{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from __future__ import print_function\n",
    "\n",
    "PROJECT_DIR = '/root/shared/Documents/final_proj'\n",
    "BASE_MODELS_DIR = os.path.join(PROJECT_DIR, 'models/trained_models')\n",
    "ATTR_MODELS_DIR = os.path.join(BASE_MODELS_DIR, 'attribute_models')\n",
    "BASE_FEATURES_DIR = os.path.join(PROJECT_DIR, 'extracted_features')\n",
    "SVM_MODELS_DIR = os.path.join(PROJECT_DIR, 'models/svm_models')\n",
    "SPLITS_DIR = os.path.join(PROJECT_DIR,'datasets/splits/annotator_consistency3')\n",
    "STATS_MODELS_DIR = os.path.join(SVM_MODELS_DIR, 'stats')\n",
    "\n",
    "ARCH = 'caffeNet'\n",
    "LAYER = 'fc7'\n",
    "\n",
    "DOMAIN='domain'\n",
    "RELATION='relation'\n",
    "DATA_TYPE=DOMAIN\n",
    "\n",
    "CONFIG = LAYER + '_' + DATA_TYPE + '_' + ARCH\n",
    "\n",
    "if DATA_TYPE == DOMAIN:\n",
    "    labels_path = os.path.join(SPLITS_DIR,'domain_single_body1_{}_5.txt')\n",
    "else:\n",
    "    labels_path = os.path.join(SPLITS_DIR,'single_body1_{}_16.txt')\n",
    "\n",
    "end2end_model = True\n",
    "\n",
    "if end2end_model:\n",
    "    FEATURES_DIR = os.path.join(BASE_FEATURES_DIR, 'end_to_end_features', CONFIG)    \n",
    "else:    \n",
    "    FEATURES_DIR = os.path.join(BASE_FEATURES_DIR, 'attribute_features', CONFIG)\n",
    "\n",
    "stored_features_dir = os.path.join(FEATURES_DIR, 'all_splits_numpy_format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caffe\n",
    "from caffe.proto import caffe_pb2\n",
    "import leveldb\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "def load_levelDB_as_array(db_path):\n",
    "    db = leveldb.LevelDB(db_path)\n",
    "    datum = caffe_pb2.Datum()\n",
    "\n",
    "    items = []\n",
    "\n",
    "    for key, value in db.RangeIter():\n",
    "        datum.ParseFromString(value)\n",
    "        data = caffe.io.datum_to_array(datum)\n",
    "        items.append(data)\n",
    "\n",
    "    result = np.array(items).reshape(len(items), len(items[0]))\n",
    "    return result\n",
    "\n",
    "def preprocess_attributes(levelDB_dirs=None, raw_numpy_dirs=None, matlab_dirs=None):\n",
    "\n",
    "    splits = ['train', 'test', 'eval']\n",
    "    attribute_features = {split:{} for split in splits}\n",
    "\n",
    "    ###########################################################################\n",
    "    # features in levelDB format\n",
    "    if levelDB_dirs:\n",
    "        for directory in levelDB_dirs:\n",
    "            for split in splits:\n",
    "                attribute_models = os.listdir(os.path.join(directory, split))\n",
    "\n",
    "                for attr_name in attribute_models:\n",
    "                    features = load_levelDB_as_array(os.path.join(directory, split, attr_name))\n",
    "                    attribute_features[split][attr_name] = features\n",
    "                    print(\"Convert from levelDB format {} dataset attribute {} with dim: {}\".format(split, attr_name, features.shape))\n",
    "\n",
    "    ###########################################################################\n",
    "    # features in numpy format\n",
    "    if raw_numpy_dirs:\n",
    "        for directory in raw_numpy_dirs:\n",
    "            for numpy_file in os.listdir(directory):\n",
    "                filename, ext = os.path.splitext(numpy_file)\n",
    "                if ext.lower() == '.npy':\n",
    "                    # load numpy\n",
    "                    features = np.load(os.path.join(directory, numpy_file))\n",
    "                    # find split\n",
    "                    for candidate_split in splits:\n",
    "                        if candidate_split in filename:\n",
    "                            split = candidate_split\n",
    "                            break\n",
    "                    else:\n",
    "                        split = None\n",
    "                    # the folder name is the attribute name\n",
    "                    attr_name = os.path.basename(directory)\n",
    "\n",
    "                    attribute_features[split][attr_name] = features\n",
    "                    print(\"Load numpy format {} dataset attribute {} with dim: {}\".format(split, attr_name, features.shape))\n",
    "            \n",
    "    ###########################################################################\n",
    "    # features in matlab format\n",
    "    if matlab_dirs:\n",
    "        for directory in matlab_dirs:\n",
    "            for matfile in os.listdir(directory):\n",
    "                filename, ext = os.path.splitext(matfile)\n",
    "                if ext.lower() == '.mat':\n",
    "                    # load matfile (dict format)\n",
    "                    matfile_dict = scipy.io.loadmat(os.path.join(directory, matfile))\n",
    "                    attr_name, split = filename.rsplit('_', 1)\n",
    "                    # access numpy field\n",
    "                    features = matfile_dict[attr_name]\n",
    "                    attribute_features[split][attr_name] = features\n",
    "                    print(\"Convert from matlab format {} dataset attribute {} with dim: {}\".format(split, attr_name, features.shape))\n",
    "\n",
    "    ###########################################################################\n",
    "    \n",
    "    return attribute_features\n",
    "\n",
    "def save_features(attribute_features, features_dir, compressed=True):\n",
    "    if not (os.path.exists(features_dir) and os.path.isdir(features_dir)):\n",
    "        os.mkdir(features_dir)\n",
    "    \n",
    "    for split, attributes in attribute_features.items():\n",
    "        for attr_name, features in attributes.items():\n",
    "            features_path = os.path.join(features_dir, '{}_{}').format(attr_name, split)\n",
    "            \n",
    "            # save file in compress format and float16\n",
    "            if compressed:\n",
    "                np.savez_compressed(features_path, features.astype(np.float16))\n",
    "            else:\n",
    "                np.save(features_path, features)\n",
    "            \n",
    "            print(\"Saved {}.{} ...\".format(features_path, 'npz' if compressed else 'np'))\n",
    "\n",
    "def load_features(features_dir):\n",
    "    attribute_features = {split:{} for split in ['train', 'test', 'eval']}\n",
    "\n",
    "    for numpy_file in os.listdir(features_dir):        \n",
    "        # Split the extension from the path and normalise it to lowercase.        \n",
    "        filename, ext = os.path.splitext(numpy_file)\n",
    "        ext = ext.lower()\n",
    "\n",
    "        # path\n",
    "        features_path = os.path.join(features_dir, numpy_file)\n",
    "        \n",
    "        if ext == '.npz':\n",
    "            with np.load(features_path) as data:\n",
    "                features = data['arr_0']\n",
    "        elif ext == '.npy':\n",
    "            features = np.load(features_path)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        attr_name, split = filename.rsplit('_', 1)\n",
    "        attribute_features[split][attr_name] = features\n",
    "\n",
    "        print(\"Loading {}...\".format(features_path))\n",
    "    \n",
    "    return attribute_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /root/shared/Documents/final_proj/extracted_features/end_to_end_features/fc7_domain_caffeNet/all_splits_numpy_format/domain_body_train.npz...\n",
      "Loading /root/shared/Documents/final_proj/extracted_features/end_to_end_features/fc7_domain_caffeNet/all_splits_numpy_format/domain_body_eval.npz...\n",
      "Loading /root/shared/Documents/final_proj/extracted_features/end_to_end_features/fc7_domain_caffeNet/all_splits_numpy_format/domain_body_test.npz...\n"
     ]
    }
   ],
   "source": [
    "process_features = False\n",
    "\n",
    "# preprocess features from original formats (leveldb, numpy, matlab)\n",
    "if process_features:\n",
    "    if end2end_model:\n",
    "        # LEVELDB DIRS\n",
    "        levelDB_dirs = [FEATURES_DIR]\n",
    "        attribute_features = preprocess_attributes(levelDB_dirs)\n",
    "    else:\n",
    "        # LEVELDB DIRS\n",
    "        levelDB_dirs = [FEATURES_DIR]\n",
    "        # MATLAB DIRS\n",
    "        matlab_dirs = [os.path.join(ATTR_MODELS_DIR,'localation_scale_data(annotator_consistency3)')]        \n",
    "        # NUMPY DIRS\n",
    "        numpy_dirs = [os.path.join(ATTR_MODELS_DIR, 'imsitu_body_activity(annotator_consistency3)'),\n",
    "                      os.path.join(ATTR_MODELS_DIR, 'body_immediacy(annotator_consistency3)')]          \n",
    "\n",
    "        attribute_features = preprocess_attributes(levelDB_dirs, numpy_dirs, matlab_dirs)\n",
    "\n",
    "    if not (os.path.exists(stored_features_dir) and os.path.isdir(stored_features_dir)):\n",
    "        os.mkdir(stored_features_dir)\n",
    "    \n",
    "    # save features to disk\n",
    "    save_features(attribute_features, stored_features_dir, compressed=True)\n",
    "\n",
    "else:\n",
    "    # load features from disk\n",
    "    attribute_features = load_features(stored_features_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attribute selector\n",
    "\n",
    "all_attributes = sorted(attribute_features['test'].keys())\n",
    "get_attr_by_keyword = lambda list_attrs, single_attr: [attr_name for attr_name in list_attrs if single_attr in attr_name]\n",
    "\n",
    "body_attributes = get_attr_by_keyword(all_attributes, 'body')\n",
    "face_attributes = get_attr_by_keyword(all_attributes, 'face') + get_attr_by_keyword(all_attributes, 'head')\n",
    "selector = {'all': all_attributes, 'body': body_attributes, 'face': face_attributes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_iter(attributes):\n",
    "    unique_attrs = set([attr[:-2] if attr.endswith('_1') or attr.endswith('_2') else attr for attr in attributes])\n",
    "\n",
    "    for item in sorted(unique_attrs):\n",
    "        yield item\n",
    "\n",
    "if end2end_model:\n",
    "    seq = attribute_iter(all_attributes)\n",
    "else:\n",
    "    seq = attribute_iter(['all', 'body', 'face'] + all_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'domain_body'"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = seq.next()\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['domain_body']"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select all/body/face or use single attribute (prefix is enough, e.g. body_gender)\n",
    "\n",
    "query = 'all'\n",
    "selected_attributes = selector[query] if query in selector else get_attr_by_keyword(all_attributes, query)\n",
    "selected_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate attributes\n",
    "\n",
    "fused_features = {}\n",
    "labels = {}\n",
    "\n",
    "for split in ['train', 'test', 'eval']:\n",
    "    selected_features = [attribute_features[split][attr_name] for attr_name in selected_attributes]\n",
    "    fused_features[split] = np.concatenate(selected_features, axis=1)\n",
    "\n",
    "    with open(labels_path.format(split)) as file_label_list:\n",
    "        labels[split] = np.array([file_label.split()[1] for file_label in file_label_list], dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define splits\n",
    "\n",
    "X_train = fused_features['train']\n",
    "y_train = labels['train']\n",
    "X_test = fused_features['eval']\n",
    "y_test = labels['eval']\n",
    "X_val = fused_features['test']\n",
    "y_val = labels['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# normalize data ?\n",
    "normalize = False\n",
    "if normalize:\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test  = scaler.transform(X_test)\n",
    "    X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "\n",
    "# max number of iterations\n",
    "max_iter = 1000\n",
    "# deal with unbalanced classes by using penalization weights\n",
    "fix_unbalanced=False\n",
    "if fix_unbalanced:\n",
    "    class_weight = 'balanced'\n",
    "else:\n",
    "    class_weight = None\n",
    "\n",
    "# defined classifier format\n",
    "# prefix: as needed (e.g. query) | mtype: end2end or attr |\n",
    "# nnarch: e.g. caffeNet or VGG | dtype: domain or relation |\n",
    "# clf: description of the classifier | \n",
    "# ext: file extension (b for serialized binary objects, txt for text)\n",
    "# HINT: ordering chosen to match FEATURES_DIR format\n",
    "FILE_FORMAT = '{prefix}_{mtype}_{layer}_{dtype}_{nnarch}_{clf}'\n",
    "\n",
    "# pickle dumps extension\n",
    "CLF_DUMP_EXT = '.b'\n",
    "\n",
    "# fill file format\n",
    "load_model_prefix = query\n",
    "model_type = 'end2end' if end2end_model else 'attr'\n",
    "balance_descr = 'balanced' if class_weight == 'balanced' else 'unbalanced'\n",
    "clf_description = '{}_loss_{}_epochs_{}_{}'.format('sgd', 'squared_hinge', max_iter, balance_descr)\n",
    "\n",
    "PREFILLED_FILE_FORMAT = FILE_FORMAT.format(prefix=load_model_prefix, mtype=model_type, layer=LAYER,\n",
    "                                           nnarch=ARCH, dtype=DATA_TYPE, clf=clf_description)\n",
    "pretrained_bin = PREFILLED_FILE_FORMAT + CLF_DUMP_EXT\n",
    "\n",
    "# (pre)trained SVM path to load/save from/to disk\n",
    "load_model_path = os.path.join(SVM_MODELS_DIR, pretrained_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: /root/shared/Documents/final_proj/models/svm_models/all_end2end_fc7_domain_caffeNet_sgd_loss_squared_hinge_epochs_1000_unbalanced.b\n",
      "Training SGD (SVM loss)  from scratch...\n"
     ]
    }
   ],
   "source": [
    "# train a svm model from scratch or load it from disk when possible \n",
    "\n",
    "import pickle\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# reuse precomputed model?\n",
    "reuse_model = True\n",
    "\n",
    "# automatically computed (don't modify)\n",
    "compute_model = not reuse_model\n",
    "\n",
    "if reuse_model:    \n",
    "    if os.path.exists(load_model_path):\n",
    "        print(\"Loading precomputed SGD (SVM loss) from disk...\")\n",
    "        with open(load_model_path, 'rb') as f:\n",
    "            clf = pickle.load(f)\n",
    "    else:\n",
    "        # if model is not found, recompute from scratch\n",
    "        print(\"File not found: {}\".format(load_model_path))\n",
    "        compute_model = True\n",
    "\n",
    "if compute_model:\n",
    "    clf = SGDClassifier(loss=\"squared_hinge\", alpha=0.0001, max_iter=max_iter, n_jobs=-1, \n",
    "                        average=True, tol=1e-3, class_weight=class_weight)\n",
    "\n",
    "    print(\"Training SGD (SVM loss)  from scratch...\")\n",
    "    clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=0.0001, average=True, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='squared_hinge', max_iter=1000,\n",
      "       n_iter=None, n_jobs=-1, penalty='l2', power_t=0.5,\n",
      "       random_state=None, shuffle=True, tol=0.001, verbose=0,\n",
      "       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep training\n",
    "\n",
    "extend_training = False\n",
    "extra_iters = 200\n",
    "\n",
    "# for experimenting purposes, consider changing the file name\n",
    "# before storing the model \n",
    "if extend_training:\n",
    "    clf.max_iter = extra_iters\n",
    "    clf.warm_start = True\n",
    "    print(\"Extending training of SGD for {} iterations...\".format(extra_iters))\n",
    "    clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statistics(val_stats=None, test_stats=None, fdesc=sys.stdout):\n",
    "    for description, stats in [('Validation set:', val_stats), (('Test set:', test_stats))]:\n",
    "        \n",
    "        if stats is not None:\n",
    "            print(description, file=fdesc)\n",
    "            accuracy, confusion_matrix, report = stats\n",
    "            print('Confusion matrix:', file=fdesc)\n",
    "            print(confusion_matrix, file=fdesc)\n",
    "            print(file=fdesc)\n",
    "            print(report, file=fdesc)\n",
    "            print('SGD accuracy: {:.3f}'.format(accuracy), file=fdesc)\n",
    "            print('------------------------------------------------', file=fdesc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set:\n",
      "Confusion matrix:\n",
      "[[  3  34  17   0  26]\n",
      " [  0 127   7   1 155]\n",
      " [  1  15   7   0  26]\n",
      " [  1  10   0   0  18]\n",
      " [  4 130   0   1 126]]\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.33      0.04      0.07        80\n",
      "          1       0.40      0.44      0.42       290\n",
      "          2       0.23      0.14      0.18        49\n",
      "          3       0.00      0.00      0.00        29\n",
      "          4       0.36      0.48      0.41       261\n",
      "\n",
      "avg / total       0.35      0.37      0.34       709\n",
      "\n",
      "SGD accuracy: 0.371\n",
      "------------------------------------------------\n",
      "Test set:\n",
      "Confusion matrix:\n",
      "[[  82  216    4    0   82]\n",
      " [ 158 1280   36    3  600]\n",
      " [  23  114   24    0  152]\n",
      " [  16   41    4    2  129]\n",
      " [  37  323    3    0 1777]]\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.26      0.21      0.23       384\n",
      "          1       0.65      0.62      0.63      2077\n",
      "          2       0.34      0.08      0.12       313\n",
      "          3       0.40      0.01      0.02       192\n",
      "          4       0.65      0.83      0.73      2140\n",
      "\n",
      "avg / total       0.59      0.62      0.59      5106\n",
      "\n",
      "SGD accuracy: 0.620\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "def compute_stats(X, y, clf):\n",
    "    y_predicted = clf.predict(X)\n",
    "    acc = sklearn.metrics.accuracy_score(y, y_predicted)\n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(y, y_predicted)\n",
    "    report = sklearn.metrics.classification_report(y, y_predicted)\n",
    "\n",
    "    return acc, confusion_matrix, report\n",
    "\n",
    "val_stats, test_stats = compute_stats(X_val, y_val, clf), compute_stats(X_test, y_test, clf)\n",
    "print_statistics(val_stats=val_stats, test_stats=test_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store statistics in disk\n",
    "\n",
    "STATS_EXT = '.txt'\n",
    "save_stats = False\n",
    "\n",
    "if save_stats:\n",
    "    # create directory if it doesn't exist already\n",
    "    if not (os.path.exists(STATS_MODELS_DIR) and os.path.isdir(STATS_MODELS_DIR)):\n",
    "        os.mkdir(STATS_MODELS_DIR)\n",
    "\n",
    "    stats_file = PREFILLED_FILE_FORMAT + STATS_EXT\n",
    "    stats_path = os.path.join(STATS_MODELS_DIR, stats_file)\n",
    "\n",
    "    with open(stats_path, 'wt') as f:\n",
    "        print(\"Storing statistics in {}...\".format(stats_path)) # to stdout\n",
    "        print_statistics(val_stats=val_stats, test_stats=test_stats, fdesc=f) # to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained model in serialized binary format\n",
    "\n",
    "save_model = False\n",
    "if save_model:\n",
    "    # a different name could be used\n",
    "    store_model_path = os.path.join(SVM_MODELS_DIR, pretrained_bin)\n",
    "    \n",
    "    if not (os.path.exists(SVM_MODELS_DIR) and os.path.isdir(SVM_MODELS_DIR)):\n",
    "        os.mkdir(SVM_MODELS_DIR)\n",
    "        \n",
    "    with open(store_model_path, 'wb') as f:\n",
    "        pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train other classifiers\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "train_rf = False\n",
    "if train_rf:\n",
    "    clf_rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, class_weight=class_weight)\n",
    "    print(\"Training Random Forest from scratch...\")\n",
    "    clf_rf.fit(X_train, y_train)\n",
    "    print_statistics(test_stats=compute_stats(X_test, y_test, clf_rf))\n",
    "    \n",
    "train_svm = False\n",
    "if train_svm:\n",
    "    C = 1.0\n",
    "    clf_svm = LinearSVC(C=C, class_weight='balanced')\n",
    "    \n",
    "    print(\"Training LinearSVC C={} from scratch...\".format(C))\n",
    "    clf_svm.fit(X_train, y_train)\n",
    "    print_statistics(test_stats=compute_stats(X_test, y_test, clf_svm))"
   ]
  }
 ],
 "metadata": {
  "description": "Instant recognition with a pre-trained model and a tour of the net interface for visualizing features and parameters layer-by-layer.",
  "example_name": "Image Classification and Filter Visualization",
  "include_in_docs": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "priority": 1
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
