{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from __future__ import print_function\n",
    "\n",
    "PROJECT_DIR = '/root/shared/Documents/final_proj'\n",
    "BASE_MODELS_DIR = os.path.join(PROJECT_DIR, 'models/trained_models')\n",
    "ATTR_MODELS_DIR = os.path.join(BASE_MODELS_DIR, 'attribute_models')\n",
    "BASE_FEATURES_DIR = os.path.join(PROJECT_DIR, 'extracted_features')\n",
    "SVM_MODELS_DIR = os.path.join(PROJECT_DIR, 'models/svm_models')\n",
    "STATS_MODELS_DIR = os.path.join(SVM_MODELS_DIR, 'stats')\n",
    "\n",
    "LAYER = 'fc7'\n",
    "\n",
    "DOMAIN='domain'\n",
    "RELATION='relation'\n",
    "DATA_TYPE=DOMAIN\n",
    "\n",
    "LAYER_DIR = LAYER + '_' + DATA_TYPE\n",
    "if DATA_TYPE == DOMAIN:\n",
    "    labels_path = '../../datasets/splits/relation_consistency3/domain_single_body1_{}_5.txt'\n",
    "else:\n",
    "    labels_path = '../../datasets/splits/annotator_consistency3/single_body1_{}_16.txt'    \n",
    "\n",
    "end2end_model = True\n",
    "if end2end_model:\n",
    "    FEATURES_DIR = os.path.join(BASE_FEATURES_DIR, 'end_to_end_features', LAYER_DIR)\n",
    "    stored_features_dir = os.path.join(FEATURES_DIR, 'all_splits_numpy_format')    \n",
    "else:    \n",
    "    FEATURES_DIR = os.path.join(BASE_FEATURES_DIR, 'attribute_features', LAYER_DIR)\n",
    "    stored_features_dir = os.path.join(FEATURES_DIR, 'all_splits_numpy_format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caffe\n",
    "from caffe.proto import caffe_pb2\n",
    "import leveldb\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "def load_levelDB_as_array(db_path):\n",
    "    db = leveldb.LevelDB(db_path)\n",
    "    datum = caffe_pb2.Datum()\n",
    "\n",
    "    items = []\n",
    "\n",
    "    for key, value in db.RangeIter():\n",
    "        datum.ParseFromString(value)\n",
    "        data = caffe.io.datum_to_array(datum)\n",
    "        items.append(data)\n",
    "\n",
    "    result = np.array(items).reshape(len(items), len(items[0]))\n",
    "    return result\n",
    "\n",
    "def preprocess_attributes(levelDB_dirs=None, raw_numpy_dirs=None, matlab_dirs=None):\n",
    "\n",
    "    splits = ['train', 'test', 'eval']\n",
    "    attribute_features = {split:{} for split in splits}\n",
    "\n",
    "    ###########################################################################\n",
    "    # features in levelDB format\n",
    "    if levelDB_dirs:\n",
    "        for directory in levelDB_dirs:\n",
    "            for split in splits:\n",
    "                attribute_models = os.listdir(os.path.join(directory, split))\n",
    "\n",
    "                for attr_name in attribute_models:\n",
    "                    features = load_levelDB_as_array(os.path.join(directory, split, attr_name))\n",
    "                    attribute_features[split][attr_name] = features\n",
    "                    print(\"Convert from levelDB format {} dataset attribute {} with dim: {}\".format(split, attr_name, features.shape))\n",
    "\n",
    "    ###########################################################################\n",
    "    # features in numpy format\n",
    "    if raw_numpy_dirs:\n",
    "        for directory in raw_numpy_dirs:\n",
    "            for numpy_file in os.listdir(directory):\n",
    "                filename, ext = os.path.splitext(numpy_file)\n",
    "                if ext.lower() == '.npy':\n",
    "                    # load numpy\n",
    "                    features = np.load(os.path.join(directory, numpy_file))\n",
    "                    # find split\n",
    "                    for candidate_split in splits:\n",
    "                        if candidate_split in filename:\n",
    "                            split = candidate_split\n",
    "                            break\n",
    "                    else:\n",
    "                        split = None\n",
    "                    # the folder name is the attribute name\n",
    "                    attr_name = os.path.basename(directory)\n",
    "\n",
    "                    attribute_features[split][attr_name] = features\n",
    "                    print(\"Load numpy format {} dataset attribute {} with dim: {}\".format(split, attr_name, features.shape))\n",
    "            \n",
    "    ###########################################################################\n",
    "    # features in matlab format\n",
    "    if matlab_dirs:\n",
    "        for directory in matlab_dirs:\n",
    "            for matfile in os.listdir(directory):\n",
    "                filename, ext = os.path.splitext(matfile)\n",
    "                if ext.lower() == '.mat':\n",
    "                    # load matfile (dict format)\n",
    "                    matfile_dict = scipy.io.loadmat(os.path.join(directory, matfile))\n",
    "                    attr_name, split = filename.rsplit('_', 1)\n",
    "                    # access numpy field\n",
    "                    features = matfile_dict[attr_name]\n",
    "                    attribute_features[split][attr_name] = features\n",
    "                    print(\"Convert from matlab format {} dataset attribute {} with dim: {}\".format(split, attr_name, features.shape))\n",
    "\n",
    "    ###########################################################################\n",
    "    \n",
    "    return attribute_features\n",
    "\n",
    "def save_features(attribute_features, features_dir):\n",
    "    for split, attributes in attribute_features.items():\n",
    "        for attr_name, features in attributes.items():\n",
    "            features_path = os.path.join(features_dir, '{}_{}').format(attr_name, split)\n",
    "            \n",
    "            # save file in compress format and float16\n",
    "            # np.save(features_path, features)\n",
    "            np.savez_compressed(features_path, features.astype(np.float16))\n",
    "            \n",
    "            print(\"Saved {}...\".format(features_path))\n",
    "\n",
    "def load_features(features_dir):\n",
    "    attribute_features = {split:{} for split in ['train', 'test', 'eval']}\n",
    "\n",
    "    for numpy_file in os.listdir(features_dir):        \n",
    "        # Split the extension from the path and normalise it to lowercase.        \n",
    "        filename, ext = os.path.splitext(numpy_file)\n",
    "        ext = ext.lower()\n",
    "\n",
    "        # path\n",
    "        features_path = os.path.join(features_dir, numpy_file)\n",
    "        \n",
    "        if ext == '.npz':\n",
    "            with np.load(features_path) as data:\n",
    "                features = data['arr_0']\n",
    "        elif ext == '.npy':\n",
    "            features = np.load(features_path)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        attr_name, split = filename.rsplit('_', 1)\n",
    "        attribute_features[split][attr_name] = features\n",
    "\n",
    "        print(\"Loading {}...\".format(features_path))\n",
    "    \n",
    "    return attribute_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /root/shared/Documents/final_proj/extracted_features/end_to_end_features/fc7_domain/all_splits_numpy_format/domain_body_train.npy...\n",
      "Loading /root/shared/Documents/final_proj/extracted_features/end_to_end_features/fc7_domain/all_splits_numpy_format/domain_body_test.npy...\n",
      "Loading /root/shared/Documents/final_proj/extracted_features/end_to_end_features/fc7_domain/all_splits_numpy_format/domain_body_eval.npy...\n"
     ]
    }
   ],
   "source": [
    "process_features = False\n",
    "\n",
    "# preprocess features from original formats (leveldb, numpy, matlab)\n",
    "if process_features:\n",
    "    if end2end_model:\n",
    "        # LEVELDB DIRS\n",
    "        levelDB_dirs = [FEATURES_DIR]\n",
    "        attribute_features = preprocess_attributes(levelDB_dirs)\n",
    "        pass\n",
    "    else:\n",
    "        if DATA_TYPE == RELATION:\n",
    "            # LEVELDB DIRS\n",
    "            levelDB_dirs = [FEATURES_DIR]\n",
    "            # MATLAB DIRS\n",
    "            matlab_dirs = [os.path.join(ATTR_MODELS_DIR,'localation_scale_data(annotator_consistency3)')]        \n",
    "            # NUMPY DIRS\n",
    "            numpy_dirs = [os.path.join(ATTR_MODELS_DIR, 'imsitu_body_activity(annotator_consistency3)'),\n",
    "                          os.path.join(ATTR_MODELS_DIR, 'body_immediacy(annotator_consistency3)')]\n",
    "        else:\n",
    "            # LEVELDB DIRS\n",
    "            levelDB_dir = [FEATURES_DIR]\n",
    "            # MATLAB DIRS\n",
    "            matlab_dirs = None\n",
    "            # NUMPY DIRS\n",
    "            numpy_dirs = [os.path.join(ATTR_MODELS_DIR, 'imsitu_body_activity(relation_consistency3)')]            \n",
    "\n",
    "        attribute_features = preprocess_attributes(levelDB_dirs, numpy_dirs, matlab_dirs)\n",
    "    \n",
    "\n",
    "    if not (os.path.exists(stored_features_dir) and os.path.isdir(stored_features_dir)):\n",
    "        os.mkdir(stored_features_dir)\n",
    "    \n",
    "    # save features to disk\n",
    "    save_features(attribute_features, stored_features_dir)\n",
    "\n",
    "else:\n",
    "    # load features from disk\n",
    "    attribute_features = load_features(stored_features_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attribute selector\n",
    "\n",
    "all_attributes = sorted(attribute_features['test'].keys())\n",
    "get_attr_by_keyword = lambda list_attrs, single_attr: [attr_name for attr_name in list_attrs if single_attr in attr_name]\n",
    "\n",
    "body_attributes = get_attr_by_keyword(all_attributes, 'body')\n",
    "face_attributes = get_attr_by_keyword(all_attributes, 'face') + get_attr_by_keyword(all_attributes, 'head')\n",
    "selector = {'all': all_attributes, 'body': body_attributes, 'face': face_attributes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_iter(attributes):\n",
    "    unique_attrs = set([attr[:-2] if attr.endswith('_1') or attr.endswith('_2') else attr for attr in attributes])\n",
    "\n",
    "    for item in sorted(unique_attrs):\n",
    "        yield item\n",
    "\n",
    "if end2end_model:\n",
    "    seq = attribute_iter(all_attributes)\n",
    "else:\n",
    "    seq = attribute_iter(['all', 'body', 'face'] + all_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'domain_body'"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = seq.next()\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['domain_body']"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select all/body/face or use single attribute (prefix is enough, e.g. body_gender)\n",
    "\n",
    "#query = 'all'\n",
    "selected_attributes = selector[query] if query in selector else get_attr_by_keyword(all_attributes, query)\n",
    "selected_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate attributes\n",
    "\n",
    "fused_features = {}\n",
    "for split in ['train', 'test', 'eval']:\n",
    "    selected_features = [attribute_features[split][attr_name] for attr_name in selected_attributes]\n",
    "    fused_features[split] = np.concatenate(selected_features, axis=1)\n",
    "\n",
    "labels = {}\n",
    "for split in ['train', 'test', 'eval']:\n",
    "    with open(labels_path.format(split)) as file_label_list:\n",
    "        labels[split] = np.array([file_label.split()[1] for file_label in file_label_list], dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define splits\n",
    "\n",
    "full_training_set = False\n",
    "if full_training_set:\n",
    "    X_train = np.concatenate([fused_features['train'], fused_features['test']])\n",
    "    X_test = fused_features['eval']\n",
    "    y_train = np.concatenate([labels['train'], labels['test']])\n",
    "    y_test = labels['eval']\n",
    "else:\n",
    "    X_train = fused_features['train']\n",
    "    y_train = labels['train']\n",
    "    X_test = fused_features['eval']\n",
    "    y_test = labels['eval']\n",
    "    X_val = fused_features['test']\n",
    "    y_val = labels['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# normalize data ?\n",
    "normalize = False\n",
    "if normalize:\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test  = scaler.transform(X_test)\n",
    "    \n",
    "    if not full_training_set:\n",
    "        X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "\n",
    "# max number of iterations\n",
    "max_iter = 1000\n",
    "\n",
    "# pickle dumps extension\n",
    "CLF_DUMP_EXT = 'b'\n",
    "\n",
    "# defined classifier format\n",
    "FILE_FORMAT = '{prefix}_{mtype}_{dtype}_{clf}_{epochs}.{ext}'\n",
    "\n",
    "load_model_prefix = query\n",
    "model_type = 'end2end' if end2end_model else 'attr'\n",
    "clf_description = 'sgd_squared_hinge'\n",
    "\n",
    "pretrained_bin = FILE_FORMAT.format(prefix=load_model_prefix, mtype=model_type, clf=clf_description, \n",
    "                                    dtype=DATA_TYPE, epochs=max_iter, ext=CLF_DUMP_EXT)\n",
    "\n",
    "load_model_path = os.path.join(SVM_MODELS_DIR, pretrained_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SGD (SVM loss)  from scratch...\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# reuse precomputed model?\n",
    "reuse_model = False\n",
    "\n",
    "# automatically computed (don't modify)\n",
    "compute_model = not reuse_model\n",
    "\n",
    "if reuse_model:    \n",
    "    if os.path.exists(load_model_path):\n",
    "        print(\"Loading precomputed SGD (SVM loss) from disk...\")\n",
    "        with open(load_model_path, 'rb') as f:\n",
    "            clf = pickle.load(f)\n",
    "    else:\n",
    "        # if model is not found, recompute from scratch\n",
    "        print(\"File not found: {}\".format(load_model_path))\n",
    "        compute_model = True\n",
    "\n",
    "if compute_model:\n",
    "    clf = SGDClassifier(loss=\"squared_hinge\", alpha=0.0001, max_iter=max_iter, n_jobs=-1, \n",
    "                        average=True, tol=1e-3, class_weight='balanced')\n",
    "\n",
    "    print(\"Training SGD (SVM loss)  from scratch...\")\n",
    "    clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=0.0001, average=True, class_weight='balanced',\n",
      "       epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='squared_hinge', max_iter=1000,\n",
      "       n_iter=None, n_jobs=-1, penalty='l2', power_t=0.5,\n",
      "       random_state=None, shuffle=True, tol=0.001, verbose=0,\n",
      "       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep training\n",
    "extend_training = False\n",
    "extra_iters = 200\n",
    "\n",
    "# for experimenting purposes, if you want to save\n",
    "if extend_training:\n",
    "    clf.max_iter = extra_iters\n",
    "    clf.warm_start = True\n",
    "    print(\"Extending training of SGD for {} iterations...\".format(extra_iters))\n",
    "    clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[ 138  172   37    4   61]\n",
      " [ 326 1237  300   37  556]\n",
      " [  38   73   92    3  124]\n",
      " [  23   47   21   35   87]\n",
      " [  96  401   72   44 1747]]\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.22      0.33      0.27       412\n",
      "          1       0.64      0.50      0.56      2456\n",
      "          2       0.18      0.28      0.22       330\n",
      "          3       0.28      0.16      0.21       213\n",
      "          4       0.68      0.74      0.71      2360\n",
      "\n",
      "avg / total       0.59      0.56      0.57      5771\n",
      "\n",
      "SGD accuracy: 0.563\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "y_predicted = clf.predict(X_test)\n",
    "report = sklearn.metrics.classification_report(y_test, y_predicted)\n",
    "acc = sklearn.metrics.accuracy_score(y_test, y_predicted)\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_predicted)\n",
    "\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix)\n",
    "print()\n",
    "print(report)\n",
    "print('SGD accuracy: {:.3f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing statistics in /root/shared/Documents/final_proj/models/svm_models/stats/domain_body_end2end_domain_sgd_squared_hinge_1000.txt...\n"
     ]
    }
   ],
   "source": [
    "# store statistics in disk\n",
    "\n",
    "STATS_EXT = 'txt'\n",
    "save_stats = True\n",
    "\n",
    "if save_stats:\n",
    "    # create directory if it doesn't exist already\n",
    "    if not (os.path.exists(STATS_MODELS_DIR) and os.path.isdir(STATS_MODELS_DIR)):\n",
    "        os.mkdir(STATS_MODELS_DIR)\n",
    "\n",
    "    stats_file = FILE_FORMAT.format(prefix=load_model_prefix, mtype=model_type, clf=clf_description, \n",
    "                                    dtype=DATA_TYPE, epochs=max_iter, ext=STATS_EXT)        \n",
    "\n",
    "    stats_path = os.path.join(STATS_MODELS_DIR, stats_file)\n",
    "    with open(stats_path, 'wt') as f:\n",
    "        print(\"Storing statistics in {}...\".format(stats_path))\n",
    "        \n",
    "        # save statistics\n",
    "        print('Confusion matrix:', file=f) \n",
    "        print(confusion_matrix, file=f )\n",
    "        print(file=f)\n",
    "        print(report, file=f)\n",
    "        print('SGD accuracy: {:.3f}'.format(acc), file=f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_model_prefix = query\n",
    "used_epochs = max_iter #  modify this parameter if extend_training was used\n",
    "store_model_path = os.path.join(SVM_MODELS_DIR, clf_name_format.format(prefix=store_model_prefix, mtype=model_type,\n",
    "                                                                       clf=clf_description, dtype=DATA_TYPE, epochs=used_epochs))\n",
    "save_model = True\n",
    "if save_model:\n",
    "    if not (os.path.exists(SVM_MODELS_DIR) and os.path.isdir(SVM_MODELS_DIR)):\n",
    "        os.mkdir(SVM_MODELS_DIR)\n",
    "        \n",
    "    with open(store_model_path, 'wb') as f:\n",
    "        pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LinearSVC C=1.0 from scratch...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.19      0.41      0.26       412\n",
      "          1       0.63      0.45      0.53      2456\n",
      "          2       0.15      0.33      0.20       330\n",
      "          3       0.18      0.26      0.21       213\n",
      "          4       0.72      0.63      0.67      2360\n",
      "\n",
      "avg / total       0.59      0.51      0.54      5771\n",
      "\n",
      "LinearSVC accuracy: 0.509\n"
     ]
    }
   ],
   "source": [
    "# train other classifiers\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "train_rf = False\n",
    "if train_rf:\n",
    "    clf_rf = RandomForestClassifier(n_estimators=50, n_jobs=-1, class_weight='balanced')\n",
    "    print(\"Training Random Forest  from scratch...\")\n",
    "    clf_rf.fit(X_train, y_train)\n",
    "\n",
    "    y_predicted = clf_rf.predict(X_test)\n",
    "\n",
    "    print(sklearn.metrics.classification_report(y_test, y_predicted))\n",
    "    print('RF accuracy: {:.3f}'.format(sklearn.metrics.accuracy_score(y_test, y_predicted)))\n",
    "\n",
    "train_svm = False\n",
    "if train_svm:\n",
    "    C = 1.0\n",
    "    clf_svm = LinearSVC(C=C, class_weight='balanced')\n",
    "    \n",
    "    print(\"Training LinearSVC C={} from scratch...\".format(C))\n",
    "    clf_svm.fit(X_train, y_train)\n",
    "\n",
    "    y_predicted = clf_svm.predict(X_test)\n",
    "\n",
    "    print(sklearn.metrics.classification_report(y_test, y_predicted))\n",
    "    print('LinearSVC accuracy: {:.3f}'.format(sklearn.metrics.accuracy_score(y_test, y_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "description": "Instant recognition with a pre-trained model and a tour of the net interface for visualizing features and parameters layer-by-layer.",
  "example_name": "Image Classification and Filter Visualization",
  "include_in_docs": true,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "priority": 1.0
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
