{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_notebook(fix_python_path=True, reduce_margins=True, plot_inline=True):\n",
    "    if reduce_margins:\n",
    "        # Reduce side margins of the notebook\n",
    "        from IPython.core.display import display, HTML\n",
    "        display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "    if fix_python_path:\n",
    "        # add egosocial to the python path\n",
    "        import os, sys\n",
    "        sys.path.extend([os.path.dirname(os.path.abspath('.'))])\n",
    "\n",
    "    if plot_inline:\n",
    "        # Plots inside cells\n",
    "        %matplotlib inline\n",
    "    \n",
    "    global __file__\n",
    "    __file__ = 'Notebook'\n",
    "\n",
    "setup_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Constants Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers.noise import AlphaDropout\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import egosocial.config\n",
    "from egosocial.core.attributes import AttributeSelector\n",
    "from egosocial.core.types import relation_to_domain, relation_to_domain_vec\n",
    "from egosocial.utils.keras.autolosses import AutoMultiLossWrapper\n",
    "from egosocial.utils.logging import setup_logging\n",
    "from egosocial.utils.keras.callbacks import PlotLearning\n",
    "from egosocial.utils.keras.backend import limit_gpu_allocation_tensorflow\n",
    "from egosocial.utils.io import load_features\n",
    "\n",
    "# constants\n",
    "DOMAIN, RELATION = 'domain', 'relation'\n",
    "END_TO_END, ATTRIBUTES = 'end_to_end', 'attributes'\n",
    "\n",
    "N_CLS_RELATION, N_CLS_DOMAIN = 16, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit GPU memory allocation with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_gpu_allocation_tensorflow(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unused functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(X, y, clf):\n",
    "    y_predicted = clf.predict(X)\n",
    "    acc = sklearn.metrics.accuracy_score(y, y_predicted)\n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(y, y_predicted)\n",
    "    report = sklearn.metrics.classification_report(y, y_predicted)\n",
    "\n",
    "    return acc, confusion_matrix, report\n",
    "\n",
    "\n",
    "def print_statistics(val_stats=None, test_stats=None, fdesc=sys.stdout):\n",
    "    for description, stats in [('Validation set:', val_stats),\n",
    "                               ('Test set:', test_stats)]:\n",
    "\n",
    "        if stats is not None:\n",
    "            print(description, file=fdesc)\n",
    "            accuracy, confusion_matrix, report = stats\n",
    "            print('Confusion matrix:', file=fdesc)\n",
    "            print(confusion_matrix, file=fdesc)\n",
    "            print(file=fdesc)\n",
    "            print(report, file=fdesc)\n",
    "            print('SGD accuracy: {:.3f}'.format(accuracy), file=fdesc)\n",
    "            print('------------------------------------------------',\n",
    "                  file=fdesc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input arguments and fake main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    def __init__(self, args):\n",
    "        self.DATA_TYPE = RELATION\n",
    "        self.ARCH = 'caffeNet'\n",
    "        self.LAYER = 'fc7'\n",
    "\n",
    "        self.CONFIG = '{}_{}_{}'.format(self.LAYER, self.DATA_TYPE, self.ARCH)\n",
    "\n",
    "        # setup directories\n",
    "        self.PROJECT_DIR = args.project_dir\n",
    "        self.BASE_MODELS_DIR = os.path.join(self.PROJECT_DIR,\n",
    "                                            'models/trained_models')\n",
    "        self.ATTR_MODELS_DIR = os.path.join(self.BASE_MODELS_DIR,\n",
    "                                            'attribute_models')\n",
    "        self.SVM_MODELS_DIR = os.path.join(self.PROJECT_DIR,\n",
    "                                           'models/svm_models')\n",
    "\n",
    "        self.SPLITS_DIR = os.path.join(self.PROJECT_DIR,\n",
    "                                       'datasets/splits/annotator_consistency3')\n",
    "\n",
    "        self.STATS_MODELS_DIR = os.path.join(self.SVM_MODELS_DIR, 'stats')\n",
    "\n",
    "        LABEL_FILE_FMT = 'single_body1_{}_16.txt'\n",
    "        self.LABEL_FILES = {split: os.path.join(self.SPLITS_DIR,\n",
    "                                                LABEL_FILE_FMT.format(split))\n",
    "                            for split in ('train', 'test', 'eval')}\n",
    "\n",
    "        self.IS_END2END = False\n",
    "\n",
    "        self.BASE_FEATURES_DIR = os.path.join(self.PROJECT_DIR,\n",
    "                                              'extracted_features')\n",
    "        self.FEATURES_DIR = os.path.join(self.BASE_FEATURES_DIR,\n",
    "                                         'attribute_features',\n",
    "                                         self.CONFIG)\n",
    "\n",
    "        self.STORED_FEATURES_DIR = os.path.join(self.FEATURES_DIR,\n",
    "                                                'all_splits_numpy_format')\n",
    "\n",
    "        self.PROCESS_FEATURES = args.port_features\n",
    "\n",
    "        self.EPOCHS = args.epochs\n",
    "        self.BATCH_SIZE = args.batch_size\n",
    "\n",
    "        # reuse precomputed model?\n",
    "        self.REUSE_MODEL = args.reuse_model\n",
    "        # save model to disk?\n",
    "        self.SAVE_MODEL = args.save_model\n",
    "        # save model statistics to disk?\n",
    "        self.SAVE_STATS = args.save_stats\n",
    "        \n",
    "def positive_int(value):\n",
    "    ivalue = int(value)\n",
    "    if ivalue <= 0:\n",
    "        raise argparse.ArgumentTypeError(\n",
    "            \"%s is an invalid positive int value\" % value)\n",
    "    return ivalue\n",
    "\n",
    "def main(*fake_args):\n",
    "    setup_logging(egosocial.config.LOGGING_CONFIG)\n",
    "\n",
    "    entry_msg = 'Reproduce experiments in Social Relation Recognition paper.'\n",
    "    parser = argparse.ArgumentParser(description=entry_msg)\n",
    "\n",
    "    parser.add_argument('--project_dir', required=True,\n",
    "                        help='Base directory.')\n",
    "\n",
    "    parser.add_argument('--port_features', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Whether port features from other formats to'\n",
    "                             'numpy.')\n",
    "\n",
    "    parser.add_argument('--reuse_model', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Use precomputed model if available.')\n",
    "\n",
    "    parser.add_argument('--save_model', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Save model to disk.')\n",
    "\n",
    "    parser.add_argument('--save_stats', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Save statistics to disk.')\n",
    "\n",
    "    parser.add_argument('--epochs', required=False, type=positive_int,\n",
    "                        default=100,\n",
    "                        help='Max number of epochs.')\n",
    "\n",
    "    parser.add_argument('--batch_size', required=False, type=positive_int,\n",
    "                        default=32,\n",
    "                        help='Batch size.')\n",
    "\n",
    "    # TODO: implement correctly\n",
    "    args = parser.parse_args(*fake_args)\n",
    "    # keep configuration\n",
    "    conf = Configuration(args)\n",
    "\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_to_relation_map():\n",
    "    W = [np.zeros(N_CLS_RELATION) for _ in range(N_CLS_DOMAIN)]\n",
    "    for rel in range(N_CLS_RELATION):\n",
    "        dom = relation_to_domain(rel)\n",
    "        W[dom] += to_categorical(rel, N_CLS_RELATION)\n",
    "    return np.array(W).T\n",
    "\n",
    "def prepare_data_split_for_keras(data_split):\n",
    "    x_train, x_val, x_test, *labels = data_split\n",
    "    # one-hot encoding for relation\n",
    "    y_train_rel, y_val_rel, y_test_rel = [\n",
    "        to_categorical(y, N_CLS_RELATION) for y in labels\n",
    "    ]\n",
    "    # one-hot encoding for domain\n",
    "    y_train_dom, y_val_dom, y_test_dom = [\n",
    "        to_categorical(relation_to_domain_vec(y), N_CLS_DOMAIN) for y in labels\n",
    "    ]\n",
    "\n",
    "    x_train_inputs = {'attribute_features': x_train}\n",
    "    y_train_outputs = {'relation': y_train_rel, 'domain': y_train_dom}\n",
    "    x_val_inputs = {'attribute_features': x_val}\n",
    "    y_val_outputs = {'relation': y_val_rel, 'domain': y_val_dom}\n",
    "    x_test_inputs = {'attribute_features': x_test}\n",
    "    y_test_outputs = {'relation': y_test_rel, 'domain': y_test_dom}\n",
    "    \n",
    "    result = dict(train=(x_train_inputs, y_train_outputs),\n",
    "                  val=(x_val_inputs, y_val_outputs),\n",
    "                  test=(x_test_inputs, y_test_outputs))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def compute_class_weight_relation_domain(y_train_rel):\n",
    "    y_train = dict(relation=y_train_rel, domain=relation_to_domain_vec(y_train_rel))\n",
    "\n",
    "    class_weight = {}\n",
    "    for y_type, y_data in y_train.items():\n",
    "        classes = sorted(np.unique(y_data))\n",
    "        weights = compute_class_weight('balanced', classes, y_data)\n",
    "        class_weight[y_type] = dict(zip(classes, weights))\n",
    "\n",
    "    return class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_top_down(n_features):\n",
    "    input_features = Input(shape=[n_features], \n",
    "                           name='attribute_features',\n",
    "                           dtype='float')\n",
    "    x = input_features\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Dense(128, name='dense_1',\n",
    "                   activation='elu',\n",
    "                   bias_initializer='lecun_normal',\n",
    "                   kernel_initializer='lecun_normal',\n",
    "                   bias_regularizer=l2(0.1),\n",
    "                   kernel_regularizer=l2(0.1),              \n",
    "             )(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = AlphaDropout(0.25)(x)\n",
    "\n",
    "    domain = Dense(N_CLS_DOMAIN, name='domain',\n",
    "                   activation='softmax',\n",
    "                   bias_regularizer=l2(0.1),\n",
    "                   kernel_regularizer=l2(0.1),\n",
    "                  )(x)\n",
    "\n",
    "    x = keras.layers.concatenate([x, domain])\n",
    "    \n",
    "    relation = Dense(N_CLS_RELATION, name='relation',\n",
    "                     activation='softmax',\n",
    "                     bias_regularizer=l2(0.1),\n",
    "                     kernel_regularizer=l2(0.1),\n",
    "                    )(x)\n",
    "\n",
    "    model = Model(inputs=[input_features], outputs=[domain, relation])\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_fix_domain(n_features):\n",
    "\n",
    "    input_features = Input(shape=[n_features], \n",
    "                           name='attribute_features',\n",
    "                           dtype='float',\n",
    "                           )\n",
    "\n",
    "    x = input_features\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Dense(128, name='dense_1',\n",
    "                   activation='selu',\n",
    "                   bias_initializer='lecun_normal',\n",
    "                   kernel_initializer='lecun_normal',\n",
    "                   bias_regularizer=l2(0.01),\n",
    "                   kernel_regularizer=l2(0.01), \n",
    "             )(x)\n",
    "    \n",
    "    relation = Dense(N_CLS_RELATION, name='relation',\n",
    "                     activation='softmax',\n",
    "                     bias_regularizer=l2(0.01),\n",
    "                     kernel_regularizer=l2(0.01),\n",
    "                    )(x)    \n",
    "    \n",
    "    domain = Dense(N_CLS_DOMAIN, name='domain',\n",
    "                   activation='linear',\n",
    "                   use_bias=False, trainable=False,\n",
    "                   weights=[domain_to_relation_map()],\n",
    "                  )(relation)\n",
    "    \n",
    "    model = Model(inputs=[input_features], outputs=[domain, relation])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialClassifierWithPreComputedFeatures:\n",
    "    \n",
    "    def __init__(self, features_dir, label_files):\n",
    "        self._features_dir = features_dir\n",
    "        self._label_files = label_files\n",
    "\n",
    "        self._log = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "        # parameters\n",
    "        # enables dimentionality reduction\n",
    "        self._dim_reduction = True\n",
    "        # force recomputing PCA every time\n",
    "        self._refit_pca = False\n",
    "\n",
    "        # if any of these parameters change, PCA should be recomputed\n",
    "        # features quantization (smaller Q promotes sparsity)\n",
    "        self._quantization = False\n",
    "        self._Q = 32\n",
    "        # parameters for PCA search\n",
    "        self._min_dim = 50 # min number of components\n",
    "        self._max_dim = 200 # max number of components\n",
    "        self._min_expl_var = 0.8 # min desired explained variance\n",
    "        self._max_pca_retries = 3 # max number of retries\n",
    "        # if parameter is set in (0,1] \n",
    "        # full PCA is fitted  and keep number of components for the \n",
    "        # required expl. var; otherwise it performs search\n",
    "        self._pca_conf = self._min_dim\n",
    "\n",
    "        # cache PCA instances\n",
    "        self._precomputed_pca = {}\n",
    "        # keep features\n",
    "        self._attribute_features = None\n",
    "        # keep labels\n",
    "        self._labels = None\n",
    "        # initialize when data split is configured\n",
    "        self._n_features = None\n",
    "        # initialize when model is configured\n",
    "        self._model_wrapper = None\n",
    "        self.model = None\n",
    "\n",
    "    def load_data(self):\n",
    "        self._attribute_features = load_features(self._features_dir, self._parse_filename)\n",
    "        self._labels = self._load_labels(self._label_files)\n",
    "\n",
    "        attributes = self.list_attributes() # needs _attribute_features already set\n",
    "        self._log.info('Found {} attributes. List: '.format(len(attributes)))\n",
    "        for attr in attributes:\n",
    "            self._log.info('{}'.format(attr))\n",
    "\n",
    "        # reset internal fields\n",
    "        self._n_features = None\n",
    "        self.model = None\n",
    "        # pca gets reset only if refit_pca is enabled \n",
    "        if self._refit_pca:\n",
    "            self._precomputed_pca = {}\n",
    "\n",
    "    def list_attributes(self):\n",
    "        # list attributes\n",
    "        if self._attribute_features:\n",
    "            return sorted(self._attribute_features['train'].keys())\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def get_data_split(self, selected_attributes, preprocess=True):\n",
    "        # get data splits composed by selected attributes only\n",
    "        # preprocess the data\n",
    "\n",
    "        # splits (switch from caffe's split name convention to keras's convention)\n",
    "        _train, _val, _test = 'train', 'test', 'eval'\n",
    "        attribute_features = self._attribute_features\n",
    "        labels = self._labels\n",
    "\n",
    "        # assert attributes\n",
    "        assert attribute_features\n",
    "        assert labels        \n",
    "\n",
    "        if preprocess:\n",
    "            features = defaultdict(dict)\n",
    "            # preprocess each selected attribute individually\n",
    "            for attr in selected_attributes:\n",
    "                data = self._preprocess_data(attribute_features[_train][attr]\n",
    "                                            , attribute_features[_val][attr]\n",
    "                                            , attribute_features[_test][attr]\n",
    "                                            , data_id=attr)\n",
    "                for split_idx, split in enumerate((_train, _val, _test)):\n",
    "                    features[split][attr] = data[split_idx]\n",
    "        else:\n",
    "            features = attribute_features\n",
    "\n",
    "        # concatenate attributes\n",
    "        fused_features = {}\n",
    "        for split in (_train, _val, _test):\n",
    "            selected_features = [features[split][attr]\n",
    "                                 for attr in selected_attributes]\n",
    "\n",
    "            fused_features[split] = np.concatenate(selected_features, axis=1)\n",
    "\n",
    "        result = [fused_features[split] for split in (_train, _val, _test)]\n",
    "        result.extend([labels[split] for split in (_train, _val, _test)])\n",
    "\n",
    "        # init number of dimensions\n",
    "        self._n_features = fused_features[_train].shape[1]\n",
    "\n",
    "        return tuple(result)\n",
    "\n",
    "\n",
    "    def prepare_data(self, data_split):\n",
    "        return prepare_data_split_for_keras(data_split)\n",
    "\n",
    "    def init_model(self, model_type='top_down'):\n",
    "        assert model_type in ('top_down', 'fix_domain') \n",
    "        assert self._n_features is not None        \n",
    "        if model_type == 'top_down':\n",
    "            model = create_model_top_down(self._n_features)\n",
    "        else:\n",
    "            model = create_model_fix_domain(self._n_features)\n",
    "        self._log.info('Initializing {} model'.format(model_type))\n",
    "        # compile model with default values\n",
    "        # predifined metrics: domain and relations accuracies\n",
    "        self.set_custom_model(model, metrics=['accuracy'])\n",
    "\n",
    "    def set_custom_model(self, model, \n",
    "                         optimizer='adam', \n",
    "                         loss='categorical_crossentropy', \n",
    "                         loss_weights='auto',\n",
    "                         **kwargs):\n",
    "        assert self._n_features is not None\n",
    "        # check number of features\n",
    "        assert len(model.inputs[0].shape) == 2\n",
    "        assert self._n_features == model.inputs[0].shape[1]\n",
    "\n",
    "        # wrapper allows to train the loss weights\n",
    "        self._model_wrapper = AutoMultiLossWrapper(model)\n",
    "        self._model_wrapper.compile(optimizer=optimizer, loss=loss, \n",
    "                                    loss_weights=loss_weights, **kwargs)\n",
    "\n",
    "        self.model = self._model_wrapper.model\n",
    "        self._log.info(self.model.summary())\n",
    "\n",
    "    def fit(self, train_data, validation_data, **kwargs):\n",
    "        self._log.info(\"Training model from scratch...\")\n",
    "        # validation data becomes mandatory\n",
    "        return self.model.fit(*train_data,\n",
    "                              validation_data=validation_data,\n",
    "                              **kwargs)\n",
    "\n",
    "    def evaluate(self, test_data, **kwargs):\n",
    "        return self.model.evaluate(*test_data, **kwargs)\n",
    "\n",
    "    def _parse_filename(self, numpy_file):\n",
    "        # split the extension from the path and normalize it to lowercase.\n",
    "        filename, ext = os.path.splitext(numpy_file)\n",
    "        ext = ext.lower()\n",
    "\n",
    "        # extract attribute name and split information\n",
    "        attr_name, split = filename.rsplit('_', 1)\n",
    "        # some attributes are splitted in two files (one for each person)\n",
    "        # create a list unique attributes name\n",
    "        if attr_name.endswith('_1') or attr_name.endswith('_2'):\n",
    "            attr_name = attr_name[:-2]\n",
    "\n",
    "        return split, attr_name, ext\n",
    "\n",
    "    def _load_labels(self, label_files):\n",
    "        # splits (switch from caffe's split name convention to keras's convention)\n",
    "        _train, _val, _test = 'train', 'test', 'eval'\n",
    "\n",
    "        labels = {}\n",
    "        for split in (_train, _val, _test):\n",
    "            with open(label_files[split]) as label_file:\n",
    "                labels[split] = np.array([label.split()[1] for label in label_file],\n",
    "                                          dtype=np.int)\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def _preprocess_data(self, x_train, x_val, x_test, data_id='data'):\n",
    "        self._log.debug('Preprocessing {}.'.format(data_id))\n",
    "        data_split = [x_train, x_val, x_test]\n",
    "\n",
    "        n_features = x_train.shape[1]\n",
    "        \n",
    "        # some sort of data normalization is always required for pca\n",
    "        if self._quantization: # quantization requires data in range [0, 1] \n",
    "            scaler = Normalizer(norm='l2').fit(data_split[0])\n",
    "        else:\n",
    "            scaler = StandardScaler().fit(data_split[0])\n",
    "\n",
    "        self._log.debug('Applying data normalization to {}.'.format(data_id))\n",
    "        data_split = [scaler.transform(x) for x in data_split]\n",
    "\n",
    "        assert self._min_dim >= 1\n",
    "        \n",
    "        if n_features < self._min_dim:\n",
    "            self._log.debug(\"Skip Q-sparsity and dim reduction for {}.\" \\\n",
    "                            \"Min number of dims: {}. Found: {}\" \\\n",
    "                            .format(data_id, self._min_dim, n_features))\n",
    "            return tuple(data_split)        \n",
    "        \n",
    "        if self._quantization:\n",
    "            assert self._Q >= 1\n",
    "            # small Q promotes sparsity\n",
    "            self._log.debug('Applying Q-sparsity Q={} to {}'.format(self._Q, data_id))\n",
    "            data_split = [np.floor(self._Q * x) for x in data_split]\n",
    "\n",
    "        if not self._dim_reduction:\n",
    "            return tuple(data_split)\n",
    "            \n",
    "        if data_id in self._precomputed_pca and not self._refit_pca:\n",
    "            # use precomputed model\n",
    "            self._log.debug('Using precomputed PCA for {}'.format(data_id))            \n",
    "            pca = self._precomputed_pca[data_id]\n",
    "        else:\n",
    "            assert self._pca_conf > 0\n",
    "            # compute pca from scratch            \n",
    "            if 0 < self._pca_conf <= 1:\n",
    "                # running pca with min explained variance takes much longer\n",
    "                self._log.debug('Fitting full PCA for {}'.format(data_id))\n",
    "                pca = PCA(self._pca_conf)\n",
    "                pca.fit(data_split[0])\n",
    "            else:\n",
    "                assert self._max_dim >= 1\n",
    "                assert self._min_expl_var > 0\n",
    "\n",
    "                # search starts in the given number of components\n",
    "                n_components = self._pca_conf\n",
    "                # search min number of components with required expl. var\n",
    "                for retry in range(self._max_pca_retries): # max number of retries\n",
    "                    self._log.debug('Fitting fast PCA retry {} for {}'.format(retry+1, data_id))\n",
    "                    # running pca with number of components is much faster\n",
    "                    pca = PCA(n_components, svd_solver='randomized')\n",
    "                    pca.fit(data_split[0])\n",
    "\n",
    "                    expl_var = np.sum(pca.explained_variance_ratio_)\n",
    "                    # sometimes pca fails to compute the expl. var (is set to NaN)\n",
    "                    if (not np.isnan(expl_var) and expl_var < self._min_expl_var and n_components < self._max_dim):\n",
    "                        n_components *= 2 # exponential search\n",
    "                    else:\n",
    "                        # if pca fails or the min expl. var is achieved or max retries\n",
    "                        break # stop trying\n",
    "            # store pca coefficients for future use\n",
    "            self._log.debug('Storing PCA fit for {}'.format(data_id))\n",
    "            self._precomputed_pca[data_id] = pca\n",
    "\n",
    "        explained_var = np.sum(pca.explained_variance_ratio_)\n",
    "        n_components = pca.n_components_\n",
    "        msg = 'Applying PCA with explained var {} dims {} to {}'\n",
    "        self._log.debug(msg.format(explained_var, n_components, data_id))\n",
    "        # pca transformationl\n",
    "        data_split = [pca.transform(x) for x in data_split]\n",
    "\n",
    "        return tuple(data_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake call to main to process inputs arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\n",
    "    \"--project_dir\", \"/home/shared/Documents/final_proj\",\n",
    "    \"--epochs\", \"30\",\n",
    "    \"--batch_size\", \"256\",\n",
    "]\n",
    "\n",
    "conf = main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading precomputed features and labels (may take some time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper = SocialClassifierWithPreComputedFeatures(conf.STORED_FEATURES_DIR, \n",
    "                                                 conf.LABEL_FILES)\n",
    "\n",
    "# load features and labels\n",
    "helper.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "helper.list_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure dimensionality reduction\n",
    "helper._min_dim = 25\n",
    "helper._max_dim = 200\n",
    "helper._max_pca_retries = 3\n",
    "helper._pca_conf = 200\n",
    "helper._min_expl_var = 0.95\n",
    "helper._quantization = False\n",
    "helper._dim_reduction = True\n",
    "helper._Q = 32\n",
    "helper._refit_pca = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select attributes (default all), prepare splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_selector = AttributeSelector(helper.list_attributes())\n",
    "\n",
    "# all / face / body / or single attribute (accept name substring, e.g. activity)\n",
    "attributes_query = 'all'\n",
    "# expand all / face / body / single attribute\n",
    "selected_attributes = attribute_selector.filter(attributes_query)\n",
    "helper._log.info('Selected attribute(s): {}'.format(attributes_query))\n",
    "\n",
    "# prepare splits for selected attributes\n",
    "data_split = helper.get_data_split(selected_attributes, preprocess=True)\n",
    "\n",
    "# prepate data for keras (multiple outputs for domain/relation and one-hot encoding)\n",
    "keras_data_split = helper.prepare_data(data_split)\n",
    "\n",
    "# class_weight for keras (balance domain/relation instances)\n",
    "class_weight = compute_class_weight_relation_domain(data_split[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# after preparing data (needs input dimensions)\n",
    "\n",
    "# helper.init_model('top_down')\n",
    "\n",
    "# allows more flexibility\n",
    "helper.set_custom_model(\n",
    "    create_model_top_down(helper._n_features),\n",
    "    optimizer=keras.optimizers.Adam(0.0001, decay=1e-6),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = conf.BATCH_SIZE\n",
    "epochs = conf.EPOCHS\n",
    "\n",
    "# FIXME: set directory correctly\n",
    "checkpoint_path = os.path.join(egosocial.config.MODELS_CACHE_DIR, 'multi_attribute',\n",
    "                               'weights.{epoch:02d}-{val_loss:.2f}.h5')\n",
    "callbacks = [\n",
    "#            ModelCheckpoint(\n",
    "#                filepath=checkpoint_path, \n",
    "#                monitor='val_loss', save_best_only=True\n",
    "#            ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.1, patience=10, min_lr=0.00001\n",
    "    ),\n",
    "    PlotLearning(update_step=1),\n",
    "]\n",
    "\n",
    "\n",
    "hist = helper.fit(\n",
    "    keras_data_split['train'], \n",
    "    keras_data_split['val'],\n",
    "    batch_size=batch_size, epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "#    class_weight=class_weight,\n",
    "    verbose=0, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = helper.evaluate(\n",
    "    keras_data_split['test'],\n",
    "    batch_size=batch_size\n",
    ")\n",
    "for score, metric_name in zip(scores, helper.model.metrics_names):\n",
    "    helper._log.info(\"%s : %0.4f\" % (metric_name, score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
