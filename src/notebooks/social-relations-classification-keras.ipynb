{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Constants Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# render matplot figures inside jupyter notebooks\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "# add egosocial to the python path\n",
    "from os.path import dirname, abspath\n",
    "sys.path.extend([dirname(abspath('.'))])\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers.noise import AlphaDropout\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import egosocial.config\n",
    "from egosocial.core.types import relation_to_domain, relation_to_domain_vec\n",
    "from egosocial.utils.caffe.misc import load_levelDB_as_array\n",
    "from egosocial.utils.keras.autolosses import AutoMultiLossWrapper\n",
    "from egosocial.utils.logging import setup_logging\n",
    "\n",
    "# constants\n",
    "DOMAIN, RELATION = 'domain', 'relation'\n",
    "END_TO_END, ATTRIBUTES = 'end_to_end', 'attributes'\n",
    "\n",
    "N_CLS_RELATION, N_CLS_DOMAIN = 16, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit GPU memory allocation with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "# allocates 25% of total memory\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.25  \n",
    "tf_session = tf.Session(config=config)\n",
    "K.set_session(tf_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unused functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(conf):\n",
    "    # preprocess features from original formats (leveldb, numpy, matlab)\n",
    "    if conf.PROCESS_FEATURES:\n",
    "        if conf.IS_END2END:\n",
    "            # LEVELDB DIRS\n",
    "            levelDB_dirs = [conf.FEATURES_DIR]\n",
    "            attribute_features = preprocess_attributes(levelDB_dirs)\n",
    "        else:\n",
    "            # LEVELDB DIRS\n",
    "            levelDB_dirs = [conf.FEATURES_DIR]\n",
    "            # MATLAB DIRS\n",
    "            matlab_dirs = [\n",
    "                os.path.join(conf.ATTR_MODELS_DIR,\n",
    "                             'localation_scale_data(annotator_consistency3)')\n",
    "            ]\n",
    "            # NUMPY DIRS\n",
    "            numpy_dirs = [\n",
    "                os.path.join(conf.ATTR_MODELS_DIR,\n",
    "                             'imsitu_body_activity(annotator_consistency3)'),\n",
    "                os.path.join(conf.ATTR_MODELS_DIR,\n",
    "                             'body_immediacy(annotator_consistency3)')\n",
    "            ]\n",
    "\n",
    "            attribute_features = preprocess_attributes(levelDB_dirs, numpy_dirs,\n",
    "                                                       matlab_dirs)\n",
    "\n",
    "        if not (os.path.isdir(conf.STORED_FEATURES_DIR)):\n",
    "            os.mkdir(conf.STORED_FEATURES_DIR)\n",
    "\n",
    "        # save features to disk\n",
    "        save_features(attribute_features, conf.STORED_FEATURES_DIR,\n",
    "                      compressed=True)\n",
    "\n",
    "    else:\n",
    "        # load features from disk\n",
    "        attribute_features = load_features(conf.STORED_FEATURES_DIR)\n",
    "\n",
    "    return attribute_features\n",
    "\n",
    "\n",
    "def compute_stats(X, y, clf):\n",
    "    y_predicted = clf.predict(X)\n",
    "    acc = sklearn.metrics.accuracy_score(y, y_predicted)\n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(y, y_predicted)\n",
    "    report = sklearn.metrics.classification_report(y, y_predicted)\n",
    "\n",
    "    return acc, confusion_matrix, report\n",
    "\n",
    "\n",
    "def print_statistics(val_stats=None, test_stats=None, fdesc=sys.stdout):\n",
    "    for description, stats in [('Validation set:', val_stats),\n",
    "                               ('Test set:', test_stats)]:\n",
    "\n",
    "        if stats is not None:\n",
    "            print(description, file=fdesc)\n",
    "            accuracy, confusion_matrix, report = stats\n",
    "            print('Confusion matrix:', file=fdesc)\n",
    "            print(confusion_matrix, file=fdesc)\n",
    "            print(file=fdesc)\n",
    "            print(report, file=fdesc)\n",
    "            print('SGD accuracy: {:.3f}'.format(accuracy), file=fdesc)\n",
    "            print('------------------------------------------------',\n",
    "                  file=fdesc)\n",
    "\n",
    "\n",
    "def preprocess_attributes(levelDB_dirs=None, raw_numpy_dirs=None,\n",
    "                          matlab_dirs=None):\n",
    "    splits = ['train', 'test', 'eval']\n",
    "    attribute_features = {split: {} for split in splits}\n",
    "\n",
    "    ###########################################################################\n",
    "    # features in levelDB format\n",
    "    if levelDB_dirs:\n",
    "        for directory in levelDB_dirs:\n",
    "            for split in splits:\n",
    "                attribute_models = os.listdir(os.path.join(directory, split))\n",
    "\n",
    "                for attr_name in attribute_models:\n",
    "                    features = load_levelDB_as_array(\n",
    "                        os.path.join(directory, split, attr_name))\n",
    "                    attribute_features[split][attr_name] = features\n",
    "                    msg = \"Convert from levelDB dataset: {} attribute: {} \" \\\n",
    "                          \"dim: {}\"\n",
    "                    print(msg.format(split, attr_name, features.shape))\n",
    "\n",
    "    ###########################################################################\n",
    "    # features in numpy format\n",
    "    if raw_numpy_dirs:\n",
    "        for directory in raw_numpy_dirs:\n",
    "            for numpy_file in os.listdir(directory):\n",
    "                filename, ext = os.path.splitext(numpy_file)\n",
    "                if ext.lower() == '.npy':\n",
    "                    # load numpy\n",
    "                    features = np.load(os.path.join(directory, numpy_file))\n",
    "                    # find split\n",
    "                    for candidate_split in splits:\n",
    "                        if candidate_split in filename:\n",
    "                            split = candidate_split\n",
    "                            break\n",
    "                    else:\n",
    "                        split = None\n",
    "                    # the folder name is the attribute name\n",
    "                    attr_name = os.path.basename(directory)\n",
    "\n",
    "                    attribute_features[split][attr_name] = features\n",
    "                    msg = \"Load numpy format dataset: {} attribute: {} dim: {}\"\n",
    "                    print(msg.format(split, attr_name, features.shape))\n",
    "\n",
    "    ###########################################################################\n",
    "    # features in matlab format\n",
    "    if matlab_dirs:\n",
    "        for directory in matlab_dirs:\n",
    "            for matfile in os.listdir(directory):\n",
    "                filename, ext = os.path.splitext(matfile)\n",
    "                if ext.lower() == '.mat':\n",
    "                    # load matfile (dict format)\n",
    "                    matfile_dict = scipy.io.loadmat(\n",
    "                        os.path.join(directory, matfile))\n",
    "                    attr_name, split = filename.rsplit('_', 1)\n",
    "                    # access numpy field\n",
    "                    features = matfile_dict[attr_name]\n",
    "                    attribute_features[split][attr_name] = features\n",
    "                    msg = \"Convert matlab format dataset: {} attribute: {} \" \\\n",
    "                          \"dim: {}\"\n",
    "                    print(msg.format(split, attr_name, features.shape))\n",
    "\n",
    "    ###########################################################################\n",
    "\n",
    "    return attribute_features\n",
    "\n",
    "\n",
    "def save_features(attribute_features, features_dir, compressed=True):\n",
    "    if not (os.path.exists(features_dir) and os.path.isdir(features_dir)):\n",
    "        os.mkdir(features_dir)\n",
    "\n",
    "    for split, attributes in attribute_features.items():\n",
    "        for attr_name, features in attributes.items():\n",
    "            features_path = os.path.join(features_dir, '{}_{}').format(\n",
    "                attr_name, split)\n",
    "\n",
    "            # save file in compress format and float16\n",
    "            if compressed:\n",
    "                np.savez_compressed(features_path, features.astype(np.float16))\n",
    "            else:\n",
    "                np.save(features_path, features)\n",
    "\n",
    "            print(\"Saved {}.{} ...\".format(features_path,\n",
    "                                           'npz' if compressed else 'np'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input arguments and fake main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    def __init__(self, args):\n",
    "        self.DATA_TYPE = RELATION\n",
    "        self.ARCH = 'caffeNet'\n",
    "        self.LAYER = 'fc7'\n",
    "\n",
    "        self.CONFIG = '{}_{}_{}'.format(self.LAYER, self.DATA_TYPE, self.ARCH)\n",
    "\n",
    "        # setup directories\n",
    "        self.PROJECT_DIR = args.project_dir\n",
    "        self.BASE_MODELS_DIR = os.path.join(self.PROJECT_DIR,\n",
    "                                            'models/trained_models')\n",
    "        self.ATTR_MODELS_DIR = os.path.join(self.BASE_MODELS_DIR,\n",
    "                                            'attribute_models')\n",
    "        self.SVM_MODELS_DIR = os.path.join(self.PROJECT_DIR,\n",
    "                                           'models/svm_models')\n",
    "\n",
    "        self.SPLITS_DIR = os.path.join(self.PROJECT_DIR,\n",
    "                                       'datasets/splits/annotator_consistency3')\n",
    "\n",
    "        self.STATS_MODELS_DIR = os.path.join(self.SVM_MODELS_DIR, 'stats')\n",
    "\n",
    "        LABEL_FILE_FMT = 'single_body1_{}_16.txt'\n",
    "        self.LABEL_FILES = {split: os.path.join(self.SPLITS_DIR,\n",
    "                                                LABEL_FILE_FMT.format(split))\n",
    "                            for split in ('train', 'test', 'eval')}\n",
    "\n",
    "        self.IS_END2END = False\n",
    "\n",
    "        self.BASE_FEATURES_DIR = os.path.join(self.PROJECT_DIR,\n",
    "                                              'extracted_features')\n",
    "        self.FEATURES_DIR = os.path.join(self.BASE_FEATURES_DIR,\n",
    "                                         'attribute_features',\n",
    "                                         self.CONFIG)\n",
    "\n",
    "        self.STORED_FEATURES_DIR = os.path.join(self.FEATURES_DIR,\n",
    "                                                'all_splits_numpy_format')\n",
    "\n",
    "        self.PROCESS_FEATURES = args.port_features\n",
    "\n",
    "        self.EPOCHS = args.epochs\n",
    "        self.BATCH_SIZE = args.batch_size\n",
    "\n",
    "        # reuse precomputed model?\n",
    "        self.REUSE_MODEL = args.reuse_model\n",
    "        # save model to disk?\n",
    "        self.SAVE_MODEL = args.save_model\n",
    "        # save model statistics to disk?\n",
    "        self.SAVE_STATS = args.save_stats\n",
    "        \n",
    "def positive_int(value):\n",
    "    ivalue = int(value)\n",
    "    if ivalue <= 0:\n",
    "        raise argparse.ArgumentTypeError(\n",
    "            \"%s is an invalid positive int value\" % value)\n",
    "    return ivalue\n",
    "\n",
    "def main(*fake_args):\n",
    "    setup_logging(egosocial.config.LOGGING_CONFIG)\n",
    "\n",
    "    entry_msg = 'Reproduce experiments in Social Relation Recognition paper.'\n",
    "    parser = argparse.ArgumentParser(description=entry_msg)\n",
    "\n",
    "    parser.add_argument('--project_dir', required=True,\n",
    "                        help='Base directory.')\n",
    "\n",
    "    parser.add_argument('--port_features', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Whether port features from other formats to'\n",
    "                             'numpy.')\n",
    "\n",
    "    parser.add_argument('--reuse_model', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Use precomputed model if available.')\n",
    "\n",
    "    parser.add_argument('--save_model', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Save model to disk.')\n",
    "\n",
    "    parser.add_argument('--save_stats', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Save statistics to disk.')\n",
    "\n",
    "    parser.add_argument('--epochs', required=False, type=positive_int,\n",
    "                        default=100,\n",
    "                        help='Max number of epochs.')\n",
    "\n",
    "    parser.add_argument('--batch_size', required=False, type=positive_int,\n",
    "                        default=32,\n",
    "                        help='Batch size.')\n",
    "\n",
    "    # TODO: implement correctly\n",
    "    args = parser.parse_args(*fake_args)\n",
    "    # keep configuration\n",
    "    conf = Configuration(args)\n",
    "\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributeSelector:\n",
    "    def __init__(self, all_attrs):\n",
    "        body_attributes = self.filter_by_keyword(all_attrs, 'body')\n",
    "        face_attributes = self.filter_by_keyword(all_attrs, 'face')\n",
    "        face_attributes.extend(self.filter_by_keyword(all_attrs, 'head'))\n",
    "\n",
    "        self._selector = {'all': all_attrs,\n",
    "                          'body': body_attributes,\n",
    "                          'face': face_attributes}\n",
    "\n",
    "    def filter(self, query):\n",
    "        if query in self._selector:\n",
    "            selected_attributes = self._selector[query]\n",
    "        else:\n",
    "            selected_attributes = self.filter_by_keyword(self._selector['all'],\n",
    "                                                         query)\n",
    "\n",
    "        return selected_attributes\n",
    "\n",
    "    def filter_by_keyword(self, attribute_list, key):\n",
    "        return [attr_name for attr_name in attribute_list if key in attr_name]\n",
    "\n",
    "def domain_to_relation_map():\n",
    "    W = [np.zeros(N_CLS_RELATION) for _ in range(N_CLS_DOMAIN)]\n",
    "    for rel in range(N_CLS_RELATION):\n",
    "        dom = relation_to_domain(rel)\n",
    "        W[dom] += to_categorical(rel, N_CLS_RELATION)\n",
    "    return np.array(W).T\n",
    "\n",
    "def prepare_data_split_for_keras(data_split):\n",
    "    x_train, x_val, x_test, *labels = data_split\n",
    "    # one-hot encoding for relation\n",
    "    y_train_rel, y_val_rel, y_test_rel = [\n",
    "        to_categorical(y, N_CLS_RELATION) for y in labels\n",
    "    ]\n",
    "    # one-hot encoding for domain\n",
    "    y_train_dom, y_val_dom, y_test_dom = [\n",
    "        to_categorical(relation_to_domain_vec(y), N_CLS_DOMAIN) for y in labels\n",
    "    ]\n",
    "\n",
    "    x_train_inputs = {'attribute_features': x_train}\n",
    "    y_train_outputs = {'relation': y_train_rel, 'domain': y_train_dom}\n",
    "    x_val_inputs = {'attribute_features': x_val}\n",
    "    y_val_outputs = {'relation': y_val_rel, 'domain': y_val_dom}\n",
    "    x_test_inputs = {'attribute_features': x_test}\n",
    "    y_test_outputs = {'relation': y_test_rel, 'domain': y_test_dom}\n",
    "    \n",
    "    result = dict(train=(x_train_inputs, y_train_outputs),\n",
    "                  val=(x_val_inputs, y_val_outputs),\n",
    "                  test=(x_test_inputs, y_test_outputs))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def compute_class_weight_relation_domain(y_train_rel):\n",
    "    y_train = dict(relation=y_train_rel, domain=relation_to_domain_vec(y_train_rel))\n",
    "\n",
    "    class_weight = {}\n",
    "    for y_type, y_data in y_train.items():\n",
    "        classes = sorted(np.unique(y_data))\n",
    "        weights = compute_class_weight('balanced', classes, y_data)\n",
    "        class_weight[y_type] = dict(zip(classes, weights))\n",
    "\n",
    "    return class_weight\n",
    "\n",
    "class PlotLearning(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, figsize=(20, 13), update_step=1):\n",
    "        super(PlotLearning, self).__init__()\n",
    "        self.figsize = figsize\n",
    "        self.update_step = update_step\n",
    "        # sort legends, training first\n",
    "        self._legend_key = lambda name: 'val' if name.startswith('val_') else 'train'\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "    \n",
    "        self.metrics = defaultdict(list)\n",
    "        \n",
    "        self._metric_map = defaultdict()\n",
    "        self._metric_map.update({name:idx for idx, name in enumerate(self.model.metrics_names)})\n",
    "        self._metric_map.update({'val_{}'.format(name):idx for idx, name in enumerate(self.model.metrics_names)})\n",
    "\n",
    "        # add other logs if needed (get next available index)\n",
    "        next_idx = lambda : len(set(self._metric_map.values()))\n",
    "        self._metric_map.default_factory = next_idx\n",
    "        \n",
    "        plt.figure(figsize=self.figsize)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):        \n",
    "        # update internal state\n",
    "        # split logs in different lists\n",
    "        self.x.append(self.i)        \n",
    "        self.i += 1\n",
    "        \n",
    "        n_plots = 0 # number of unique subplots\n",
    "        for metric_name in logs.keys():\n",
    "            self.metrics[metric_name].append(logs[metric_name])\n",
    "            # gets the max index (creates new ones if needed)\n",
    "            n_plots = max(n_plots, self._metric_map[metric_name])\n",
    "        # indices are zero-based\n",
    "        n_plots += 1\n",
    "\n",
    "        # refresh screen every 'update_step' iterations\n",
    "        if (self.i-1) % self.update_step == 0:\n",
    "            self.plot(n_plots)\n",
    "    \n",
    "    def plot(self, n_plots=None):\n",
    "        n_plots = n_plots if n_plots else self._metric_map.default_factory()\n",
    "\n",
    "        # grid (keeps rectangular shape as compact as possible)\n",
    "        # if not a square shape, horizontal axis is slightly larger than vertical one\n",
    "        nrows = int(np.floor(np.sqrt(n_plots))) # vertical axis\n",
    "        ncols = int(np.ceil(1.0 * n_plots / nrows)) # horizontal axis\n",
    "        # figure containing multiple plots\n",
    "        f, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=self.figsize)\n",
    "        clear_output(wait=True)        \n",
    "        # squeeze array\n",
    "        ax.shape = (nrows * ncols,)\n",
    "        # plot in the same subfigure training and validation data for a given metric\n",
    "        for metric_name in sorted(self.metrics.keys(), key=self._legend_key):\n",
    "            idx = self._metric_map[metric_name]\n",
    "            ax[idx].plot(self.x, self.metrics[metric_name], label=metric_name)\n",
    "            ax[idx].legend()\n",
    "                \n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_top_down(n_features):\n",
    "    input_features = Input(shape=[n_features], \n",
    "                           name='attribute_features',\n",
    "                           dtype='float')\n",
    "    x = input_features\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Dense(128, name='dense_1',\n",
    "                   activation='elu',\n",
    "                   bias_initializer='lecun_normal',\n",
    "                   kernel_initializer='lecun_normal',\n",
    "                   bias_regularizer=l2(0.1),\n",
    "                   kernel_regularizer=l2(0.1),              \n",
    "             )(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = AlphaDropout(0.25)(x)\n",
    "\n",
    "    domain = Dense(N_CLS_DOMAIN, name='domain',\n",
    "                   activation='softmax',\n",
    "                   bias_regularizer=l2(0.1),\n",
    "                   kernel_regularizer=l2(0.1),\n",
    "                  )(x)\n",
    "\n",
    "    x = keras.layers.concatenate([x, domain])\n",
    "    \n",
    "    relation = Dense(N_CLS_RELATION, name='relation',\n",
    "                     activation='softmax',\n",
    "                     bias_regularizer=l2(0.1),\n",
    "                     kernel_regularizer=l2(0.1),\n",
    "                    )(x)\n",
    "\n",
    "    model = Model(inputs=[input_features], outputs=[domain, relation])\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_fix_domain(n_features):\n",
    "\n",
    "    input_features = Input(shape=[n_features], \n",
    "                           name='attribute_features',\n",
    "                           dtype='float',\n",
    "                           )\n",
    "\n",
    "    x = input_features\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Dense(128, name='dense_1',\n",
    "                   activation='selu',\n",
    "                   bias_initializer='lecun_normal',\n",
    "                   kernel_initializer='lecun_normal',\n",
    "                   bias_regularizer=l2(0.01),\n",
    "                   kernel_regularizer=l2(0.01), \n",
    "             )(x)\n",
    "    \n",
    "    relation = Dense(N_CLS_RELATION, name='relation',\n",
    "                     activation='softmax',\n",
    "                     bias_regularizer=l2(0.01),\n",
    "                     kernel_regularizer=l2(0.01),\n",
    "                    )(x)    \n",
    "    \n",
    "    domain = Dense(N_CLS_DOMAIN, name='domain',\n",
    "                   activation='linear',\n",
    "                   use_bias=False, trainable=False,\n",
    "                   weights=[domain_to_relation_map()],\n",
    "                  )(relation)\n",
    "    \n",
    "    model = Model(inputs=[input_features], outputs=[domain, relation])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialClassifierWithPreComputedFeatures:\n",
    "    \n",
    "    def __init__(self, features_dir, label_files):\n",
    "        self._features_dir = features_dir\n",
    "        self._label_files = label_files\n",
    "\n",
    "        __file__ = 'SocialClassifierWithPreComputedFeatures'\n",
    "        self._log = logging.getLogger(os.path.basename(__file__))\n",
    "\n",
    "        # parameters\n",
    "        # enables dimentionality reduction\n",
    "        self._dim_reduction = True\n",
    "        # force recomputing PCA every time\n",
    "        self._refit_pca = False\n",
    "\n",
    "        # if any of these parameters change, PCA should be recomputed\n",
    "        # features quantization (smaller Q promotes sparsity)\n",
    "        self._quantization = False\n",
    "        self._Q = 32\n",
    "        # parameters for PCA search\n",
    "        self._min_dim = 50 # min number of components\n",
    "        self._max_dim = 200 # max number of components\n",
    "        self._min_expl_var = 0.8 # min desired explained variance\n",
    "        self._max_pca_retries = 3 # max number of retries\n",
    "        # if parameter is set in (0,1] \n",
    "        # full PCA is fitted  and keep number of components for the \n",
    "        # required expl. var; otherwise it performs search\n",
    "        self._pca_conf = self._min_dim\n",
    "\n",
    "        # cache PCA instances\n",
    "        self._precomputed_pca = {}\n",
    "        # keep features\n",
    "        self._attribute_features = None\n",
    "        # keep labels\n",
    "        self._labels = None\n",
    "        # initialize when data split is configured\n",
    "        self._n_features = None\n",
    "        # initialize when model is configured\n",
    "        self._model_wrapper = None\n",
    "        self.model = None\n",
    "\n",
    "    def load_data(self):\n",
    "        self._attribute_features = self._load_features(self._features_dir)\n",
    "        self._labels = self._load_labels(self._label_files)\n",
    "\n",
    "        attributes = self.list_attributes() # needs _attribute_features already set\n",
    "        self._log.info('Found {} attributes. List: '.format(len(attributes)))\n",
    "        for attr in attributes:\n",
    "            self._log.info('{}'.format(attr))\n",
    "\n",
    "        # reset internal fields\n",
    "        self._n_features = None\n",
    "        self.model = None\n",
    "        # pca gets reset only if refit_pca is enabled \n",
    "        if self._refit_pca:\n",
    "            self._precomputed_pca = {}\n",
    "\n",
    "    def list_attributes(self):\n",
    "        # list attributes\n",
    "        if self._attribute_features:\n",
    "            return sorted(self._attribute_features['train'].keys())\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def get_data_split(self, selected_attributes, preprocess=True):\n",
    "        # get data splits composed by selected attributes only\n",
    "        # preprocess the data\n",
    "\n",
    "        # splits (switch from caffe's split name convention to keras's convention)\n",
    "        _train, _val, _test = 'train', 'test', 'eval'\n",
    "        attribute_features = self._attribute_features\n",
    "        labels = self._labels\n",
    "\n",
    "        # assert attributes\n",
    "        assert attribute_features\n",
    "        assert labels        \n",
    "\n",
    "        if preprocess:\n",
    "            features = defaultdict(dict)\n",
    "            # preprocess each selected attribute individually\n",
    "            for attr in selected_attributes:\n",
    "                data = self._preprocess_data(attribute_features[_train][attr]\n",
    "                                            , attribute_features[_val][attr]\n",
    "                                            , attribute_features[_test][attr]\n",
    "                                            , data_id=attr)\n",
    "                for split_idx, split in enumerate((_train, _val, _test)):\n",
    "                    features[split][attr] = data[split_idx]\n",
    "        else:\n",
    "            features = attribute_features\n",
    "\n",
    "        # concatenate attributes\n",
    "        fused_features = {}\n",
    "        for split in (_train, _val, _test):\n",
    "            selected_features = [features[split][attr]\n",
    "                                 for attr in selected_attributes]\n",
    "\n",
    "            fused_features[split] = np.concatenate(selected_features, axis=1)\n",
    "\n",
    "        result = [fused_features[split] for split in (_train, _val, _test)]\n",
    "        result.extend([labels[split] for split in (_train, _val, _test)])\n",
    "\n",
    "        # init number of dimensions\n",
    "        self._n_features = fused_features[_train].shape[1]\n",
    "\n",
    "        return tuple(result)\n",
    "\n",
    "\n",
    "    def prepare_data(self, data_split):\n",
    "        return prepare_data_split_for_keras(data_split)\n",
    "\n",
    "    def init_model(self, model_type='top_down'):\n",
    "        assert model_type in ('top_down', 'fix_domain') \n",
    "        assert self._n_features is not None        \n",
    "        if model_type == 'top_down':\n",
    "            model = create_model_top_down(self._n_features)\n",
    "        else:\n",
    "            model = create_model_fix_domain(self._n_features)\n",
    "        self._log.info('Initializing {} model'.format(model_type))\n",
    "        # compile model with default values\n",
    "        # predifined metrics: domain and relations accuracies\n",
    "        self.set_custom_model(model, metrics=['accuracy'])\n",
    "\n",
    "    def set_custom_model(self, model, \n",
    "                         optimizer='adam', \n",
    "                         loss='categorical_crossentropy', \n",
    "                         loss_weights='auto',\n",
    "                         **kwargs):\n",
    "        assert self._n_features is not None\n",
    "        # check number of features\n",
    "        assert len(model.inputs[0].shape) == 2\n",
    "        assert self._n_features == model.inputs[0].shape[1]\n",
    "\n",
    "        # wrapper allows to train the loss weights\n",
    "        self._model_wrapper = AutoMultiLossWrapper(model)\n",
    "        self._model_wrapper.compile(optimizer=optimizer, loss=loss, \n",
    "                                    loss_weights=loss_weights, **kwargs)\n",
    "\n",
    "        self.model = self._model_wrapper.model\n",
    "        self._log.info(self.model.summary())\n",
    "\n",
    "    def fit(self, train_data, validation_data, **kwargs):\n",
    "        self._log.info(\"Training model from scratch...\")\n",
    "        # validation data becomes mandatory\n",
    "        return self.model.fit(*train_data,\n",
    "                              validation_data=validation_data,\n",
    "                              **kwargs)\n",
    "\n",
    "    def evaluate(self, test_data, **kwargs):\n",
    "        return self.model.evaluate(*test_data, **kwargs)    \n",
    "\n",
    "    def _load_features(self, features_dir):\n",
    "        # splits (switch from caffe's split name convention to keras's convention)\n",
    "        _train, _val, _test = 'train', 'test', 'eval'\n",
    "\n",
    "        attribute_features = {split: {} for split in (_train, _val, _test)}\n",
    "\n",
    "        for numpy_file in sorted(os.listdir(features_dir)):\n",
    "            split, attr_name, ext = self._parse_filename(numpy_file)\n",
    "\n",
    "            # absolute path\n",
    "            features_path = os.path.join(features_dir, numpy_file)\n",
    "\n",
    "            if ext in ('.npz', '.npy'):\n",
    "                self._log.debug(\"Loading {}...\".format(features_path))\n",
    "            else:\n",
    "                self._log.warning(\"Found file with unknown format.\".format(features_path))\n",
    "                continue # skip this file\n",
    "\n",
    "            if ext == '.npz':\n",
    "                with np.load(features_path) as data:\n",
    "                    features = data['arr_0']\n",
    "            else:\n",
    "                features = np.load(features_path)\n",
    "\n",
    "            # fuse attributes in several files\n",
    "            if attr_name in attribute_features[split]:\n",
    "                array = np.concatenate([attribute_features[split][attr_name], \n",
    "                                        features.astype('float32', copy=False)],\n",
    "                                       axis=1)\n",
    "            else:\n",
    "                array = features.astype('float32', copy=False)\n",
    "\n",
    "            attribute_features[split][attr_name] = array\n",
    "\n",
    "        return attribute_features    \n",
    "\n",
    "    def _parse_filename(self, numpy_file):\n",
    "        # split the extension from the path and normalize it to lowercase.\n",
    "        filename, ext = os.path.splitext(numpy_file)\n",
    "        ext = ext.lower()\n",
    "\n",
    "        # extract attribute name and split information\n",
    "        attr_name, split = filename.rsplit('_', 1)\n",
    "        # some attributes are splitted in two files (one for each person)\n",
    "        # create a list unique attributes name\n",
    "        if attr_name.endswith('_1') or attr_name.endswith('_2'):\n",
    "            attr_name = attr_name[:-2]\n",
    "\n",
    "        return split, attr_name, ext\n",
    "\n",
    "    def _load_labels(self, label_files):\n",
    "        # splits (switch from caffe's split name convention to keras's convention)\n",
    "        _train, _val, _test = 'train', 'test', 'eval'\n",
    "\n",
    "        labels = {}\n",
    "        for split in (_train, _val, _test):\n",
    "            with open(label_files[split]) as label_file:\n",
    "                labels[split] = np.array([label.split()[1] for label in label_file],\n",
    "                                          dtype=np.int)\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def _preprocess_data(self, x_train, x_val, x_test, data_id='data'):\n",
    "        self._log.debug('Preprocessing {}.'.format(data_id))\n",
    "        data_split = [x_train, x_val, x_test]\n",
    "\n",
    "        n_features = x_train.shape[1]\n",
    "        \n",
    "        # some sort of data normalization is always required for pca\n",
    "        if self._quantization: # quantization requires data in range [0, 1] \n",
    "            scaler = Normalizer(norm='l2').fit(data_split[0])\n",
    "        else:\n",
    "            scaler = StandardScaler().fit(data_split[0])\n",
    "\n",
    "        self._log.debug('Applying data normalization to {}.'.format(data_id))\n",
    "        data_split = [scaler.transform(x) for x in data_split]\n",
    "\n",
    "        assert self._min_dim >= 1\n",
    "        \n",
    "        if n_features < self._min_dim:\n",
    "            self._log.debug(\"Skip Q-sparsity and dim reduction for {}.\" \\\n",
    "                            \"Min number of dims: {}. Found: {}\" \\\n",
    "                            .format(data_id, self._min_dim, n_features))\n",
    "            return tuple(data_split)        \n",
    "        \n",
    "        if self._quantization:\n",
    "            assert self._Q >= 1\n",
    "            # small Q promotes sparsity\n",
    "            self._log.debug('Applying Q-sparsity Q={} to {}'.format(self._Q, data_id))\n",
    "            data_split = [np.floor(self._Q * x) for x in data_split]\n",
    "\n",
    "        if not self._dim_reduction:\n",
    "            return tuple(data_split)\n",
    "            \n",
    "        if data_id in self._precomputed_pca and not self._refit_pca:\n",
    "            # use precomputed model\n",
    "            self._log.debug('Using precomputed PCA for {}'.format(data_id))            \n",
    "            pca = self._precomputed_pca[data_id]\n",
    "        else:\n",
    "            assert self._pca_conf > 0\n",
    "            # compute pca from scratch            \n",
    "            if 0 < self._pca_conf <= 1:\n",
    "                # running pca with min explained variance takes much longer\n",
    "                self._log.debug('Fitting full PCA for {}'.format(data_id))\n",
    "                pca = PCA(self._pca_conf)\n",
    "                pca.fit(data_split[0])\n",
    "            else:\n",
    "                assert self._max_dim >= 1\n",
    "                assert self._min_expl_var > 0\n",
    "\n",
    "                # search starts in the given number of components\n",
    "                n_components = self._pca_conf\n",
    "                # search min number of components with required expl. var\n",
    "                for retry in range(self._max_pca_retries): # max number of retries\n",
    "                    self._log.debug('Fitting fast PCA retry {} for {}'.format(retry+1, data_id))\n",
    "                    # running pca with number of components is much faster\n",
    "                    pca = PCA(n_components, svd_solver='randomized')\n",
    "                    pca.fit(data_split[0])\n",
    "\n",
    "                    expl_var = np.sum(pca.explained_variance_ratio_)\n",
    "                    # sometimes pca fails to compute the expl. var (is set to NaN)\n",
    "                    if (not np.isnan(expl_var) and expl_var < self._min_expl_var and n_components < self._max_dim):\n",
    "                        n_components *= 2 # exponential search\n",
    "                    else:\n",
    "                        # if pca fails or the min expl. var is achieved or max retries\n",
    "                        break # stop trying\n",
    "            # store pca coefficients for future use\n",
    "            self._log.debug('Storing PCA fit for {}'.format(data_id))\n",
    "            self._precomputed_pca[data_id] = pca\n",
    "\n",
    "        explained_var = np.sum(pca.explained_variance_ratio_)\n",
    "        n_components = pca.n_components_\n",
    "        msg = 'Applying PCA with explained var {} dims {} to {}'\n",
    "        self._log.debug(msg.format(explained_var, n_components, data_id))\n",
    "        # pca transformationl\n",
    "        data_split = [pca.transform(x) for x in data_split]\n",
    "\n",
    "        return tuple(data_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake call to main to process inputs arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\n",
    "    \"--project_dir\", \"/home/shared/Documents/final_proj\",\n",
    "    \"--epochs\", \"30\",\n",
    "    \"--batch_size\", \"256\",\n",
    "]\n",
    "\n",
    "conf = main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading precomputed features and labels (may take some time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper = SocialClassifierWithPreComputedFeatures(conf.STORED_FEATURES_DIR, \n",
    "                                                 conf.LABEL_FILES)\n",
    "\n",
    "# load features and labels\n",
    "helper.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "helper.list_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure dimensionality reduction\n",
    "helper._min_dim = 25\n",
    "helper._max_dim = 200\n",
    "helper._max_pca_retries = 3\n",
    "helper._pca_conf = 200\n",
    "helper._min_expl_var = 0.95\n",
    "helper._quantization = False\n",
    "helper._dim_reduction = True\n",
    "helper._Q = 32\n",
    "helper._refit_pca = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select attributes (default all), prepare splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_selector = AttributeSelector(helper.list_attributes())\n",
    "\n",
    "# all / face / body / or single attribute (accept name substring, e.g. activity)\n",
    "attributes_query = 'all'\n",
    "# expand all / face / body / single attribute\n",
    "selected_attributes = attribute_selector.filter(attributes_query)\n",
    "helper._log.info('Selected attribute(s): {}'.format(attributes_query))\n",
    "\n",
    "# prepare splits for selected attributes\n",
    "data_split = helper.get_data_split(selected_attributes, preprocess=True)\n",
    "\n",
    "# prepate data for keras (multiple outputs for domain/relation and one-hot encoding)\n",
    "keras_data_split = helper.prepare_data(data_split)\n",
    "\n",
    "# class_weight for keras (balance domain/relation instances)\n",
    "class_weight = compute_class_weight_relation_domain(data_split[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# after preparing data (needs input dimensions)\n",
    "\n",
    "# helper.init_model('top_down')\n",
    "\n",
    "# allows more flexibility\n",
    "helper.set_custom_model(\n",
    "    create_model_top_down(helper._n_features),\n",
    "    optimizer=keras.optimizers.Adam(0.0001, decay=1e-6),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = conf.BATCH_SIZE\n",
    "epochs = conf.EPOCHS\n",
    "\n",
    "# FIXME: set directory correctly\n",
    "checkpoint_path = os.path.join(egosocial.config.MODELS_CACHE_DIR, 'multi_attribute',\n",
    "                               'weights.{epoch:02d}-{val_loss:.2f}.h5')\n",
    "callbacks = [\n",
    "#            ModelCheckpoint(\n",
    "#                filepath=checkpoint_path, \n",
    "#                monitor='val_loss', save_best_only=True\n",
    "#            ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.1, patience=10, min_lr=0.00001\n",
    "    ),\n",
    "    PlotLearning(update_step=1),\n",
    "]\n",
    "\n",
    "\n",
    "hist = helper.fit(\n",
    "    keras_data_split['train'], \n",
    "    keras_data_split['val'],\n",
    "    batch_size=batch_size, epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "#    class_weight=class_weight,\n",
    "    verbose=0, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = helper.evaluate(\n",
    "    keras_data_split['test'],\n",
    "    batch_size=batch_size\n",
    ")\n",
    "for score, metric_name in zip(scores, helper.model.metrics_names):\n",
    "    helper._log.info(\"%s : %0.4f\" % (metric_name, score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
