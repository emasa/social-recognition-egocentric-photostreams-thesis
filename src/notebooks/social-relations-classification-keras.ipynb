{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Constants Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2001e9ef35e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# !/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# render matplot figures inside jupyter notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# add egosocial to the python path\n",
    "from os.path import dirname, abspath\n",
    "sys.path.extend([dirname(dirname(abspath(__file__)))])\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers.noise import AlphaDropout\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import egosocial.config\n",
    "from egosocial.core.types import relation_to_domain, relation_to_domain_vec\n",
    "from egosocial.utils.caffe.misc import load_levelDB_as_array\n",
    "from egosocial.utils.keras.autolosses import AutoMultiLossWrapper\n",
    "from egosocial.utils.logging import setup_logging\n",
    "\n",
    "# constants\n",
    "DOMAIN, RELATION = 'domain', 'relation'\n",
    "END_TO_END, ATTRIBUTES = 'end_to_end', 'attributes'\n",
    "\n",
    "N_CLS_RELATION, N_CLS_DOMAIN = 16, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unused functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(conf):\n",
    "    # preprocess features from original formats (leveldb, numpy, matlab)\n",
    "    if conf.PROCESS_FEATURES:\n",
    "        if conf.IS_END2END:\n",
    "            # LEVELDB DIRS\n",
    "            levelDB_dirs = [conf.FEATURES_DIR]\n",
    "            attribute_features = preprocess_attributes(levelDB_dirs)\n",
    "        else:\n",
    "            # LEVELDB DIRS\n",
    "            levelDB_dirs = [conf.FEATURES_DIR]\n",
    "            # MATLAB DIRS\n",
    "            matlab_dirs = [\n",
    "                os.path.join(conf.ATTR_MODELS_DIR,\n",
    "                             'localation_scale_data(annotator_consistency3)')\n",
    "            ]\n",
    "            # NUMPY DIRS\n",
    "            numpy_dirs = [\n",
    "                os.path.join(conf.ATTR_MODELS_DIR,\n",
    "                             'imsitu_body_activity(annotator_consistency3)'),\n",
    "                os.path.join(conf.ATTR_MODELS_DIR,\n",
    "                             'body_immediacy(annotator_consistency3)')\n",
    "            ]\n",
    "\n",
    "            attribute_features = preprocess_attributes(levelDB_dirs, numpy_dirs,\n",
    "                                                       matlab_dirs)\n",
    "\n",
    "        if not (os.path.isdir(conf.STORED_FEATURES_DIR)):\n",
    "            os.mkdir(conf.STORED_FEATURES_DIR)\n",
    "\n",
    "        # save features to disk\n",
    "        save_features(attribute_features, conf.STORED_FEATURES_DIR,\n",
    "                      compressed=True)\n",
    "\n",
    "    else:\n",
    "        # load features from disk\n",
    "        attribute_features = load_features(conf.STORED_FEATURES_DIR)\n",
    "\n",
    "    return attribute_features\n",
    "\n",
    "\n",
    "def compute_stats(X, y, clf):\n",
    "    y_predicted = clf.predict(X)\n",
    "    acc = sklearn.metrics.accuracy_score(y, y_predicted)\n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(y, y_predicted)\n",
    "    report = sklearn.metrics.classification_report(y, y_predicted)\n",
    "\n",
    "    return acc, confusion_matrix, report\n",
    "\n",
    "\n",
    "def print_statistics(val_stats=None, test_stats=None, fdesc=sys.stdout):\n",
    "    for description, stats in [('Validation set:', val_stats),\n",
    "                               ('Test set:', test_stats)]:\n",
    "\n",
    "        if stats is not None:\n",
    "            print(description, file=fdesc)\n",
    "            accuracy, confusion_matrix, report = stats\n",
    "            print('Confusion matrix:', file=fdesc)\n",
    "            print(confusion_matrix, file=fdesc)\n",
    "            print(file=fdesc)\n",
    "            print(report, file=fdesc)\n",
    "            print('SGD accuracy: {:.3f}'.format(accuracy), file=fdesc)\n",
    "            print('------------------------------------------------',\n",
    "                  file=fdesc)\n",
    "\n",
    "\n",
    "def preprocess_attributes(levelDB_dirs=None, raw_numpy_dirs=None,\n",
    "                          matlab_dirs=None):\n",
    "    splits = ['train', 'test', 'eval']\n",
    "    attribute_features = {split: {} for split in splits}\n",
    "\n",
    "    ###########################################################################\n",
    "    # features in levelDB format\n",
    "    if levelDB_dirs:\n",
    "        for directory in levelDB_dirs:\n",
    "            for split in splits:\n",
    "                attribute_models = os.listdir(os.path.join(directory, split))\n",
    "\n",
    "                for attr_name in attribute_models:\n",
    "                    features = load_levelDB_as_array(\n",
    "                        os.path.join(directory, split, attr_name))\n",
    "                    attribute_features[split][attr_name] = features\n",
    "                    msg = \"Convert from levelDB dataset: {} attribute: {} \" \\\n",
    "                          \"dim: {}\"\n",
    "                    print(msg.format(split, attr_name, features.shape))\n",
    "\n",
    "    ###########################################################################\n",
    "    # features in numpy format\n",
    "    if raw_numpy_dirs:\n",
    "        for directory in raw_numpy_dirs:\n",
    "            for numpy_file in os.listdir(directory):\n",
    "                filename, ext = os.path.splitext(numpy_file)\n",
    "                if ext.lower() == '.npy':\n",
    "                    # load numpy\n",
    "                    features = np.load(os.path.join(directory, numpy_file))\n",
    "                    # find split\n",
    "                    for candidate_split in splits:\n",
    "                        if candidate_split in filename:\n",
    "                            split = candidate_split\n",
    "                            break\n",
    "                    else:\n",
    "                        split = None\n",
    "                    # the folder name is the attribute name\n",
    "                    attr_name = os.path.basename(directory)\n",
    "\n",
    "                    attribute_features[split][attr_name] = features\n",
    "                    msg = \"Load numpy format dataset: {} attribute: {} dim: {}\"\n",
    "                    print(msg.format(split, attr_name, features.shape))\n",
    "\n",
    "    ###########################################################################\n",
    "    # features in matlab format\n",
    "    if matlab_dirs:\n",
    "        for directory in matlab_dirs:\n",
    "            for matfile in os.listdir(directory):\n",
    "                filename, ext = os.path.splitext(matfile)\n",
    "                if ext.lower() == '.mat':\n",
    "                    # load matfile (dict format)\n",
    "                    matfile_dict = scipy.io.loadmat(\n",
    "                        os.path.join(directory, matfile))\n",
    "                    attr_name, split = filename.rsplit('_', 1)\n",
    "                    # access numpy field\n",
    "                    features = matfile_dict[attr_name]\n",
    "                    attribute_features[split][attr_name] = features\n",
    "                    msg = \"Convert matlab format dataset: {} attribute: {} \" \\\n",
    "                          \"dim: {}\"\n",
    "                    print(msg.format(split, attr_name, features.shape))\n",
    "\n",
    "    ###########################################################################\n",
    "\n",
    "    return attribute_features\n",
    "\n",
    "\n",
    "def save_features(attribute_features, features_dir, compressed=True):\n",
    "    if not (os.path.exists(features_dir) and os.path.isdir(features_dir)):\n",
    "        os.mkdir(features_dir)\n",
    "\n",
    "    for split, attributes in attribute_features.items():\n",
    "        for attr_name, features in attributes.items():\n",
    "            features_path = os.path.join(features_dir, '{}_{}').format(\n",
    "                attr_name, split)\n",
    "\n",
    "            # save file in compress format and float16\n",
    "            if compressed:\n",
    "                np.savez_compressed(features_path, features.astype(np.float16))\n",
    "            else:\n",
    "                np.save(features_path, features)\n",
    "\n",
    "            print(\"Saved {}.{} ...\".format(features_path,\n",
    "                                           'npz' if compressed else 'np'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input arguments and fake main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    def __init__(self, args):\n",
    "        self.DATA_TYPE = RELATION\n",
    "        self.ARCH = 'caffeNet'\n",
    "        self.LAYER = 'fc7'\n",
    "\n",
    "        self.CONFIG = '{}_{}_{}'.format(self.LAYER, self.DATA_TYPE, self.ARCH)\n",
    "\n",
    "        # setup directories\n",
    "        self.PROJECT_DIR = args.project_dir\n",
    "        self.BASE_MODELS_DIR = os.path.join(self.PROJECT_DIR,\n",
    "                                            'models/trained_models')\n",
    "        self.ATTR_MODELS_DIR = os.path.join(self.BASE_MODELS_DIR,\n",
    "                                            'attribute_models')\n",
    "        self.SVM_MODELS_DIR = os.path.join(self.PROJECT_DIR,\n",
    "                                           'models/svm_models')\n",
    "\n",
    "        self.SPLITS_DIR = os.path.join(self.PROJECT_DIR,\n",
    "                                       'datasets/splits/annotator_consistency3')\n",
    "\n",
    "        self.STATS_MODELS_DIR = os.path.join(self.SVM_MODELS_DIR, 'stats')\n",
    "\n",
    "        LABEL_FILE_FMT = 'single_body1_{}_16.txt'\n",
    "        self.LABEL_FILES = {split: os.path.join(self.SPLITS_DIR,\n",
    "                                                LABEL_FILE_FMT.format(split))\n",
    "                            for split in ('train', 'test', 'eval')}\n",
    "\n",
    "        self.IS_END2END = False\n",
    "\n",
    "        self.BASE_FEATURES_DIR = os.path.join(self.PROJECT_DIR,\n",
    "                                              'extracted_features')\n",
    "        self.FEATURES_DIR = os.path.join(self.BASE_FEATURES_DIR,\n",
    "                                         'attribute_features',\n",
    "                                         self.CONFIG)\n",
    "\n",
    "        self.STORED_FEATURES_DIR = os.path.join(self.FEATURES_DIR,\n",
    "                                                'all_splits_numpy_format')\n",
    "\n",
    "        self.PROCESS_FEATURES = args.port_features\n",
    "\n",
    "        self.EPOCHS = args.epochs\n",
    "        self.BATCH_SIZE = args.batch_size\n",
    "\n",
    "        # reuse precomputed model?\n",
    "        self.REUSE_MODEL = args.reuse_model\n",
    "        # save model to disk?\n",
    "        self.SAVE_MODEL = args.save_model\n",
    "        # save model statistics to disk?\n",
    "        self.SAVE_STATS = args.save_stats\n",
    "        \n",
    "def positive_int(value):\n",
    "    ivalue = int(value)\n",
    "    if ivalue <= 0:\n",
    "        raise argparse.ArgumentTypeError(\n",
    "            \"%s is an invalid positive int value\" % value)\n",
    "    return ivalue\n",
    "\n",
    "def main(*fake_args):\n",
    "    setup_logging(egosocial.config.LOGGING_CONFIG)\n",
    "\n",
    "    entry_msg = 'Reproduce experiments in Social Relation Recognition paper.'\n",
    "    parser = argparse.ArgumentParser(description=entry_msg)\n",
    "\n",
    "    parser.add_argument('--project_dir', required=True,\n",
    "                        help='Base directory.')\n",
    "\n",
    "    parser.add_argument('--port_features', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Whether port features from other formats to'\n",
    "                             'numpy.')\n",
    "\n",
    "    parser.add_argument('--reuse_model', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Use precomputed model if available.')\n",
    "\n",
    "    parser.add_argument('--save_model', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Save model to disk.')\n",
    "\n",
    "    parser.add_argument('--save_stats', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Save statistics to disk.')\n",
    "\n",
    "    parser.add_argument('--epochs', required=False, type=positive_int,\n",
    "                        default=100,\n",
    "                        help='Max number of epochs.')\n",
    "\n",
    "    parser.add_argument('--batch_size', required=False, type=positive_int,\n",
    "                        default=32,\n",
    "                        help='Batch size.')\n",
    "\n",
    "    # TODO: implement correctly\n",
    "    args = parser.parse_args(*fake_args)\n",
    "    # keep configuration\n",
    "    conf = Configuration(args)\n",
    "\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributeSelector:\n",
    "    def __init__(self, all_attrs):\n",
    "        body_attributes = self.filter_by_keyword(all_attrs, 'body')\n",
    "        face_attributes = self.filter_by_keyword(all_attrs, 'face')\n",
    "        face_attributes.extend(self.filter_by_keyword(all_attrs, 'head'))\n",
    "\n",
    "        self._selector = {'all': all_attrs,\n",
    "                          'body': body_attributes,\n",
    "                          'face': face_attributes}\n",
    "\n",
    "    def filter(self, query):\n",
    "        if query in self._selector:\n",
    "            selected_attributes = self._selector[query]\n",
    "        else:\n",
    "            selected_attributes = self.filter_by_keyword(self._selector,\n",
    "                                                         query)\n",
    "\n",
    "        return selected_attributes\n",
    "\n",
    "    def filter_by_keyword(self, attribute_list, key):\n",
    "        return [attr_name for attr_name in attribute_list if key in attr_name]\n",
    "    \n",
    "def create_model_top_down(n_features):\n",
    "    input_features = Input(shape=[n_features], \n",
    "                           name='attribute_features',\n",
    "                           dtype='float')\n",
    "    x = input_features\n",
    "\n",
    "    x = Dense(128, name='dense_1',\n",
    "                   activation='selu',\n",
    "                   bias_initializer='lecun_normal',\n",
    "                   kernel_initializer='lecun_normal',\n",
    "             )(x)\n",
    "\n",
    "    x = AlphaDropout(0.25)(x)\n",
    "\n",
    "    domain = Dense(N_CLS_DOMAIN, name='domain',\n",
    "                   activation='softmax',\n",
    "                   bias_regularizer=regularizers.l2(0.1),\n",
    "                   kernel_regularizer=regularizers.l2(0.1),\n",
    "                  )(x)\n",
    "\n",
    "    x = keras.layers.concatenate([x, domain])\n",
    "    \n",
    "    relation = Dense(N_CLS_RELATION, name='relation',\n",
    "                     activation='softmax',\n",
    "                     bias_regularizer=regularizers.l2(0.1),\n",
    "                     kernel_regularizer=regularizers.l2(0.1),\n",
    "                    )(x)\n",
    "\n",
    "    model = Model(inputs=[input_features], outputs=[domain, relation])\n",
    "\n",
    "    return model\n",
    "\n",
    "def domain_to_relation_map():\n",
    "    W = [np.zeros(N_CLS_RELATION) for _ in range(N_CLS_DOMAIN)]\n",
    "    for rel in range(N_CLS_RELATION):\n",
    "        dom = relation_to_domain(rel)\n",
    "        W[dom] += to_categorical(rel, N_CLS_RELATION)\n",
    "    return np.array(W).T\n",
    "\n",
    "def create_model_fix_domain(n_features):\n",
    "\n",
    "    input_features = Input(shape=[n_features], \n",
    "                           name='attribute_features',\n",
    "                           dtype='float',\n",
    "                           )\n",
    "\n",
    "    x = input_features\n",
    "\n",
    "    relation = Dense(N_CLS_RELATION, name='relation',\n",
    "                     activation='softmax',\n",
    "                     bias_regularizer=l2(0.05),\n",
    "                     kernel_regularizer=l2(0.05)\n",
    "                    )(x)    \n",
    "    \n",
    "    domain = Dense(N_CLS_DOMAIN, name='domain',\n",
    "                   activation='linear',\n",
    "                   use_bias=False, trainable=False,\n",
    "                   weights=[domain_to_relation_map()],\n",
    "                  )(relation)\n",
    "    \n",
    "    model = Model(inputs=[input_features], outputs=[domain, relation])\n",
    "\n",
    "    return model\n",
    "\n",
    "def prepare_data_split_for_keras(data_split):\n",
    "    x_train, x_val, x_test, *labels = data_split\n",
    "    # one-hot encoding for relation\n",
    "    y_train_rel, y_val_rel, y_test_rel = [\n",
    "        to_categorical(y, N_CLS_RELATION) for y in labels\n",
    "    ]\n",
    "    # one-hot encoding for domain\n",
    "    y_train_dom, y_val_dom, y_test_dom = [\n",
    "        to_categorical(relation_to_domain_vec(y), N_CLS_DOMAIN) for y in labels\n",
    "    ]\n",
    "\n",
    "    x_train_inputs = {'attribute_features': x_train}\n",
    "    y_train_outputs = {'relation': y_train_rel, 'domain': y_train_dom}\n",
    "    x_val_inputs = {'attribute_features': x_val}\n",
    "    y_val_outputs = {'relation': y_val_rel, 'domain': y_val_dom}\n",
    "    x_test_inputs = {'attribute_features': x_test}\n",
    "    y_test_outputs = {'relation': y_test_rel, 'domain': y_test_dom}\n",
    "    \n",
    "    result = dict(train=(x_train_inputs, y_train_outputs),\n",
    "                  val=(x_val_inputs, y_val_outputs),\n",
    "                  test=(x_test_inputs, y_test_outputs))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialClassifierWithPreComputedFeatures:\n",
    "    \n",
    "    def __init__(self, features_dir, label_files):\n",
    "        self._features_dir = features_dir\n",
    "        self._label_files = label_files        \n",
    "        self._log = logging.getLogger(os.path.basename(__file__))        \n",
    "        \n",
    "        # parameters\n",
    "        # enables dimentionality reduction\n",
    "        self._dim_reduction = True\n",
    "        # cache PCA instances\n",
    "        self._precomputed_pca = {}        \n",
    "        # force recomputing PCA every time\n",
    "        self._refit_pca = False\n",
    "\n",
    "        # if any of these parameters change, PCA should be recomputed\n",
    "        # features quantization (smaller Q promotes sparsity)\n",
    "        self._quantization = False\n",
    "        self._Q = 32\n",
    "        # parameters for PCA search\n",
    "        self._min_dim = 50 # min number of components\n",
    "        self._max_dim = 200 # max number of components\n",
    "        self._min_expl_var = 0.8 # min desired explained variance\n",
    "        self._max_pca_retries = 3 # max number of retries\n",
    "        # if parameter is set in (0,1] \n",
    "        # full PCA is fitted  and keep number of components for the \n",
    "        # required expl. var; otherwise it performs search\n",
    "        self._pca_conf = self._min_dim\n",
    "\n",
    "        self._attribute_features = None\n",
    "        self._labels = None\n",
    "        self._n_features = None\n",
    "        self.model = None\n",
    "        \n",
    "    def load_data(self):\n",
    "        self._attribute_features = self._load_features(self._features_dir)\n",
    "        self._labels = self._load_labels(self._label_files)\n",
    "\n",
    "        attributes = self.list_attributes()\n",
    "        \n",
    "        self._log.info('Found {} attributes. List: '.format(len(attributes)))\n",
    "        for attr in attributes:\n",
    "            self._log.info('{}'.format(attr))\n",
    "        \n",
    "        # reset internal attributes\n",
    "        self._n_features = None\n",
    "        self.model = None\n",
    "\n",
    "    def list_attributes(self):\n",
    "        # list attributes\n",
    "        if self._attribute_features:\n",
    "            return sorted(self._attribute_features['train'].keys())\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def get_data_split(self, selected_attributes, preprocess=True):\n",
    "        # get data splits composed by selected attributes only\n",
    "        # preprocess the data\n",
    "\n",
    "        # splits (switch from caffe's split name convention to keras's convention)\n",
    "        _train, _val, _test = 'train', 'test', 'eval'\n",
    "        attribute_features = self._attribute_features\n",
    "        labels = self._labels\n",
    "        \n",
    "        # assert attributes\n",
    "        assert attribute_features\n",
    "        assert labels        \n",
    "        \n",
    "        if preprocess:\n",
    "            features = defaultdict(dict)\n",
    "            # preprocess each selected attribute individually\n",
    "            for attr in selected_attributes:\n",
    "                data = self._preprocess_data(attribute_features[_train][attr]\n",
    "                                            , attribute_features[_val][attr]\n",
    "                                            , attribute_features[_test][attr]\n",
    "                                            , data_id=attr)\n",
    "                for split_idx, split in enumerate((_train, _val, _test)):\n",
    "                    features[split][attr] = data[split_idx]\n",
    "        else:\n",
    "            features = attribute_features\n",
    "\n",
    "        # concatenate attributes\n",
    "        fused_features = {}\n",
    "        for split in (_train, _val, _test):\n",
    "            selected_features = [features[split][attr]\n",
    "                                 for attr in selected_attributes]\n",
    "\n",
    "            fused_features[split] = np.concatenate(selected_features, axis=1)\n",
    "\n",
    "        result = [fused_features[split] for split in (_train, _val, _test)]\n",
    "        result.extend([labels[split] for split in (_train, _val, _test)])\n",
    "        \n",
    "        # init number of dimensions\n",
    "        self._n_features = fused_features[_train].shape[1]\n",
    "        \n",
    "        return tuple(result)\n",
    "\n",
    "            \n",
    "    def prepare_data(self, data_split):\n",
    "        return prepare_data_split_for_keras(data_split)\n",
    "\n",
    "    def init_model(self, model_type='top_down'):\n",
    "        assert model_type in ('top_down', 'fix_domain') \n",
    "        assert self._n_features is not None\n",
    "        \n",
    "        if model_type == 'top_down':\n",
    "            model = create_model_top_down(self._n_features)\n",
    "        else:\n",
    "            model = create_model_fix_domain(self._n_features)\n",
    "        self._log.info('Initializing {} model'.format(model_type))\n",
    "        \n",
    "        # wrapper allows to train the loss weights\n",
    "        self._model_wrapper = AutoMultiLossWrapper(model)\n",
    "        self._model_wrapper.compile(\n",
    "            optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            loss_weights='auto',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        self.model = self._model_wrapper.model\n",
    "        self._log.info(self.model.summary())\n",
    "            \n",
    "        return self.model\n",
    "    \n",
    "    def fit(self, train_data, validation_data, batch_size=32, epochs=10):\n",
    "        # FIXME: set directory correctly\n",
    "        checkpoint_path = os.path.join(egosocial.config.MODELS_CACHE_DIR,\n",
    "                                       'multi_attribute',\n",
    "                                       'weights.{epoch:02d}-{val_loss:.2f}.h5')\n",
    "        callbacks = [\n",
    "#            ModelCheckpoint(\n",
    "#                filepath=checkpoint_path, \n",
    "#                monitor='val_loss', save_best_only=True\n",
    "#            ),\n",
    "#            TensorBoard(\n",
    "#                log_dir='./logs', \n",
    "#                write_images=True, write_graph=True\n",
    "#            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss', \n",
    "                factor=0.1, patience=5, min_lr=0.00001\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        self._log.info(\"Training model from scratch...\")\n",
    "        \n",
    "        return self.model.fit(\n",
    "            *train_data,\n",
    "            batch_size=batch_size, epochs=epochs,\n",
    "            validation_data=validation_data,\n",
    "            callbacks=callbacks, verbose=1,\n",
    "        )\n",
    "\n",
    "    def evaluate(self, test_data, batch_size=32):\n",
    "        return self.model.evaluate(\n",
    "            *test_data,\n",
    "            batch_size=batch_size,\n",
    "            verbose=1\n",
    "        )    \n",
    "        \n",
    "    def _load_features(self, features_dir):\n",
    "        # splits (switch from caffe's split name convention to keras's convention)\n",
    "        _train, _val, _test = 'train', 'test', 'eval'\n",
    "\n",
    "        attribute_features = {split: {} for split in (_train, _val, _test)}\n",
    "\n",
    "        for numpy_file in sorted(os.listdir(features_dir)):\n",
    "            split, attr_name, ext = self._parse_filename(numpy_file)\n",
    "            \n",
    "            # absolute path\n",
    "            features_path = os.path.join(features_dir, numpy_file)\n",
    "\n",
    "            if ext in ('.npz', '.npy'):\n",
    "                self._log.debug(\"Loading {}...\".format(features_path))\n",
    "            else:\n",
    "                self._log.warn(\"Found file with unknown format.\".format(features_path))\n",
    "                continue # skip this file\n",
    "            \n",
    "            if ext == '.npz':\n",
    "                with np.load(features_path) as data:\n",
    "                    features = data['arr_0']\n",
    "            else:\n",
    "                features = np.load(features_path)\n",
    "            \n",
    "            # fuse attributes in several files\n",
    "            if attr_name in attribute_features[split]:\n",
    "                array = np.concatenate([attribute_features[split][attr_name], \n",
    "                                        features.astype('float32', copy=False)],\n",
    "                                       axis=1)\n",
    "            else:\n",
    "                array = features.astype('float32', copy=False)\n",
    "\n",
    "            attribute_features[split][attr_name] = array\n",
    "\n",
    "        return attribute_features    \n",
    "\n",
    "    def _parse_filename(self, numpy_file):\n",
    "        # split the extension from the path and normalize it to lowercase.\n",
    "        filename, ext = os.path.splitext(numpy_file)\n",
    "        ext = ext.lower()\n",
    "\n",
    "        # extract attribute name and split information\n",
    "        attr_name, split = filename.rsplit('_', 1)\n",
    "        # some attributes are splitted in two files (one for each person)\n",
    "        # create a list unique attributes name\n",
    "        if attr_name.endswith('_1') or attr_name.endswith('_2'):\n",
    "            attr_name = attr_name[:-2]\n",
    "\n",
    "        return split, attr_name, ext\n",
    "    \n",
    "    def _load_labels(self, label_files):\n",
    "        # splits (switch from caffe's split name convention to keras's convention)\n",
    "        _train, _val, _test = 'train', 'test', 'eval'\n",
    "\n",
    "        labels = {}\n",
    "        for split in (_train, _val, _test):\n",
    "            with open(label_files[split]) as label_file:\n",
    "                labels[split] = np.array([label.split()[1] for label in label_file],\n",
    "                                          dtype=np.int)\n",
    "        \n",
    "        return labels\n",
    "\n",
    "    def _preprocess_data(self, x_train, x_val, x_test, data_id='data'):\n",
    "        \n",
    "        self._log.debug('Preprocessing {}.'.format(data_id))\n",
    "        data_split = [x_train, x_val, x_test]\n",
    "        \n",
    "        if not self._dim_reduction:\n",
    "            self._log.warn('Dim reduction disabled.')\n",
    "            return tuple(data_split)\n",
    "        \n",
    "        assert self._n_features >= 1\n",
    "        assert self._min_dim >= 1\n",
    "        \n",
    "        if self._n_features < self._min_dim:\n",
    "            self._log.debug('Skip dim reduction for {}. Min number of dims: {}.' \\\n",
    "                            .format(data_id, self._min_dim))\n",
    "            return tuple(data_split)\n",
    "        # some sort of data normalization is always required for pca\n",
    "        if self._quantization:\n",
    "            scaler = Normalizer(norm='l2').fit(data_split[0])\n",
    "        else:\n",
    "            scaler = StandardScaler().fit(data_split[0])\n",
    "\n",
    "        self._log.debug('Applying data normalization to {}.'.format(data_id))\n",
    "        data_split = [scaler.transform(x) for x in data_split]\n",
    "        \n",
    "        if self._quantization:\n",
    "            assert self._Q >= 1\n",
    "            # Q promotes sparsity, default\n",
    "            self._log.debug('Applying Q-sparsity Q={} to {}'.format(Q, data_id))\n",
    "            data_split = [np.floor(self._Q * x) for x in data_split]\n",
    "\n",
    "        if data_id in self._precomputed_pca and not self._refit_pca:\n",
    "            # use precomputed model\n",
    "            self._log.debug('Using precomputed PCA for {}'.format(data_id))            \n",
    "            pca = self._precomputed_pca['data_id']\n",
    "        else:\n",
    "            assert self._pca_conf > 0\n",
    "            # compute pca from scratch            \n",
    "            if 0 < self._pca_conf <= 1:\n",
    "                # running pca with min explained variance takes much longer\n",
    "                self._log.debug('Fitting full PCA for {}'.format(data_id))\n",
    "                pca = PCA(self._pca_conf)\n",
    "                pca.fit(data_split[0])\n",
    "            else:\n",
    "                assert self._max_dim >= 1\n",
    "                assert self._min_expl_var > 0\n",
    "                \n",
    "                # search starts in the given number of components\n",
    "                n_components = self._pca_conf\n",
    "                # search min number of components with required expl. var\n",
    "                for retry in range(self._max_pca_retries): # max number of retries\n",
    "                    self._log.debug('Fitting fast PCA retry {} for {}'.format(retry+1, data_id))\n",
    "                    # running pca with number of components is much faster\n",
    "                    pca = PCA(n_components, svd_solver='randomized')\n",
    "                    pca.fit(data_split[0])\n",
    "\n",
    "                    expl_var = np.sum(pca.explained_variance_ratio_)\n",
    "                    # sometimes pca fails to compute the expl. var (is set to NaN)\n",
    "                    if (not np.isnan(explained_var) and expl_var < self._min_expl_var \n",
    "                        and n_components < self._max_dim):\n",
    "                        n_components *= 2 # exponential search\n",
    "                    else:\n",
    "                        # if pca fails or the min expl. var is achieved or max retries\n",
    "                        break # stop trying\n",
    "            # store pca coefficients for future use\n",
    "            self._log.debug('Storing PCA fit for {}'.format(data_id))\n",
    "            self._precomputed_pca['data_id'] = pca\n",
    "            \n",
    "        explained_var = np.sum(pca.explained_variance_ratio_)\n",
    "        n_components = pca.n_components_\n",
    "        msg = 'Applying PCA with explained var {} dims {} to {}'\n",
    "        self._log.debug(msg.format(explained_var, n_components, data_id))\n",
    "        # pca transformation\n",
    "        data_split = [pca.transform(x) for x in data_split]\n",
    "        \n",
    "        return tuple(data_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake call to main to process arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'setup_logging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-7be75a45a5c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m ]\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-7297498ee384>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(*fake_args)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfake_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0msetup_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0megosocial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOGGING_CONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mentry_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Reproduce experiments in Social Relation Recognition paper.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'setup_logging' is not defined"
     ]
    }
   ],
   "source": [
    "args = [\n",
    "    \"--project_dir\", \"/home/shared/Documents/final_proj\",\n",
    "    \"--epochs\", \"30\",\n",
    "    \"--batch_size\", \"128\",\n",
    "]\n",
    "\n",
    "conf = main(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading precomputed features and labels (may take some time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c452a4eae1db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m helper = SocialClassifierWithPreComputedFeatures(conf.STORED_FEATURES_DIR, \n\u001b[0m\u001b[1;32m      2\u001b[0m                                                  conf.LABEL_FILES)\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# load features and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conf' is not defined"
     ]
    }
   ],
   "source": [
    "helper = SocialClassifierWithPreComputedFeatures(conf.STORED_FEATURES_DIR, \n",
    "                                                 conf.LABEL_FILES)\n",
    "\n",
    "# load features and labels\n",
    "helper.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select attributes (default all), prepare splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_selector = AttributeSelector(helper.list_attributes())\n",
    "\n",
    "# all / face / body / or single attributes\n",
    "attributes_query = 'all'\n",
    "# expand all / face / body / single attribute\n",
    "selected_attributes = attribute_selector.filter(attributes_query)\n",
    "helper._log.info('Selected attribute(s): {}'.format(attributes_query))\n",
    "\n",
    "# prepare splits for selected attributes\n",
    "data_split = helper.get_data_split(selected_attributes, preprocess=True)\n",
    "\n",
    "# prepate data for keras (outputs domain/relation and one-hot encoding)\n",
    "keras_data_split = helper.prepare_data(data_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after preparing data (needs input dimensions)\n",
    "helper.init_model('top_down')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = conf.BATCH_SIZE\n",
    "epochs = conf.EPOCHS\n",
    "\n",
    "hist = helper.fit(\n",
    "    keras_data_split['train'], \n",
    "    keras_data_split['val'],\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for idx, loss_name in enumerate(hist.history):\n",
    "    plt.subplot(len(hist.history),1,idx+1)\n",
    "    plt.plot(hist.history[loss_name])\n",
    "    plt.title(loss_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = helper.evaluate(\n",
    "    keras_data_split['test'],\n",
    "    batch_size=batch_size\n",
    ")\n",
    "for score, metric_name in zip(scores, model.metrics_names):\n",
    "    helper._log.info(\"%s : %0.4f\" % (metric_name, score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
