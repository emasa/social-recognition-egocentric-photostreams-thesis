{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_notebook(fix_python_path=True, reduce_margins=True, plot_inline=True):\n",
    "    if reduce_margins:\n",
    "        # Reduce side margins of the notebook\n",
    "        from IPython.core.display import display, HTML\n",
    "        display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "    if fix_python_path:\n",
    "        # add egosocial to the python path\n",
    "        import os, sys\n",
    "        sys.path.extend([os.path.dirname(os.path.abspath('.'))])\n",
    "\n",
    "    if plot_inline:\n",
    "        # Plots inside cells\n",
    "        %matplotlib inline\n",
    "\n",
    "setup_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Constants Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import threading\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import SVG\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Input\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Cropping2D \n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.noise import AlphaDropout\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "import egosocial\n",
    "import egosocial.config\n",
    "from egosocial.core.types import relation_to_domain\n",
    "from egosocial.core.types import relation_to_domain_vec\n",
    "from egosocial.utils.filesystem import create_directory \n",
    "from egosocial.utils.filesystem import check_directory\n",
    "from egosocial.utils.logging import setup_logging\n",
    "from egosocial.utils.keras.autolosses import AutoMultiLossWrapper\n",
    "from egosocial.utils.keras.backend import limit_gpu_allocation_tensorflow\n",
    "from egosocial.utils.keras.callbacks import PlotLearning\n",
    "from egosocial.utils.keras.layers import LRN\n",
    "from egosocial.utils.keras.utils import flow_from_dirs\n",
    "from egosocial.utils.keras.utils import fuse_inputs_generator\n",
    "\n",
    "# TODO: move these constants to the library\n",
    "DOMAIN, RELATION = 'domain', 'relation'\n",
    "END_TO_END, ATTRIBUTES = 'end_to_end', 'attributes'\n",
    "N_CLS_RELATION, N_CLS_DOMAIN = 16, 5\n",
    "RELATION_LABELS = [str(label) for label in range(N_CLS_RELATION)]\n",
    "FILE_FORMAT = '{prefix}{dtype}{idx}_{split}_{n_cls}{ext}'\n",
    "\n",
    "MODEL_INPUTS = ('body/1', 'body/2', 'face/1', 'face/2')\n",
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "\n",
    "# it should be (227, 227, 3), but the model adds padding anyway\n",
    "INPUT_SHAPE = (228, 228, 3)\n",
    "SHARED_SEED = 1\n",
    "STREAM_IDS = [str(idx) for idx in [1, 2]]\n",
    "\n",
    "DEPRECATED_HOME = '/root/'\n",
    "NEW_HOME='/home/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit GPU memory allocation with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if K.backend() == 'tensorflow':\n",
    "    limit_gpu_allocation_tensorflow(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input arguments and fake main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file(split, idx=1, dtype='body', n_cls=N_CLS_RELATION, \n",
    "             ext='.txt', prefix='single_'):\n",
    "    return FILE_FORMAT.format(split=split, prefix=prefix, dtype=dtype, \n",
    "                              idx=idx, n_cls=n_cls, ext=ext)\n",
    "\n",
    "class Configuration:\n",
    "    def __init__(self, args):\n",
    "        # setup directories\n",
    "        self.PROJECT_DIR = args.project_dir\n",
    "\n",
    "        self.SPLITS_DIR = os.path.join(self.PROJECT_DIR,\n",
    "                                       'datasets/splits/annotator_consistency3')\n",
    "\n",
    "        # splits (switch from caffe's split name convention to keras's convention)        \n",
    "        _train, _val, _test = 'train', 'test', 'eval'        \n",
    "        self.LABEL_FILES = {split: os.path.join(self.SPLITS_DIR, get_file(split=split))\n",
    "                            for split in (_train, _val, _test)}\n",
    "\n",
    "        self.EPOCHS = args.epochs\n",
    "        self.BATCH_SIZE = args.batch_size\n",
    "        self.LR = args.lr\n",
    "        \n",
    "        # reuse precomputed model?\n",
    "        self.REUSE_MODEL = args.reuse_model\n",
    "        # save model to disk?\n",
    "        self.SAVE_MODEL = args.save_model\n",
    "        # save model statistics to disk?\n",
    "        self.SAVE_STATS = args.save_stats\n",
    "        \n",
    "        self.FAKE_DIR = os.path.join(self.PROJECT_DIR, 'datasets', 'fake_dir')\n",
    "        \n",
    "        self.MODELS_CACHE_DIR = os.path.join(self.PROJECT_DIR, \n",
    "                                             'models/keras_models/models_keras_repr')\n",
    "        self.MODELS_CACHE_SUBDIR = 'bins' \n",
    "\n",
    "def non_negative_float(value):\n",
    "    fvalue = float(value)\n",
    "    if fvalue < 0:\n",
    "        raise argparse.ArgumentTypeError(\n",
    "            \"%s is an invalid non negative float value\" % value)\n",
    "    return fvalue\n",
    "        \n",
    "def positive_int(value):\n",
    "    ivalue = int(value)\n",
    "    if ivalue <= 0:\n",
    "        raise argparse.ArgumentTypeError(\n",
    "            \"%s is an invalid positive int value\" % value)\n",
    "    return ivalue\n",
    "\n",
    "def main(*fake_args):\n",
    "    setup_logging(egosocial.config.LOGGING_CONFIG)\n",
    "\n",
    "    entry_msg = 'Reproduce experiments in Social Relation Recognition paper.'\n",
    "    parser = argparse.ArgumentParser(description=entry_msg)\n",
    "\n",
    "    parser.add_argument('--project_dir', required=True,\n",
    "                        help='Base directory.')\n",
    "\n",
    "    parser.add_argument('--reuse_model', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Use precomputed model if available.')\n",
    "\n",
    "    parser.add_argument('--save_model', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Save model to disk.')\n",
    "\n",
    "    parser.add_argument('--save_stats', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Save statistics to disk.')\n",
    "\n",
    "    parser.add_argument('--epochs', required=False, type=positive_int,\n",
    "                        default=100,\n",
    "                        help='Max number of epochs.')\n",
    "\n",
    "    parser.add_argument('--batch_size', required=False, type=positive_int,\n",
    "                        default=32,\n",
    "                        help='Batch size.')\n",
    "    \n",
    "    parser.add_argument('--lr', required=False, type=non_negative_float,\n",
    "                        default=0.001,\n",
    "                        help='Initial learning rate.')    \n",
    "    \n",
    "    # TODO: implement correctly\n",
    "    args = parser.parse_args(*fake_args)\n",
    "    # keep configuration\n",
    "    conf = Configuration(args)\n",
    "    # check directories\n",
    "    check_directory(conf.PROJECT_DIR, 'Project')\n",
    "    check_directory(conf.SPLITS_DIR, 'Splits')\n",
    "    check_directory(os.path.join(conf.MODELS_CACHE_DIR, conf.MODELS_CACHE_SUBDIR), 'Snapshots')\n",
    "    \n",
    "    return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move to utils\n",
    "class AttributeSelector:\n",
    "    def __init__(self, all_attrs):\n",
    "        body_attributes = self.filter_by_keyword(all_attrs, 'body')\n",
    "        face_attributes = self.filter_by_keyword(all_attrs, 'face')\n",
    "        face_attributes.extend(self.filter_by_keyword(all_attrs, 'head'))\n",
    "\n",
    "        self._selector = {'all': all_attrs,\n",
    "                          'body': body_attributes,\n",
    "                          'face': face_attributes}\n",
    "\n",
    "    def filter(self, query):\n",
    "        query = query.lower()\n",
    "        if query in self._selector:\n",
    "            selected_attributes = self._selector[query]\n",
    "        else:\n",
    "            selected_attributes = self.filter_by_keyword(self._selector['all'],\n",
    "                                                         query)\n",
    "\n",
    "        return selected_attributes\n",
    "\n",
    "    def filter_by_keyword(self, attribute_list, key):\n",
    "        return [attr_name for attr_name in attribute_list if key in attr_name]\n",
    "\n",
    "# TODO: move to utils\n",
    "def domain_to_relation_map():\n",
    "    W = [np.zeros(N_CLS_RELATION) for _ in range(N_CLS_DOMAIN)]\n",
    "    for rel in range(N_CLS_RELATION):\n",
    "        dom = relation_to_domain(rel)\n",
    "        W[dom] += to_categorical(rel, N_CLS_RELATION)\n",
    "    return np.array(W).T    \n",
    "\n",
    "# TODO: move to utils\n",
    "def get_relation_domain(data_batch, inputs, mode=None):\n",
    "    mode = mode if mode else 'both_splitted'\n",
    "    assert mode in ('both_splitted', 'both_fused', 'relation', 'domain')\n",
    "    # relation output (every attribute has the same output)\n",
    "    y_rel = data_batch[0][1] # indices 0: first input, 1: label data\n",
    "    \n",
    "    if mode in ('both_splitted', 'both_fused', 'domain'):\n",
    "        # inverse of function to_categorical\n",
    "        plain_y_rel = np.argmax(y_rel, axis=1)\n",
    "        # final output is domain, relation\n",
    "        y_dom = to_categorical(relation_to_domain_vec(plain_y_rel), N_CLS_DOMAIN)\n",
    "\n",
    "    if mode == 'both_splitted':\n",
    "        return dict(domain=y_dom, relation=y_rel)\n",
    "    elif mode == 'both_fused':\n",
    "        return dict(domain_relation=np.concatenate((y_dom, y_rel), axis=1))\n",
    "    elif mode == 'domain':\n",
    "        return dict(domain=y_dom)\n",
    "    else:\n",
    "        return dict(relation=y_rel)    \n",
    "\n",
    "\n",
    "def create_data_split_generators(directory, \n",
    "                                 train_gen_args=None, \n",
    "                                 test_val_gen_args=None,\n",
    "                                 fix_path_cbk=None,\n",
    "                                 output_mode=None,\n",
    "                                 **kwargs):\n",
    "    check_directory(directory)\n",
    "    \n",
    "    inputs = list(MODEL_INPUTS)\n",
    "    def input_dirs(split):\n",
    "        return [os.path.join(directory, input_name, split) \n",
    "                for input_name in inputs]\n",
    "\n",
    "    # splits (switch from caffe's split name convention to keras's convention)    \n",
    "    _train, _val, _test = 'train', 'test', 'eval'\n",
    "        \n",
    "    split_gen_args = [ (_train, train_gen_args)\n",
    "                     , (_val, test_val_gen_args)\n",
    "                     , (_test, test_val_gen_args)]\n",
    "\n",
    "    split_generators = []\n",
    "    for split, gen_args in split_gen_args:\n",
    "        gens = flow_from_dirs(\n",
    "            input_dirs(split), \n",
    "            gen_args=gen_args,\n",
    "            fix_path_cbk=fix_path_cbk,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        fused_gen = fuse_inputs_generator(\n",
    "            gens, inputs, \n",
    "            outputs_callback=partial(get_relation_domain, mode=output_mode)\n",
    "        )\n",
    "        \n",
    "        split_generators.append(fused_gen)\n",
    "        \n",
    "    return split_generators\n",
    "\n",
    "def process_line(line):\n",
    "    image_path, label = line.strip().rsplit(' ', 1)\n",
    "\n",
    "    if image_path.startswith(DEPRECATED_HOME):\n",
    "        image_path = os.path.join(NEW_HOME, image_path[len(DEPRECATED_HOME):])\n",
    "\n",
    "    return image_path, label\n",
    "\n",
    "class FakeDirectory:\n",
    "\n",
    "    def __init__(self, fake_dir, splits_dir):\n",
    "        self.fake_dir = fake_dir\n",
    "        self.splits_dir = splits_dir \n",
    "                \n",
    "        self._fake_link_fmt = 'fakelink{}_{}'\n",
    "        self._file_map = {}\n",
    "        \n",
    "    def create_structure(self):\n",
    "        \n",
    "        self._file_map = {}\n",
    "        \n",
    "        if os.path.isdir(self.fake_dir):\n",
    "            shutil.rmtree(self.fake_dir)\n",
    "\n",
    "        create_directory(self.fake_dir, 'Fake')\n",
    "\n",
    "        for dtype in ('body', 'face'):\n",
    "            dtype_dir = os.path.join(self.fake_dir, dtype)\n",
    "            create_directory(dtype_dir, 'Fake {}'.format(dtype))\n",
    "\n",
    "            # set dtype to body or face\n",
    "            file_callback = lambda **kwargs: get_file(dtype=dtype, **kwargs)      \n",
    "\n",
    "            mapping = self._get_split_mapping(file_callback=file_callback)\n",
    "\n",
    "            for images_dir, images_file in mapping.items():\n",
    "                fake_images_dir = os.path.join(dtype_dir, images_dir)\n",
    "\n",
    "                images_file_path = os.path.join(self.splits_dir, images_dir, images_file)\n",
    "                with open(images_file_path) as f:\n",
    "                    for fake_id, line in enumerate(f):\n",
    "                        image_path, label = process_line(line)\n",
    "                        \n",
    "                        fake_label_dir = os.path.join(fake_images_dir, str(label))\n",
    "\n",
    "                        # keras asks for a different directory for each class\n",
    "                        if not os.path.exists(fake_label_dir):\n",
    "                            create_directory(fake_label_dir, 'Label')                            \n",
    "                        \n",
    "                        # create symlinks for every entry\n",
    "                        # a name may appear in several entries, so an unique fake name is created\n",
    "                        fake_name = self._fake_link_fmt.format(fake_id, os.path.basename(image_path))\n",
    "                        fake_path = os.path.join(fake_label_dir, fake_name)\n",
    "                        \n",
    "                        # keep mapping fakefile -> image_path\n",
    "                        self._file_map[fake_path] = image_path\n",
    "\n",
    "                        os.symlink(image_path, fake_path)\n",
    "\n",
    "\n",
    "    def _get_split_mapping(self, file_callback=get_file):\n",
    "        # splits (switch from caffe's split name convention to keras's convention)\n",
    "        _train, _val, _test = 'train', 'test', 'eval'\n",
    "\n",
    "        check_directory(self.splits_dir, 'Splits')\n",
    "\n",
    "        mapping = {}\n",
    "\n",
    "        for split in (_train, _val, _test):\n",
    "            for idx in STREAM_IDS:\n",
    "                key = '{idx}{sep}{split}'.format(split=split, sep=os.path.sep, idx=idx)\n",
    "                value =  os.path.join(self.splits_dir, file_callback(split=split, idx=idx))\n",
    "                mapping[key] = value\n",
    "\n",
    "        return mapping\n",
    "\n",
    "    def fix_fake_link(self, directory, filename):\n",
    "\n",
    "        fake_path = os.path.join(directory, filename)\n",
    "        assert fake_path in self._file_map        \n",
    "        image_path = self._file_map[fake_path]\n",
    "\n",
    "        fake_label_dir = os.path.split(fake_path)[0]\n",
    "        label = os.path.split(fake_label_dir)[1]\n",
    "\n",
    "        original_dir, basename = os.path.split(image_path)\n",
    "\n",
    "        # common ancestor\n",
    "        commonprefix = os.path.commonprefix([original_dir, directory])\n",
    "        # remove commonprefix\n",
    "        fake_subdir = directory[len(commonprefix):]        \n",
    "        original_subdir = original_dir[len(commonprefix):]\n",
    "        # go up in fake path up to common prefix, then go down the real path \n",
    "        go_up = os.path.join(*['..' for folder in fake_subdir.split(os.path.sep)])    \n",
    "        go_down = original_subdir\n",
    "\n",
    "        fixed_filename = os.path.join(go_up, go_down, basename)\n",
    "        return fixed_filename\n",
    "    \n",
    "def get_models(attributes, cache_dir, cache_subdir):\n",
    "    models = {}\n",
    "\n",
    "    for attr in attributes:\n",
    "        file, url =  egosocial.config.MODELS[attr]        \n",
    "\n",
    "        model_file = keras.utils.get_file(\n",
    "            file, url,\n",
    "            cache_dir=cache_dir, \n",
    "            cache_subdir=cache_subdir,\n",
    "        )\n",
    "        \n",
    "        models[attr] = keras.models.load_model(\n",
    "            model_file, \n",
    "            custom_objects={'LRN': LRN}\n",
    "        )\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialClassifier:\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self._data_dir = data_dir\n",
    "\n",
    "        self._log = logging.getLogger(self.__class__.__name__)\n",
    "        \n",
    "        # initialize when model is configured\n",
    "        self._model_wrapper = None\n",
    "        self.model = None\n",
    "        \n",
    "        self._train_gen, self._val_gen, self._test_gen = None, None, None\n",
    "    \n",
    "    def setup_datagen(self, **kwargs):\n",
    "        self._log.debug(\"Creating data generators.\")        \n",
    "\n",
    "        preprocessing_function = kwargs.pop('preprocessing_function', None)\n",
    "        \n",
    "        train_gen_args = dict(\n",
    "            rescale=1./255,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            shear_range=0.1,\n",
    "            horizontal_flip=True,\n",
    "            preprocessing_function=preprocessing_function,\n",
    "        )\n",
    "        test_val_gen_args = dict(\n",
    "            rescale=1./255,\n",
    "            preprocessing_function=preprocessing_function,\n",
    "        )\n",
    "        \n",
    "        split_gens = create_data_split_generators(\n",
    "            self._data_dir, train_gen_args, test_val_gen_args,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        self._train_gen, self._val_gen, self._test_gen = split_gens    \n",
    "        \n",
    "    def set_model(self, model, optimizer='adam', \n",
    "                  loss='categorical_crossentropy', \n",
    "                  loss_weights='auto',\n",
    "                  **kwargs):\n",
    "        assert model\n",
    "        self._log.debug(\"Initializing model.\")\n",
    "        \n",
    "        # wrapper allows to train the loss weights\n",
    "        self._model_wrapper = AutoMultiLossWrapper(model)\n",
    "        self._model_wrapper.compile(optimizer, loss, \n",
    "                                    loss_weights=loss_weights, \n",
    "                                    **kwargs)\n",
    "\n",
    "        self.model = self._model_wrapper.model\n",
    "        self._log.info(self.model.summary())\n",
    "        \n",
    "    def fit(self, steps_per_epoch, validation_steps, **kwargs):\n",
    "        assert self.model\n",
    "        assert self._train_gen\n",
    "        assert self._val_gen\n",
    "        self._log.debug(\"Training model from scratch.\")        \n",
    "\n",
    "        # train the model on the new data for a few epochs\n",
    "        return self.model.fit_generator(\n",
    "            self._train_gen,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=self._val_gen,\n",
    "            validation_steps=validation_steps,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "    def evaluate(self, steps, **kwargs):\n",
    "        assert self.model\n",
    "        assert self._test_gen\n",
    "        self._log.debug(\"Evaluating model in test data.\")\n",
    "        \n",
    "        return self.model.evaluate_generator(\n",
    "            self._test_gen,\n",
    "            steps=steps,\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_top_down(input_features, drop_rate=0.5, l2_reg=0.01, \n",
    "                          units_factor=128, hidden_layers=2, mode=None):\n",
    "    mode = mode if mode else 'both_splitted'\n",
    "    assert mode in ('both_splitted', 'both_fused', 'relation', 'domain')\n",
    "\n",
    "    normalized_input_features = BatchNormalization()(input_features)\n",
    "    x = normalized_input_features\n",
    "    \n",
    "    if mode != 'relation':\n",
    "        dom_n_units = int(np.ceil(np.log10(N_CLS_DOMAIN) * units_factor))\n",
    "\n",
    "        for _ in range(hidden_layers):\n",
    "            x = Dense(dom_n_units,\n",
    "                          activation='selu',\n",
    "                          bias_initializer='lecun_normal',\n",
    "                          kernel_initializer='lecun_normal',\n",
    "                          bias_regularizer=l2(l2_reg),\n",
    "                          kernel_regularizer=l2(l2_reg),              \n",
    "                      )(x)\n",
    "            x = AlphaDropout(drop_rate)(x)\n",
    "\n",
    "        domain = Dense(N_CLS_DOMAIN, name='domain',\n",
    "                       activation='softmax',\n",
    "                       bias_regularizer=l2(l2_reg),\n",
    "                       kernel_regularizer=l2(l2_reg),\n",
    "                      )(x)\n",
    "\n",
    "        x = keras.layers.concatenate([normalized_input_features, domain])\n",
    "\n",
    "    if mode != 'domain':\n",
    "        rel_n_units = int(np.ceil(np.log10(N_CLS_RELATION) * units_factor))\n",
    "\n",
    "        for _ in range(hidden_layers):\n",
    "            x = Dense(rel_n_units,\n",
    "                          activation='selu',\n",
    "                          bias_initializer='lecun_normal',\n",
    "                          kernel_initializer='lecun_normal',\n",
    "                          bias_regularizer=l2(l2_reg),\n",
    "                          kernel_regularizer=l2(l2_reg),              \n",
    "                      )(x)\n",
    "            x = AlphaDropout(drop_rate)(x)\n",
    "\n",
    "        relation = Dense(N_CLS_RELATION, name='relation',\n",
    "                         activation='softmax',\n",
    "                         bias_regularizer=l2(l2_reg),\n",
    "                         kernel_regularizer=l2(l2_reg),\n",
    "                        )(x)\n",
    "\n",
    "    if mode == 'both_splitted':\n",
    "        return [domain, relation]\n",
    "    elif mode == 'both_fused':\n",
    "        concat = Concatenate(name='domain_relation')\n",
    "        return [concat([domain, relation])]\n",
    "    elif mode == 'domain':\n",
    "        return [domain]\n",
    "    else:\n",
    "        return [relation]\n",
    "\n",
    "\n",
    "def create_model_bottom_up(input_features, drop_rate=0.5, l2_reg=0.01, \n",
    "                           units_factor=128, hidden_layers=2, mode=None):\n",
    "    mode = mode if mode else 'both_splitted'\n",
    "    assert mode in ('both_splitted', 'both_fused', 'relation', 'domain')\n",
    "\n",
    "    normalized_input_features = BatchNormalization()(input_features)\n",
    "    x = normalized_input_features\n",
    "\n",
    "    # always compute relation\n",
    "    rel_n_units = int(np.ceil(np.log10(N_CLS_RELATION) * units_factor))\n",
    "    for _ in range(hidden_layers):\n",
    "        x = Dense(rel_n_units,\n",
    "                      activation='selu',\n",
    "                      bias_initializer='lecun_normal',\n",
    "                      kernel_initializer='lecun_normal',\n",
    "                      bias_regularizer=l2(l2_reg),\n",
    "                      kernel_regularizer=l2(l2_reg),              \n",
    "                  )(x)\n",
    "        x = AlphaDropout(drop_rate)(x)\n",
    "\n",
    "    # domain is a lineal combination of relation\n",
    "    relation = Dense(N_CLS_RELATION, name='relation',\n",
    "                     activation='softmax',\n",
    "                     bias_regularizer=l2(l2_reg),\n",
    "                     kernel_regularizer=l2(l2_reg),\n",
    "                    )(x)    \n",
    "    \n",
    "    if mode != 'relation':\n",
    "        # use domain knowledge, weights are frozen\n",
    "        domain = Dense(N_CLS_DOMAIN, name='domain',\n",
    "                       activation='linear',\n",
    "                       use_bias=False, trainable=False,\n",
    "                       weights=[domain_to_relation_map()],\n",
    "                      )(relation)\n",
    "\n",
    "    if mode == 'both_splitted':\n",
    "        return [domain, relation]\n",
    "    elif mode == 'both_fused':\n",
    "        concat = Concatenate(axis=-1, name='domain_relation')\n",
    "        return [concat([domain, relation])]\n",
    "    elif mode == 'domain':\n",
    "        return [domain]\n",
    "    else:\n",
    "        return [relation]\n",
    "\n",
    "\n",
    "class ModelFactory:\n",
    "    \n",
    "    def __init__(self, input_shape, image_shape=None):\n",
    "        self.input_shape = input_shape\n",
    "        self.image_shape = image_shape if image_shape else input_shape\n",
    "        self._layer_name_fmt = \"{}_attribute_{}\"\n",
    "        \n",
    "        self._log = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def create_features(self, model_inputs, models, **kwargs):\n",
    "        freeze_attributes = kwargs.get('freeze_attributes', False)\n",
    "        features_layer_idx = kwargs.get('features_layer_idx', None)\n",
    "        \n",
    "        self._log.debug('Creating model from inputs {}'.format(model_inputs))\n",
    "\n",
    "        # collect model inputs and crop them if needed\n",
    "        inputs, cropped_inputs = self._get_inputs(model_inputs)\n",
    "\n",
    "        # update layer name and freeze weights if neccesary\n",
    "        for model_set in models.values():\n",
    "            for attr_name, model in model_set.items():\n",
    "                self._update_model(model, attr_name, \n",
    "                                   freeze_weights=freeze_attributes)\n",
    "\n",
    "        # collect features from all attribute models\n",
    "        feature_outputs = self._get_features(\n",
    "            list(zip(model_inputs, cropped_inputs)), models,\n",
    "            features_layer_idx=features_layer_idx\n",
    "        )\n",
    "        \n",
    "        return inputs, feature_outputs\n",
    "\n",
    "    def create_model(self, inputs, fused_features, version='top_down', **kwargs):        \n",
    "\n",
    "        if version == 'top_down':\n",
    "            model_function = create_model_top_down\n",
    "        elif version == 'bottom_up':\n",
    "            model_function = create_model_bottom_up\n",
    "        else:\n",
    "            self._log.error('Valid models are {top_down, bottom_up}.')\n",
    "            assert version == 'top_down' or version == 'top_down'\n",
    "            \n",
    "        model = Model(\n",
    "            inputs=inputs,\n",
    "            outputs=model_function(fused_features, **kwargs)\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _get_inputs(self, model_inputs):        \n",
    "        # cropping dimensions\n",
    "        diff_shape = np.array(self.image_shape[:2]) - np.array(self.input_shape[:2])\n",
    "        crop_size = tuple(map(int, diff_shape / 2.0))\n",
    "        assert crop_size[0] >= 0 and crop_size[1] >= 0\n",
    "\n",
    "        # collect model inputs and crop them if needed\n",
    "        inputs, cropped_inputs = [], []\n",
    "        for input_name in model_inputs:\n",
    "            _input = Input(shape=self.image_shape, name=input_name)   \n",
    "            # crop image if neccesary\n",
    "            if self.input_shape != self.image_shape:\n",
    "                cropped_input = Cropping2D(\n",
    "                    crop_size, name=\"cropped_%s\" % input_name\n",
    "                )(_input)\n",
    "            else:\n",
    "                cropped_input = _input\n",
    "\n",
    "            inputs.append(_input)\n",
    "            cropped_inputs.append(cropped_input)\n",
    "            \n",
    "        return inputs, cropped_inputs\n",
    "    \n",
    "    def _get_features(self, named_inputs, models, features_layer_idx=None):\n",
    "        \n",
    "        # includes features from all attributes\n",
    "        feature_outputs = []\n",
    "        # get image types that are grouped together (e.g. body, face, context)\n",
    "        im_types = models.keys()\n",
    "        for im_type in im_types:           \n",
    "            fused_attrs = self._get_fused_model(\n",
    "                models[im_type], \n",
    "                features_layer_idx=features_layer_idx, \n",
    "                name='features_{}'.format(im_type)\n",
    "            )\n",
    "            \n",
    "            # collect features from a given image type\n",
    "            for input_name, _input in named_inputs:\n",
    "                if im_type in input_name:\n",
    "                    self._log.debug('Computing features for input: {}'.format(input_name))\n",
    "\n",
    "                    attr_input = [_input for _ in fused_attrs.inputs]\n",
    "                    attr_output = fused_attrs(attr_input)\n",
    "                    # when using just one attribute, the output is not a list\n",
    "                    if not isinstance(attr_output, list):\n",
    "                        attr_output = [attr_output]\n",
    "\n",
    "                    feature_outputs.extend(attr_output)\n",
    "\n",
    "        return feature_outputs\n",
    "\n",
    "    def fuse_features(self, feature_outputs, mode='symmetric', pooling=None):\n",
    "        fused_features = None\n",
    "\n",
    "        self._log.debug('Fusing {} outputs'.format(len(feature_outputs)))\n",
    "        \n",
    "        if mode == 'symmetric':\n",
    "            # concatenate features\n",
    "            fused_features = keras.layers.concatenate(feature_outputs)\n",
    "        else:\n",
    "            self._log.error('Valid fusion modes are {symmetric}.')\n",
    "            assert mode == 'symmetric'\n",
    "        \n",
    "        if pooling == 'max':\n",
    "            # feature downsampling\n",
    "            fused_features = GlobalMaxPooling2D()(fused_features)\n",
    "        elif pooling is None:\n",
    "            fused_features =  Flatten()(fused_features)\n",
    "        else:\n",
    "            self._log.error('Valid pooling modes are {None, max}.')\n",
    "            assert pooling is None or pooling == 'max'\n",
    "            \n",
    "        return fused_features\n",
    "        \n",
    "    \n",
    "    def _get_fused_model(self, image_type_models, \n",
    "                         features_layer_idx=None, \n",
    "                         name=None):\n",
    "        # includes inputs and outputs for a given type\n",
    "        attr_inputs, attr_outputs = [], []\n",
    "        for attr_name, attr_model in image_type_models.items():            \n",
    "            # collect attribute inputs\n",
    "            attr_inputs.append(attr_model.input)\n",
    "            # collect attribute outputs\n",
    "            if features_layer_idx is None:\n",
    "                # last layer\n",
    "                features_layer_idx = -1\n",
    "\n",
    "            layer = attr_model.layers[features_layer_idx]\n",
    "\n",
    "            self._log.debug('Collection feature tensors from {} layer {}'.format(attr_name, layer.name))\n",
    "            \n",
    "            output = layer.output\n",
    "            attr_outputs.append(output)\n",
    "\n",
    "        self._log.debug('Found {} inputs'.format(len(attr_inputs)))\n",
    "        self._log.debug('Found {} outputs'.format(len(attr_outputs)))\n",
    "        \n",
    "        # model that compute features from a given image type\n",
    "        fused_attrs = Model(inputs=attr_inputs, outputs=attr_outputs, name=name)\n",
    "        return fused_attrs\n",
    "        \n",
    "    def _update_model(self, model, attr_name, freeze_weights=False):\n",
    "        for layer in model.layers:\n",
    "            # freeze inherited attribute weights\n",
    "            layer.trainable = not freeze_weights\n",
    "            # rename layers if neccesary\n",
    "            if attr_name not in layer.name:\n",
    "                layer.name = self._layer_name_fmt.format(layer.name, attr_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake call to main to process inputs arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\n",
    "    \"--project_dir\", \"/home/shared/Documents/final_proj\",\n",
    "    \"--save_stats\",\n",
    "    \"--save_model\",\n",
    "    \"--epochs\", \"30\",\n",
    "    \"--batch_size\", \"64\",\n",
    "    \"--lr\", \"0.001\",\n",
    "]\n",
    "\n",
    "conf = main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakedir = FakeDirectory(conf.FAKE_DIR, conf.SPLITS_DIR)\n",
    "fakedir.create_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = conf.BATCH_SIZE\n",
    "\n",
    "output_mode = 'both_fused' # domain-relation outputs fused\n",
    "#output_mode = 'both_splitted' # multi-loss domain-relation\n",
    "#output_mode = 'domain' # domain only\n",
    "#output_mode = 'relation' # relation only\n",
    "\n",
    "# for AlexNet\n",
    "preprocessing_function = imagenet_utils.preprocess_input\n",
    "\n",
    "helper = SocialClassifier(conf.FAKE_DIR)\n",
    "helper.setup_datagen(\n",
    "    batch_size=batch_size,\n",
    "    seed=SHARED_SEED,\n",
    "    classes=RELATION_LABELS,\n",
    "    target_size=IMAGE_SHAPE[0:2],\n",
    "    fix_path_cbk=fakedir.fix_fake_link,\n",
    "    preprocessing_function=preprocessing_function,\n",
    "    output_mode=output_mode,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size of each split, output from the data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_validation, n_test = 13729, 709, 5106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_reuse_model = conf.REUSE_MODEL\n",
    "do_create_model = not do_reuse_model\n",
    "\n",
    "# type of model\n",
    "# model_version = 'bottom_up'\n",
    "model_version='top_down'\n",
    "# initial epoch\n",
    "cache_epochs = 0\n",
    "\n",
    "if do_reuse_model:\n",
    "    checkpoint_path = os.path.join(egosocial.config.MODELS_CACHE_DIR, 'training',\n",
    "                                   'finetuned_weights.{epoch:02d}-{val_loss:.2f}.h5')\n",
    "    # set parameters from disk\n",
    "    cache_epochs, cache_val_loss = None, None\n",
    "    model_file = checkpoint_path.format(epoch=cache_epochs, val_loss=cache_val_loss)\n",
    "\n",
    "    # FIXME: doesn't work because the custom loss is an unknown object\n",
    "    # workaround: save only the weights\n",
    "    if os.path.exists(model_file):\n",
    "        pass\n",
    "    else:\n",
    "        do_create_model = True\n",
    "        # start from scratch\n",
    "        cache_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_create_model:\n",
    "    # TODO: split submodels in definition and weights\n",
    "    # load submodels\n",
    "    selector = AttributeSelector(egosocial.config.MODEL_KEYS)\n",
    "    attribute_models = dict(\n",
    "        body=get_models(\n",
    "            selector.filter('body'), \n",
    "            conf.MODELS_CACHE_DIR, \n",
    "            conf.MODELS_CACHE_SUBDIR,\n",
    "        ), \n",
    "        face=get_models(\n",
    "            selector.filter('face'), \n",
    "            conf.MODELS_CACHE_DIR, \n",
    "            conf.MODELS_CACHE_SUBDIR,\n",
    "        ),\n",
    "    )    \n",
    "    \n",
    "    # use layer index because layers need to be rename when the \n",
    "    # ensemble of models is created\n",
    "    pool5_idx = -10\n",
    "    \n",
    "    model_factory = ModelFactory(INPUT_SHAPE, IMAGE_SHAPE)\n",
    "    inputs, feature_outputs = model_factory.create_features(\n",
    "        MODEL_INPUTS, attribute_models,\n",
    "        freeze_attributes=True,\n",
    "        features_layer_idx=pool5_idx,\n",
    "    )\n",
    "    fused_features = model_factory.fuse_features(\n",
    "        feature_outputs, \n",
    "        pooling='max'\n",
    "    )\n",
    "    model = model_factory.create_model(\n",
    "        inputs, fused_features, \n",
    "        mode=output_mode,\n",
    "        version=model_version,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = conf.LR\n",
    "\n",
    "if output_mode == 'both_fused':\n",
    "    # we can't use crossentropy because the output\n",
    "    # is not strictly softmax, but the concatenation\n",
    "    # of domain and relation outputs (both softmax)\n",
    "    loss = 'mse'\n",
    "else:\n",
    "    loss = 'categorical_crossentropy'\n",
    "    \n",
    "helper.set_model(\n",
    "    model,\n",
    "    optimizer=keras.optimizers.Adam(learning_rate, decay=1e-6),\n",
    "    metrics=['accuracy'], loss=loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "\n",
    "if conf.SAVE_MODEL:\n",
    "    checkpoint_path = os.path.join(egosocial.config.MODELS_CACHE_DIR, 'training',\n",
    "                                   'finetuned_weights.{epoch:02d}-{val_loss:.2f}.h5')\n",
    "    checkpointer = ModelCheckpoint( \n",
    "        filepath=checkpoint_path, monitor='val_loss',\n",
    "        save_best_only=True, period=5,\n",
    "    )\n",
    "    callbacks.append(checkpointer)\n",
    "\n",
    "if conf.SAVE_STATS:\n",
    "    metrics_path = os.path.join(egosocial.config.MODELS_CACHE_DIR, 'training',\n",
    "                                'metrics.csv')\n",
    "    csv_logger = CSVLogger(metrics_path)\n",
    "    callbacks.append(csv_logger)\n",
    "        \n",
    "lr_handler = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001\n",
    ")\n",
    "\n",
    "# more plots need more space\n",
    "if output_mode != 'both_splitted':\n",
    "    figsize = (20, 5)\n",
    "else:\n",
    "    figsize = (20, 13)\n",
    "\n",
    "plot_metrics = PlotLearning(update_step=1, figsize=figsize)\n",
    "\n",
    "callbacks.extend([\n",
    "    lr_handler,\n",
    "    plot_metrics,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = conf.EPOCHS\n",
    "\n",
    "hist = helper.fit(\n",
    "    steps_per_epoch=np.ceil(1.0 * n_train / batch_size),    \n",
    "    validation_steps=np.ceil(1.0 * n_validation / batch_size),\n",
    "    epochs=cache_epochs + epochs,\n",
    "    initial_epoch=cache_epochs,\n",
    "    callbacks=callbacks,\n",
    "    use_multiprocessing=False,\n",
    "    workers=2,\n",
    "    max_queue_size=5,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = helper.evaluate(\n",
    "    steps=np.ceil(1.0 * n_test / batch_size)\n",
    ")\n",
    "\n",
    "for score, metric_name in zip(scores, helper.model.metrics_names):\n",
    "    helper._log.info(\"{} : {:0.4}\".format(metric_name, score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
