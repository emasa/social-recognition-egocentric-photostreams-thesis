{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Constants Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import threading\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# add egosocial to the python path\n",
    "from os.path import dirname, abspath\n",
    "sys.path.extend([dirname(abspath('.'))])\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers.noise import AlphaDropout\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "import egosocial.config\n",
    "from egosocial.core.types import relation_to_domain, relation_to_domain_vec\n",
    "from egosocial.utils.keras.autolosses import AutoMultiLossWrapper\n",
    "from egosocial.utils.logging import setup_logging\n",
    "from egosocial.utils.keras.callbacks import PlotLearning\n",
    "from egosocial.utils.keras.backend import limit_gpu_allocation_tensorflow\n",
    "from egosocial.utils.filesystem import create_directory, check_directory\n",
    "\n",
    "# constants\n",
    "DOMAIN, RELATION = 'domain', 'relation'\n",
    "END_TO_END, ATTRIBUTES = 'end_to_end', 'attributes'\n",
    "N_CLS_RELATION, N_CLS_DOMAIN = 16, 5\n",
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "INPUT_SHAPE = (227, 227, 3)\n",
    "SHARED_SEED = 1\n",
    "STREAM_IDS = [str(idx) for idx in [1, 2]]\n",
    "\n",
    "RELATION_LABELS = [str(label) for label in range(N_CLS_RELATION)]\n",
    "DEPRECATED_HOME = '/root'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit GPU memory allocation with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_gpu_allocation_tensorflow(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input arguments and fake main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_FORMAT = '{prefix}{dtype}{idx}_{split}_{n_cls}{ext}'\n",
    "\n",
    "# TODO: move to utils\n",
    "def get_file(split, idx=1, dtype='body', n_cls=N_CLS_RELATION, \n",
    "             ext='.txt', prefix='single_'):\n",
    "    return FILE_FORMAT.format(split=split, prefix=prefix, dtype=dtype, \n",
    "                              idx=idx, n_cls=n_cls, ext=ext)\n",
    "\n",
    "class Configuration:\n",
    "    def __init__(self, args):\n",
    "        self.DATA_TYPE = RELATION\n",
    "        self.ARCH = 'caffeNet'\n",
    "        self.LAYER = 'fc7'\n",
    "\n",
    "        self.CONFIG = '{}_{}_{}'.format(self.LAYER, self.DATA_TYPE, self.ARCH)\n",
    "\n",
    "        # setup directories\n",
    "        self.PROJECT_DIR = args.project_dir\n",
    "        self.BASE_MODELS_DIR = os.path.join(self.PROJECT_DIR,\n",
    "                                            'models/trained_models')\n",
    "        self.ATTR_MODELS_DIR = os.path.join(self.BASE_MODELS_DIR,\n",
    "                                            'attribute_models')\n",
    "        self.SVM_MODELS_DIR = os.path.join(self.PROJECT_DIR,\n",
    "                                           'models/svm_models')\n",
    "\n",
    "        self.SPLITS_DIR = os.path.join(self.PROJECT_DIR,\n",
    "                                       'datasets/splits/annotator_consistency3')\n",
    "\n",
    "        self.STATS_MODELS_DIR = os.path.join(self.SVM_MODELS_DIR, 'stats')\n",
    "\n",
    "        # splits (switch from caffe's split name convention to keras's convention)        \n",
    "        _train, _val, _test = 'train', 'test', 'eval'        \n",
    "        self.LABEL_FILES = {split: os.path.join(self.SPLITS_DIR, get_file(split=split))\n",
    "                            for split in (_train, _val, _test)}\n",
    "\n",
    "        self.IS_END2END = False\n",
    "\n",
    "        self.BASE_FEATURES_DIR = os.path.join(self.PROJECT_DIR,\n",
    "                                              'extracted_features')\n",
    "        self.FEATURES_DIR = os.path.join(self.BASE_FEATURES_DIR,\n",
    "                                         'attribute_features',\n",
    "                                         self.CONFIG)\n",
    "\n",
    "        self.STORED_FEATURES_DIR = os.path.join(self.FEATURES_DIR,\n",
    "                                                'all_splits_numpy_format')\n",
    "\n",
    "        self.PROCESS_FEATURES = args.port_features\n",
    "\n",
    "        self.EPOCHS = args.epochs\n",
    "        self.BATCH_SIZE = args.batch_size\n",
    "\n",
    "        # reuse precomputed model?\n",
    "        self.REUSE_MODEL = args.reuse_model\n",
    "        # save model to disk?\n",
    "        self.SAVE_MODEL = args.save_model\n",
    "        # save model statistics to disk?\n",
    "        self.SAVE_STATS = args.save_stats\n",
    "        \n",
    "        self.FAKE_DIR = os.path.join(self.PROJECT_DIR, 'datasets', 'fake_dir')\n",
    "        \n",
    "def positive_int(value):\n",
    "    ivalue = int(value)\n",
    "    if ivalue <= 0:\n",
    "        raise argparse.ArgumentTypeError(\n",
    "            \"%s is an invalid positive int value\" % value)\n",
    "    return ivalue\n",
    "\n",
    "def main(*fake_args):\n",
    "    setup_logging(egosocial.config.LOGGING_CONFIG)\n",
    "\n",
    "    entry_msg = 'Reproduce experiments in Social Relation Recognition paper.'\n",
    "    parser = argparse.ArgumentParser(description=entry_msg)\n",
    "\n",
    "    parser.add_argument('--project_dir', required=True,\n",
    "                        help='Base directory.')\n",
    "\n",
    "    parser.add_argument('--port_features', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Whether port features from other formats to'\n",
    "                             'numpy.')\n",
    "\n",
    "    parser.add_argument('--reuse_model', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Use precomputed model if available.')\n",
    "\n",
    "    parser.add_argument('--save_model', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Save model to disk.')\n",
    "\n",
    "    parser.add_argument('--save_stats', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Save statistics to disk.')\n",
    "\n",
    "    parser.add_argument('--epochs', required=False, type=positive_int,\n",
    "                        default=100,\n",
    "                        help='Max number of epochs.')\n",
    "\n",
    "    parser.add_argument('--batch_size', required=False, type=positive_int,\n",
    "                        default=32,\n",
    "                        help='Batch size.')\n",
    "\n",
    "    # TODO: implement correctly\n",
    "    args = parser.parse_args(*fake_args)\n",
    "    # keep configuration\n",
    "    conf = Configuration(args)\n",
    "    # check directories\n",
    "    check_directory(conf.PROJECT_DIR, 'Project')\n",
    "    check_directory(conf.SPLITS_DIR, 'Splits')\n",
    "    \n",
    "    return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line(line):\n",
    "    image_path, label = line.strip().split(' ')\n",
    "    image_path = image_path.replace(DEPRECATED_HOME, '~')\n",
    "\n",
    "    if '~' in image_path:\n",
    "        image_path = os.path.expanduser(image_path)\n",
    "\n",
    "    return image_path, label\n",
    "\n",
    "def get_split_mapping(files_dir, file_callback=get_file):\n",
    "    # splits (switch from caffe's split name convention to keras's convention)\n",
    "    _train, _val, _test = 'train', 'test', 'eval'\n",
    "    \n",
    "    check_directory(files_dir, 'Splits')\n",
    "    \n",
    "    mapping = {}\n",
    "    for split in (_train, _val, _test):\n",
    "        for idx in STREAM_IDS:\n",
    "            key = '{idx}{sep}{split}'.format(split=split, sep=os.path.sep, idx=idx)\n",
    "            value =  os.path.join(files_dir, file_callback(split=split, idx=idx))\n",
    "            mapping[key] = value\n",
    "\n",
    "    return mapping\n",
    "\n",
    "def create_fake_directory(fake_dir, splits_dir, labels):\n",
    "\n",
    "    if os.path.isdir(fake_dir):\n",
    "        shutil.rmtree(fake_dir)\n",
    "    \n",
    "    create_directory(fake_dir, 'Fake')\n",
    "    \n",
    "    for dtype in ('body', 'face'):\n",
    "        dtype_dir = os.path.join(fake_dir, dtype)\n",
    "        create_directory(dtype_dir, 'Fake {}'.format(dtype))\n",
    "        \n",
    "        # set dtype to body or face\n",
    "        file_callback = lambda **kwargs: get_file(dtype=dtype, **kwargs)      \n",
    "\n",
    "        split_mapping = get_split_mapping(splits_dir, file_callback=file_callback)\n",
    "        for images_dir, images_file in split_mapping.items():\n",
    "            fake_images_dir = os.path.join(dtype_dir, images_dir)\n",
    "\n",
    "            # keras asks a different directory for each class\n",
    "            for label in labels:\n",
    "                fake_label_dir = os.path.join(fake_images_dir, str(label))\n",
    "                create_directory(fake_label_dir, 'Label')\n",
    "\n",
    "            images_file_path = os.path.join(splits_dir, images_dir, images_file)\n",
    "            with open(images_file_path) as f:\n",
    "                for fake_id, line in enumerate(f):\n",
    "                    image_path, label = process_line(line)\n",
    "                    # create symlinks for every entry\n",
    "                    # a name may appear in several entries, so an unique fake name is created\n",
    "                    fake_name = 'fakelink{}_{}'.format(fake_id, os.path.basename(image_path))\n",
    "                    fake_image_path = os.path.join(fake_images_dir, label, fake_name)\n",
    "                    if not os.path.exists(fake_image_path):\n",
    "                        os.symlink(image_path, fake_image_path)\n",
    "\n",
    "# TODO: move to utils\n",
    "class threadsafe_iter(object):\n",
    "    \"\"\"Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return next(self.it)\n",
    "\n",
    "# TODO: move to utils\n",
    "def threadsafe_generator(f):\n",
    "    \"\"\"A decorator that takes a generator function and makes it thread-safe.\n",
    "    \"\"\"\n",
    "    def g(*a, **kw):\n",
    "        return threadsafe_iter(f(*a, **kw))\n",
    "    return g\n",
    "\n",
    "# TODO: move to utils\n",
    "@threadsafe_generator\n",
    "def fuse_inputs_generator(generators, inputs, outputs_callback=None): \n",
    "    assert len(generators) == len(inputs)\n",
    "    \n",
    "    # assume single output (shared by all inputs)\n",
    "    if outputs_callback is None: # indices 0: first input, 1: label data\n",
    "        outputs_callback = lambda batch, _inputs: batch[0][1]\n",
    "    \n",
    "    while True:\n",
    "        data_batch = [next(gen) for gen in generators]\n",
    "        # multiple inputs\n",
    "        X = {input_name : data_batch[idx][0] for idx, input_name in enumerate(inputs)}        \n",
    "\n",
    "        yield X, outputs_callback(data_batch, inputs)\n",
    "\n",
    "# TODO: move to utils\n",
    "def flow_from_dirs(input_directories, **kwargs):\n",
    "    gen_args = kwargs.pop('gen_args', None)\n",
    "    gen_args = gen_args if gen_args else {}\n",
    "\n",
    "    flow_gens = []\n",
    "    for directory in input_directories:\n",
    "        check_directory(directory)\n",
    "        datagen = ImageDataGenerator(**gen_args)\n",
    "        flow_gen = datagen.flow_from_directory(\n",
    "            directory, \n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        flow_gens.append(flow_gen)\n",
    "        \n",
    "    return flow_gens\n",
    "\n",
    "# TODO: move to utils\n",
    "def get_relation_domain(data_batch, inputs):\n",
    "    # relation output (every attribute has the same output)\n",
    "    y_rel = data_batch[0][1] # indices 0: first input, 1: label data\n",
    "    # inverse of function to_categorical\n",
    "    plain_y_rel = np.argmax(y_rel, axis=1)\n",
    "    # final output is domain, relation\n",
    "    y_dom = to_categorical(relation_to_domain_vec(plain_y_rel), N_CLS_DOMAIN)\n",
    "    \n",
    "    return dict(domain=y_dom, relation=y_rel)\n",
    "\n",
    "def create_data_split_generators(directory, \n",
    "                                 train_gen_args=None, \n",
    "                                 test_val_gen_args=None,\n",
    "                                 **kwargs):\n",
    "    check_directory(directory)\n",
    "    \n",
    "    inputs = ['body/1', 'body/2', 'face/1', 'face/2']\n",
    "    def input_dirs(split):\n",
    "        return [os.path.join(directory, input_name, split) \n",
    "                for input_name in inputs]\n",
    "\n",
    "    # splits (switch from caffe's split name convention to keras's convention)    \n",
    "    _train, _val, _test = 'train', 'test', 'eval'\n",
    "        \n",
    "    split_gen_args = [ (_train, train_gen_args)\n",
    "                     , (_val, test_val_gen_args)\n",
    "                     , (_test, test_val_gen_args)]\n",
    "\n",
    "    split_generators = []\n",
    "    for split, gen_args in split_gen_args:\n",
    "        gens = flow_from_dirs(\n",
    "            input_dirs(split), \n",
    "            gen_args=gen_args,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        fused_gen = fuse_inputs_generator(\n",
    "            gens, inputs, \n",
    "            outputs_callback=get_relation_domain\n",
    "        )\n",
    "        \n",
    "        split_generators.append(fused_gen)\n",
    "        \n",
    "    return split_generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialClassifier:\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self._data_dir = data_dir\n",
    "\n",
    "        self._log = logging.getLogger(self.__class__.__name__)\n",
    "        \n",
    "        # initialize when model is configured\n",
    "        self._model_wrapper = None\n",
    "        self.model = None\n",
    "        \n",
    "        self._train_gen, self._val_gen, self._test_gen = None, None, None\n",
    "    \n",
    "    def setup_datagen(self, **kwargs):\n",
    "        self._log.debug(\"Creating data generators.\")        \n",
    "\n",
    "        train_gen_args = dict(\n",
    "            rescale=1./255,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            preprocessing_function=preprocess_input,\n",
    "        )\n",
    "        test_val_gen_args = dict(\n",
    "            rescale=1./255,\n",
    "            preprocessing_function=preprocess_input,\n",
    "        )\n",
    "        \n",
    "        split_gens = create_data_split_generators(\n",
    "            self._data_dir, train_gen_args, test_val_gen_args,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        self._train_gen, self._val_gen, self._test_gen = split_gens\n",
    "        \n",
    "    def set_model(self, model, optimizer='adam', \n",
    "                  loss='categorical_crossentropy', \n",
    "                  loss_weights='auto',\n",
    "                  **kwargs):\n",
    "        assert model\n",
    "        self._log.debug(\"Initializing model.\")\n",
    "        \n",
    "        # wrapper allows to train the loss weights\n",
    "        self._model_wrapper = AutoMultiLossWrapper(model)\n",
    "        self._model_wrapper.compile(optimizer, loss, \n",
    "                                    loss_weights=loss_weights, \n",
    "                                    **kwargs)\n",
    "\n",
    "        self.model = self._model_wrapper.model\n",
    "        self._log.info(self.model.summary())\n",
    "        \n",
    "    def fit(self, steps_per_epoch, validation_steps, **kwargs):\n",
    "        assert self.model\n",
    "        assert self._train_gen\n",
    "        assert self._val_gen\n",
    "        self._log.debug(\"Training model from scratch.\")        \n",
    "\n",
    "        # train the model on the new data for a few epochs\n",
    "        return self.model.fit_generator(\n",
    "            self._train_gen,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=self._val_gen,\n",
    "            validation_steps=validation_data,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "    def evaluate(steps, **kwargs):\n",
    "        assert self.model\n",
    "        assert self._test_gen\n",
    "        self._log.debug(\"Evaluating model in test data.\")\n",
    "        \n",
    "        return self.model.evaluate_generator(\n",
    "            self._test_gen,\n",
    "            steps=steps,\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake call to main to process inputs arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\n",
    "    \"--project_dir\", \"/home/shared/Documents/final_proj\",\n",
    "    \"--epochs\", \"30\",\n",
    "    \"--batch_size\", \"32\",\n",
    "]\n",
    "\n",
    "conf = main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_fake_directory(conf.FAKE_DIR, conf.SPLITS_DIR, RELATION_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper = SocialClassifier(conf.FAKE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = conf.BATCH_SIZE\n",
    "\n",
    "helper.setup_datagen(\n",
    "    batch_size=batch_size,\n",
    "    seed=SHARED_SEED,\n",
    "    classes=RELATION_LABELS,\n",
    "    target_size=INPUT_SHAPE[0:2],\n",
    "    follow_links=True, # fake directory uses simlinks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, target_size):\n",
    "    return None\n",
    "\n",
    "learning_rate = 0.0001\n",
    "helper.set_model(\n",
    "    create_model(INPUT_SHAPE, IMAGE_SHAPE),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate, decay=1e-6),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of each split, output from the data generator\n",
    "n_train, n_validation, n_test = 13729, 709, 5106\n",
    "\n",
    "epochs = conf.EPOCHS\n",
    "\n",
    "checkpoint_path = os.path.join(egosocial.config.MODELS_CACHE_DIR, 'training',\n",
    "                               'finetuned_weights.{epoch:02d}-{val_loss:.2f}.h5')\n",
    "checkpointer = ModelCheckpoint( \n",
    "    filepath=checkpoint_path, monitor='val_loss',\n",
    "    save_best_only=True, period=1,\n",
    ")\n",
    "\n",
    "lr_handler = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=10, min_lr=0.00001\n",
    ")\n",
    "\n",
    "# TODO: implement csv logs\n",
    "#csv_logger = CSVLogger(filename)\n",
    "\n",
    "# if plot is enabled, set verbose=0\n",
    "plot_metrics = PlotLearning(update_step=1)\n",
    "\n",
    "callbacks = [\n",
    "#    checkpointer,\n",
    "#    lr_handler, \n",
    "    plot_metrics\n",
    "]\n",
    "\n",
    "hist = helper.fit(\n",
    "    steps_per_epoch=np.ceil(1.0 * n_train / batch_size),    \n",
    "    validation_steps=np.ceil(1.0 * n_validation / batch_size),\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    workers=4,\n",
    "    max_queue_size=5,\n",
    "    verbose=0,     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = helper.evaluate( \n",
    "    steps=np.ceil(1.0 * n_test / batch_size),\n",
    "    workers=4,\n",
    "    max_queue_size=5,\n",
    ")\n",
    "\n",
    "for score, metric_name in zip(scores, helper.model.metrics_names):\n",
    "    helper._log.info(\"{} : {0.4f}\".format(metric_name, score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
