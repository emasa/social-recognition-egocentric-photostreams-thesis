{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_notebook(fix_python_path=True, reduce_margins=True, plot_inline=True):\n",
    "    if reduce_margins:\n",
    "        # Reduce side margins of the notebook\n",
    "        from IPython.core.display import display, HTML\n",
    "        display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "    if fix_python_path:\n",
    "        # add egosocial to the python path\n",
    "        import os, sys\n",
    "        sys.path.extend([os.path.dirname(os.path.abspath('.'))])\n",
    "\n",
    "    if plot_inline:\n",
    "        # Plots inside cells\n",
    "        %matplotlib inline\n",
    "\n",
    "setup_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Constants Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import threading\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.layers import Input, Dense, Dropout, Cropping2D, GlobalMaxPooling2D\n",
    "from keras.layers.noise import AlphaDropout\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import egosocial\n",
    "from egosocial.core.types import relation_to_domain, relation_to_domain_vec\n",
    "from egosocial.utils.filesystem import create_directory, check_directory\n",
    "from egosocial.utils.logging import setup_logging\n",
    "from egosocial.utils.keras.autolosses import AutoMultiLossWrapper, AutoMultiLossLayer\n",
    "from egosocial.utils.keras.backend import limit_gpu_allocation_tensorflow\n",
    "from egosocial.utils.keras.callbacks import PlotLearning\n",
    "from egosocial.utils.keras.utils import flow_from_dirs, fuse_inputs_generator\n",
    "\n",
    "# TODO: move these constants to the library\n",
    "DOMAIN, RELATION = 'domain', 'relation'\n",
    "END_TO_END, ATTRIBUTES = 'end_to_end', 'attributes'\n",
    "N_CLS_RELATION, N_CLS_DOMAIN = 16, 5\n",
    "RELATION_LABELS = [str(label) for label in range(N_CLS_RELATION)]\n",
    "FILE_FORMAT = '{prefix}{dtype}{idx}_{split}_{n_cls}{ext}'\n",
    "\n",
    "MODEL_INPUTS = ('body/1', 'body/2', 'face/1', 'face/2')\n",
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "INPUT_SHAPE = (227, 227, 3)\n",
    "SHARED_SEED = 1\n",
    "STREAM_IDS = [str(idx) for idx in [1, 2]]\n",
    "\n",
    "DEPRECATED_HOME = '/root/'\n",
    "NEW_HOME='/home/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit GPU memory allocation with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "limit_gpu_allocation_tensorflow(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input arguments and fake main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file(split, idx=1, dtype='body', n_cls=N_CLS_RELATION, \n",
    "             ext='.txt', prefix='single_'):\n",
    "    return FILE_FORMAT.format(split=split, prefix=prefix, dtype=dtype, \n",
    "                              idx=idx, n_cls=n_cls, ext=ext)\n",
    "\n",
    "class Configuration:\n",
    "    def __init__(self, args):\n",
    "        # setup directories\n",
    "        self.PROJECT_DIR = args.project_dir\n",
    "\n",
    "        self.SPLITS_DIR = os.path.join(self.PROJECT_DIR,\n",
    "                                       'datasets/splits/annotator_consistency3')\n",
    "\n",
    "        # splits (switch from caffe's split name convention to keras's convention)        \n",
    "        _train, _val, _test = 'train', 'test', 'eval'        \n",
    "        self.LABEL_FILES = {split: os.path.join(self.SPLITS_DIR, get_file(split=split))\n",
    "                            for split in (_train, _val, _test)}\n",
    "\n",
    "        self.EPOCHS = args.epochs\n",
    "        self.BATCH_SIZE = args.batch_size\n",
    "\n",
    "        # reuse precomputed model?\n",
    "        self.REUSE_MODEL = args.reuse_model\n",
    "        # save model to disk?\n",
    "        self.SAVE_MODEL = args.save_model\n",
    "        # save model statistics to disk?\n",
    "        self.SAVE_STATS = args.save_stats\n",
    "        \n",
    "        self.FAKE_DIR = os.path.join(self.PROJECT_DIR, 'datasets', 'fake_dir')\n",
    "        \n",
    "def positive_int(value):\n",
    "    ivalue = int(value)\n",
    "    if ivalue <= 0:\n",
    "        raise argparse.ArgumentTypeError(\n",
    "            \"%s is an invalid positive int value\" % value)\n",
    "    return ivalue\n",
    "\n",
    "def main(*fake_args):\n",
    "    setup_logging(egosocial.config.LOGGING_CONFIG)\n",
    "\n",
    "    entry_msg = 'Reproduce experiments in Social Relation Recognition paper.'\n",
    "    parser = argparse.ArgumentParser(description=entry_msg)\n",
    "\n",
    "    parser.add_argument('--project_dir', required=True,\n",
    "                        help='Base directory.')\n",
    "\n",
    "    parser.add_argument('--reuse_model', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Use precomputed model if available.')\n",
    "\n",
    "    parser.add_argument('--save_model', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Save model to disk.')\n",
    "\n",
    "    parser.add_argument('--save_stats', required=False,\n",
    "                        action='store_true',\n",
    "                        help='Save statistics to disk.')\n",
    "\n",
    "    parser.add_argument('--epochs', required=False, type=positive_int,\n",
    "                        default=100,\n",
    "                        help='Max number of epochs.')\n",
    "\n",
    "    parser.add_argument('--batch_size', required=False, type=positive_int,\n",
    "                        default=32,\n",
    "                        help='Batch size.')\n",
    "\n",
    "    # TODO: implement correctly\n",
    "    args = parser.parse_args(*fake_args)\n",
    "    # keep configuration\n",
    "    conf = Configuration(args)\n",
    "    # check directories\n",
    "    check_directory(conf.PROJECT_DIR, 'Project')\n",
    "    check_directory(conf.SPLITS_DIR, 'Splits')\n",
    "    \n",
    "    return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move to utils\n",
    "class AttributeSelector:\n",
    "    def __init__(self, all_attrs):\n",
    "        body_attributes = self.filter_by_keyword(all_attrs, 'body')\n",
    "        face_attributes = self.filter_by_keyword(all_attrs, 'face')\n",
    "        face_attributes.extend(self.filter_by_keyword(all_attrs, 'head'))\n",
    "\n",
    "        self._selector = {'all': all_attrs,\n",
    "                          'body': body_attributes,\n",
    "                          'face': face_attributes}\n",
    "\n",
    "    def filter(self, query):\n",
    "        query = query.lower()\n",
    "        if query in self._selector:\n",
    "            selected_attributes = self._selector[query]\n",
    "        else:\n",
    "            selected_attributes = self.filter_by_keyword(self._selector['all'],\n",
    "                                                         query)\n",
    "\n",
    "        return selected_attributes\n",
    "\n",
    "    def filter_by_keyword(self, attribute_list, key):\n",
    "        return [attr_name for attr_name in attribute_list if key in attr_name]\n",
    "\n",
    "\n",
    "# TODO: move to utils\n",
    "def get_relation_domain(data_batch, inputs):\n",
    "    # relation output (every attribute has the same output)\n",
    "    y_rel = data_batch[0][1] # indices 0: first input, 1: label data\n",
    "    # inverse of function to_categorical\n",
    "    plain_y_rel = np.argmax(y_rel, axis=1)\n",
    "    # final output is domain, relation\n",
    "    y_dom = to_categorical(relation_to_domain_vec(plain_y_rel), N_CLS_DOMAIN)\n",
    "    \n",
    "    return dict(domain=y_dom, relation=y_rel)\n",
    "\n",
    "# TODO: move to utils\n",
    "def create_data_split_generators(directory, \n",
    "                                 train_gen_args=None, \n",
    "                                 test_val_gen_args=None,\n",
    "                                 fix_path_cbk=None,\n",
    "                                 **kwargs):\n",
    "    check_directory(directory)\n",
    "    \n",
    "    inputs = list(MODEL_INPUTS)\n",
    "    def input_dirs(split):\n",
    "        return [os.path.join(directory, input_name, split) \n",
    "                for input_name in inputs]\n",
    "\n",
    "    # splits (switch from caffe's split name convention to keras's convention)    \n",
    "    _train, _val, _test = 'train', 'test', 'eval'\n",
    "        \n",
    "    split_gen_args = [ (_train, train_gen_args)\n",
    "                     , (_val, test_val_gen_args)\n",
    "                     , (_test, test_val_gen_args)]\n",
    "\n",
    "    split_generators = []\n",
    "    for split, gen_args in split_gen_args:\n",
    "        gens = flow_from_dirs(\n",
    "            input_dirs(split), \n",
    "            gen_args=gen_args,\n",
    "            fix_path_cbk=fix_path_cbk,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        fused_gen = fuse_inputs_generator(\n",
    "            gens, inputs, \n",
    "            outputs_callback=get_relation_domain\n",
    "        )\n",
    "        \n",
    "        split_generators.append(fused_gen)\n",
    "        \n",
    "    return split_generators\n",
    "\n",
    "def process_line(line):\n",
    "    image_path, label = line.strip().rsplit(' ', 1)\n",
    "\n",
    "    if image_path.startswith(DEPRECATED_HOME):\n",
    "        image_path = os.path.join(NEW_HOME, image_path[len(DEPRECATED_HOME):])\n",
    "\n",
    "    return image_path, label\n",
    "\n",
    "class FakeDirectory:\n",
    "\n",
    "    def __init__(self, fake_dir, splits_dir):\n",
    "        self.fake_dir = fake_dir\n",
    "        self.splits_dir = splits_dir \n",
    "                \n",
    "        self._fake_link_fmt = 'fakelink{}_{}'\n",
    "        self._file_map = {}\n",
    "        \n",
    "    def create_structure(self):\n",
    "        \n",
    "        self._file_map = {}\n",
    "        \n",
    "        if os.path.isdir(self.fake_dir):\n",
    "            shutil.rmtree(self.fake_dir)\n",
    "\n",
    "        create_directory(self.fake_dir, 'Fake')\n",
    "\n",
    "        for dtype in ('body', 'face'):\n",
    "            dtype_dir = os.path.join(self.fake_dir, dtype)\n",
    "            create_directory(dtype_dir, 'Fake {}'.format(dtype))\n",
    "\n",
    "            # set dtype to body or face\n",
    "            file_callback = lambda **kwargs: get_file(dtype=dtype, **kwargs)      \n",
    "\n",
    "            mapping = self._get_split_mapping(file_callback=file_callback)\n",
    "\n",
    "            for images_dir, images_file in mapping.items():\n",
    "                fake_images_dir = os.path.join(dtype_dir, images_dir)\n",
    "\n",
    "                images_file_path = os.path.join(self.splits_dir, images_dir, images_file)\n",
    "                with open(images_file_path) as f:\n",
    "                    for fake_id, line in enumerate(f):\n",
    "                        image_path, label = process_line(line)\n",
    "                        \n",
    "                        fake_label_dir = os.path.join(fake_images_dir, str(label))\n",
    "\n",
    "                        # keras asks for a different directory for each class\n",
    "                        if not os.path.exists(fake_label_dir):\n",
    "                            create_directory(fake_label_dir, 'Label')                            \n",
    "                        \n",
    "                        # create symlinks for every entry\n",
    "                        # a name may appear in several entries, so an unique fake name is created\n",
    "                        fake_name = self._fake_link_fmt.format(fake_id, os.path.basename(image_path))\n",
    "                        fake_path = os.path.join(fake_label_dir, fake_name)\n",
    "                        \n",
    "                        # keep mapping fakefile -> image_path\n",
    "                        self._file_map[fake_path] = image_path\n",
    "\n",
    "                        os.symlink(image_path, fake_path)\n",
    "\n",
    "\n",
    "    def _get_split_mapping(self, file_callback=get_file):\n",
    "        # splits (switch from caffe's split name convention to keras's convention)\n",
    "        _train, _val, _test = 'train', 'test', 'eval'\n",
    "\n",
    "        check_directory(self.splits_dir, 'Splits')\n",
    "\n",
    "        mapping = {}\n",
    "\n",
    "        for split in (_train, _val, _test):\n",
    "            for idx in STREAM_IDS:\n",
    "                key = '{idx}{sep}{split}'.format(split=split, sep=os.path.sep, idx=idx)\n",
    "                value =  os.path.join(self.splits_dir, file_callback(split=split, idx=idx))\n",
    "                mapping[key] = value\n",
    "\n",
    "        return mapping\n",
    "\n",
    "    def fix_fake_link(self, directory, filename):\n",
    "\n",
    "        fake_path = os.path.join(directory, filename)\n",
    "        assert fake_path in self._file_map        \n",
    "        image_path = self._file_map[fake_path]\n",
    "\n",
    "        fake_label_dir = os.path.split(fake_path)[0]\n",
    "        label = os.path.split(fake_label_dir)[1]\n",
    "\n",
    "        original_dir, basename = os.path.split(image_path)\n",
    "\n",
    "        # common ancestor\n",
    "        commonprefix = os.path.commonprefix([original_dir, directory])\n",
    "        # remove commonprefix\n",
    "        fake_subdir = directory[len(commonprefix):]        \n",
    "        original_subdir = original_dir[len(commonprefix):]\n",
    "        # go up in fake path up to common prefix, then go down the real path \n",
    "        go_up = os.path.join(*['..' for folder in fake_subdir.split(os.path.sep)])    \n",
    "        go_down = original_subdir\n",
    "\n",
    "        fixed_filename = os.path.join(go_up, go_down, basename)\n",
    "        return fixed_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialClassifier:\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self._data_dir = data_dir\n",
    "\n",
    "        self._log = logging.getLogger(self.__class__.__name__)\n",
    "        \n",
    "        # initialize when model is configured\n",
    "        self._model_wrapper = None\n",
    "        self.model = None\n",
    "        \n",
    "        self._train_gen, self._val_gen, self._test_gen = None, None, None\n",
    "    \n",
    "    def setup_datagen(self, **kwargs):\n",
    "        self._log.debug(\"Creating data generators.\")        \n",
    "\n",
    "        train_gen_args = dict(\n",
    "            rescale=1./255,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.1,\n",
    "            zoom_range=0.2,\n",
    "            rotation_range=20,\n",
    "            horizontal_flip=True,\n",
    "            preprocessing_function=preprocess_input,\n",
    "        )\n",
    "        test_val_gen_args = dict(\n",
    "            rescale=1./255,\n",
    "            preprocessing_function=preprocess_input,\n",
    "        )\n",
    "        \n",
    "        split_gens = create_data_split_generators(\n",
    "            self._data_dir, train_gen_args, test_val_gen_args,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        self._train_gen, self._val_gen, self._test_gen = split_gens    \n",
    "        \n",
    "    def set_model(self, model, optimizer='adam', \n",
    "                  loss='categorical_crossentropy', \n",
    "                  loss_weights='auto',\n",
    "                  **kwargs):\n",
    "        assert model\n",
    "        self._log.debug(\"Initializing model.\")\n",
    "        \n",
    "        # wrapper allows to train the loss weights\n",
    "        self._model_wrapper = AutoMultiLossWrapper(model)\n",
    "        self._model_wrapper.compile(optimizer, loss, \n",
    "                                    loss_weights=loss_weights, \n",
    "                                    **kwargs)\n",
    "\n",
    "        self.model = self._model_wrapper.model\n",
    "        self._log.info(self.model.summary())\n",
    "        \n",
    "    def fit(self, steps_per_epoch, validation_steps, **kwargs):\n",
    "        assert self.model\n",
    "        assert self._train_gen\n",
    "        assert self._val_gen\n",
    "        self._log.debug(\"Training model from scratch.\")        \n",
    "\n",
    "        # train the model on the new data for a few epochs\n",
    "        return self.model.fit_generator(\n",
    "            self._train_gen,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=self._val_gen,\n",
    "            validation_steps=validation_steps,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "    def evaluate(self, steps, **kwargs):\n",
    "        assert self.model\n",
    "        assert self._test_gen\n",
    "        self._log.debug(\"Evaluating model in test data.\")\n",
    "        \n",
    "        return self.model.evaluate_generator(\n",
    "            self._test_gen,\n",
    "            steps=steps,\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from egosocial.config import MODEL_KEYS, MODELS\n",
    "from egosocial.utils.keras.layers import LRN\n",
    "\n",
    "MODELS_CACHE_DIR = '/home/shared/Documents/final_proj/models/keras_models/models_keras_repr'\n",
    "MODELS_CACHE_SUBDIR = 'bins'\n",
    "\n",
    "def get_models(attributes):\n",
    "    models = {}\n",
    "\n",
    "    for attr in attributes:\n",
    "        file, url =  MODELS[attr]\n",
    "        \n",
    "        model_file = keras.utils.get_file(\n",
    "            file, url,\n",
    "            cache_dir=MODELS_CACHE_DIR, cache_subdir=MODELS_CACHE_SUBDIR,\n",
    "        )\n",
    "        \n",
    "        models[attr] = keras.models.load_model(model_file, \n",
    "                                              custom_objects={'LRN': LRN})\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_top_down(input_features):\n",
    "    x = input_features\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(64, name='fc_features',\n",
    "                  activation='selu',\n",
    "                  bias_initializer='lecun_normal',\n",
    "                  kernel_initializer='lecun_normal',\n",
    "                  bias_regularizer=l2(0.01),\n",
    "                  kernel_regularizer=l2(0.01),              \n",
    "              )(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = AlphaDropout(0.25)(x)\n",
    "\n",
    "    domain = Dense(N_CLS_DOMAIN, name='domain',\n",
    "                   activation='softmax',\n",
    "                   bias_regularizer=l2(0.01),\n",
    "                   kernel_regularizer=l2(0.01),\n",
    "                  )(x)\n",
    "\n",
    "    x = keras.layers.concatenate([x, domain])\n",
    "    \n",
    "    relation = Dense(N_CLS_RELATION, name='relation',\n",
    "                     activation='softmax',\n",
    "                     bias_regularizer=l2(0.01),\n",
    "                     kernel_regularizer=l2(0.01),\n",
    "                    )(x)\n",
    "\n",
    "    return [domain, relation]\n",
    "\n",
    "class ModelFactory:\n",
    "    \n",
    "    def __init__(self, input_shape, image_shape=None):\n",
    "        self.input_shape = input_shape\n",
    "        self.image_shape = image_shape if image_shape else input_shape\n",
    "\n",
    "        self._layer_name_fmt = \"{}_attribute_{}\"\n",
    "\n",
    "    def create(self, model_inputs, models, **kwargs):\n",
    "        freeze_attributes = kwargs.get('freeze_attributes', False)\n",
    "        features_layer = kwargs.get('features_layer', 'pool5')\n",
    "\n",
    "        # collect model inputs and crop them if needed\n",
    "        inputs, cropped_inputs = self._get_inputs(model_inputs)\n",
    "        # update layer name and freeze weights if neccesary\n",
    "        for model_set in models.values():\n",
    "            for attr_name, model in model_set.items():\n",
    "                self._update_model(model, attr_name, \n",
    "                                   freeze_weights=freeze_attributes)\n",
    "\n",
    "        # collect features from all attribute models\n",
    "        feature_outputs = self._get_features(cropped_inputs, models, features_layer)\n",
    "        # concatenate features        \n",
    "        fused_features = keras.layers.concatenate(feature_outputs)        \n",
    "        # feature downsampling\n",
    "        fused_features = GlobalMaxPooling2D()(fused_features)\n",
    "\n",
    "        model = Model(\n",
    "            inputs=inputs,\n",
    "            outputs=create_model_top_down(fused_features)\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _get_inputs(self, model_inputs):        \n",
    "        # cropping dimensions\n",
    "        diff_shape = (np.array(self.image_shape[:2]) - np.array(self.input_shape[:2])) / 2.0\n",
    "        crop_size = tuple(map(int, diff_shape))\n",
    "        assert crop_size[0] >= 0 and crop_size[1] >= 0\n",
    "\n",
    "        # collect model inputs and crop them if needed\n",
    "        inputs, cropped_inputs = [], []\n",
    "        for input_name in model_inputs:\n",
    "            _input = Input(shape=self.image_shape, name=input_name)        \n",
    "            # crop image if neccesary\n",
    "            if self.input_shape != self.image_shape:\n",
    "                cropped_input = Cropping2D(\n",
    "                    crop_size, name=\"cropped_%s\" % input_name\n",
    "                )(_input)\n",
    "            else:\n",
    "                cropped_input = _input\n",
    "\n",
    "            inputs.append(_input)\n",
    "            cropped_inputs.append(cropped_input)\n",
    "            \n",
    "        return inputs, cropped_inputs\n",
    "    \n",
    "    def _get_features(self, inputs, models, features_layer):\n",
    "        # includes features from all attributes\n",
    "        feature_outputs = []\n",
    "        # get image types that are grouped together (e.g. body, face, context)\n",
    "        im_types = models.keys()\n",
    "        for im_type in im_types:            \n",
    "            type_inputs, type_outputs = self._get_image_type_inputs_outputs(\n",
    "                models[im_type], features_layer\n",
    "            )\n",
    "\n",
    "            # model that fuses features from a given image type\n",
    "            fused_attrs = Model(inputs=type_inputs, outputs=type_outputs)\n",
    "            fused_attrs.name = 'features_{}'.format(im_type)        \n",
    "            # collect features from a given image type\n",
    "            for _input in inputs:\n",
    "                if im_type in _input.name:\n",
    "                    attr_output = fused_attrs([_input] * len(fused_attrs.inputs))\n",
    "                    feature_outputs.extend(attr_output)\n",
    "\n",
    "        return feature_outputs\n",
    "\n",
    "    def _get_image_type_inputs_outputs(self, image_type_models, features_layer):\n",
    "        # includes inputs and outputs for a given type\n",
    "        attr_inputs, attr_outputs = [], []\n",
    "        for attr_name, attr_model in image_type_models.items():\n",
    "            # collect attribute inputs\n",
    "            attr_inputs.append(attr_model.input)\n",
    "            # collect attribute outputs\n",
    "            features_layer_attr = self._layer_name_fmt.format(features_layer, attr_name)\n",
    "            attr_outputs.append(attr_model.get_layer(features_layer_attr).output)\n",
    "\n",
    "        return attr_inputs, attr_outputs\n",
    "        \n",
    "    def _update_model(self, model, attr_name, freeze_weights=False):\n",
    "        for layer in model.layers:\n",
    "            # freeze inherited attribute weights\n",
    "            layer.trainable = not freeze_weights\n",
    "            # rename layers if neccesary\n",
    "            if attr_name not in layer.name:\n",
    "                layer.name = self._layer_name_fmt.format(layer.name, attr_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake call to main to process inputs arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\n",
    "    \"--project_dir\", \"/home/shared/Documents/final_proj\",\n",
    "    \"--epochs\", \"30\",\n",
    "    \"--batch_size\", \"64\",\n",
    "]\n",
    "\n",
    "conf = main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakedir = FakeDirectory(conf.FAKE_DIR, conf.SPLITS_DIR)\n",
    "fakedir.create_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = conf.BATCH_SIZE\n",
    "\n",
    "helper = SocialClassifier(conf.FAKE_DIR)\n",
    "\n",
    "helper.setup_datagen(\n",
    "    batch_size=batch_size,\n",
    "    seed=SHARED_SEED,\n",
    "    classes=RELATION_LABELS,\n",
    "    target_size=IMAGE_SHAPE[0:2],\n",
    "    fix_path_cbk=fakedir.fix_fake_link,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size of each split, output from the data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_validation, n_test = 13729, 709, 5106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_reuse_model = conf.REUSE_MODEL\n",
    "do_create_model = not do_reuse_model\n",
    "cache_epochs = 0\n",
    "\n",
    "checkpoint_path = os.path.join(egosocial.config.MODELS_CACHE_DIR, 'training',\n",
    "                               'finetuned_weights.{epoch:02d}-{val_loss:.2f}.h5')\n",
    "\n",
    "if do_reuse_model:\n",
    "    # set parameters from disk\n",
    "    cache_epochs, cache_val_loss = 30, 4.03\n",
    "    model_file = checkpoint_path.format(epoch=cache_epochs, val_loss=cache_val_loss)\n",
    "\n",
    "    # FIXME: doesn't work because the custom loss is an unknown object\n",
    "    # workaround: save only the weights\n",
    "    if os.path.exists(model_file):\n",
    "        model = keras.models.load_model(\n",
    "            model_file, \n",
    "            custom_objects=dict(LRN=LRN, \n",
    "                                AutoMultiLossLayer=AutoMultiLossLayer)\n",
    "        )\n",
    "    else:\n",
    "        do_create_model = True\n",
    "        cache_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_create_model:\n",
    "    selector = AttributeSelector(MODEL_KEYS)\n",
    "    \n",
    "    attribute_models = dict(\n",
    "        body=get_models(selector.filter('body')), \n",
    "        face=get_models(selector.filter('face')),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_create_model:   \n",
    "    model_factory = ModelFactory(INPUT_SHAPE, IMAGE_SHAPE)    \n",
    "    model = model_factory.create(\n",
    "        MODEL_INPUTS, attribute_models,\n",
    "        freeze_attributes=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "\n",
    "helper.set_model(\n",
    "    model,\n",
    "    optimizer=keras.optimizers.Adam(learning_rate, decay=1e-6),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = conf.EPOCHS\n",
    "\n",
    "# checkpoint_path defined before\n",
    "checkpointer = ModelCheckpoint( \n",
    "    filepath=checkpoint_path, monitor='val_loss',\n",
    "    save_best_only=True, period=5,\n",
    ")\n",
    "\n",
    "lr_handler = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001\n",
    ")\n",
    "\n",
    "metrics_path = os.path.join(egosocial.config.MODELS_CACHE_DIR, 'training',\n",
    "                            'metrics.csv')\n",
    "csv_logger = CSVLogger(metrics_path)\n",
    "\n",
    "plot_metrics = PlotLearning(update_step=1)\n",
    "\n",
    "callbacks = [\n",
    "    checkpointer,\n",
    "    lr_handler,\n",
    "    csv_logger,\n",
    "    plot_metrics,\n",
    "]\n",
    "\n",
    "hist = helper.fit(\n",
    "    steps_per_epoch=np.ceil(1.0 * n_train / batch_size),    \n",
    "    validation_steps=np.ceil(1.0 * n_validation / batch_size),\n",
    "    epochs=cache_epochs + epochs,\n",
    "    initial_epoch=cache_epochs,\n",
    "    callbacks=callbacks,\n",
    "    use_multiprocessing=False,\n",
    "    workers=8,\n",
    "    max_queue_size=20,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = helper.model.evaluate_generator(\n",
    "    helper._test_gen,\n",
    "    steps=np.ceil(1.0 * n_test / batch_size),\n",
    "    workers=8,\n",
    "    max_queue_size=20,\n",
    ")\n",
    "\n",
    "for score, metric_name in zip(scores, helper.model.metrics_names):\n",
    "    helper._log.info(\"{} : {:0.4}\".format(metric_name, score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
